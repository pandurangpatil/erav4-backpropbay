{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/pandurangpatil/erav4-backpropbay/blob/main/session8-more-trials/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CIFAR-100 Model (Google Colab)\n",
    "\n",
    "This notebook trains a model on CIFAR-100 using Google Colab's GPU.\n",
    "\n",
    "**Training Configuration:**\n",
    "- Model: resnet50 (or model - WideResNet-28-10)\n",
    "- Epochs: 50\n",
    "- Batch Size: 256\n",
    "- Scheduler: OneCycle Learning Rate Policy\n",
    "- LR Finder: Enabled (automatically finds optimal learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/pandurangpatil/erav4-backpropbay.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the session8-more-trials directory\n",
    "%cd erav4-backpropbay/session8-more-trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n‚úì CUDA is available\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Show GPU memory\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"  GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(f\"\\n‚ö† WARNING: CUDA is not available!\")\n",
    "    print(f\"  Training will be very slow on CPU.\")\n",
    "    print(f\"  Please enable GPU in Colab: Runtime > Change runtime type > GPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"\\n‚úì Using device: {device}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup HuggingFace Token (Optional)\n",
    "\n",
    "If you want to upload your trained model to HuggingFace Hub:\n",
    "1. Go to Colab Secrets (üîë icon in left sidebar)\n",
    "2. Add a secret named `HF_TOKEN` with your HuggingFace API token\n",
    "3. Update `HF_REPO_ID` below with your repository name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HuggingFace token from Colab secrets (optional)\n",
    "HF_TOKEN = None\n",
    "HF_REPO_ID = None\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    HF_REPO_ID = 'your-username/cifar100-session8'  # ‚ö†Ô∏è Change this to your repo!\n",
    "    print(\"‚úì HuggingFace token retrieved from Colab secrets\")\n",
    "    print(f\"‚úì Will upload to: {HF_REPO_ID}\")\n",
    "    print(\"\\n‚ö†Ô∏è Remember to update HF_REPO_ID with your actual repository name!\")\nexcept Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è HuggingFace token not found in secrets\")\n",
    "    print(\"  Training will proceed without HuggingFace upload\")\n",
    "    print(\"  Models will be saved locally in checkpoint folders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Training Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFYING TRAINING FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "required_files = ['train.py', 'model.py', 'config.json', 'requirements.txt']\n",
    "\n",
    "all_exist = True\n",
    "for file in required_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"‚úì\" if exists else \"‚úó\"\n",
    "    print(f\"{status} {file}\")\n",
    "    if not exists:\n",
    "        all_exist = False\n",
    "\n",
    "print(\"=\"*70)\n",
    "if all_exist:\n",
    "    print(\"‚úì All required files found\")\n",
    "else:\n",
    "    print(\"‚ö† Some files are missing!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "The training will use the following settings:\n",
    "\n",
    "### Command Line Arguments:\n",
    "- `--epochs 50` - Train for 50 epochs\n",
    "- `--batch-size 256` - Use batch size of 256\n",
    "- `--model model` - Use the WideResNet-28-10 model from model.py\n",
    "- `--scheduler onecycle` - Use OneCycle learning rate scheduler\n",
    "- `--lr-finder` - Automatically find optimal learning rate before training\n",
    "\n",
    "### What happens during training:\n",
    "\n",
    "**Phase 1: LR Finder (3 epochs)**\n",
    "- Runs a learning rate range test from 1e-6 to 1.0\n",
    "- Identifies optimal `max_lr` and `base_lr` for OneCycle scheduler\n",
    "- Saves plot to `checkpoint_N/lr_finder_plot.png`\n",
    "- Restores model to initial state\n",
    "\n",
    "**Phase 2: Main Training (50 epochs)**\n",
    "- Uses OneCycle scheduler with automatically found learning rates\n",
    "- Applies MixUp augmentation (alpha=0.2)\n",
    "- Uses label smoothing (0.1)\n",
    "- Gradient clipping (max_norm=1.0)\n",
    "- Mixed precision training (AMP)\n",
    "- Early stopping (patience=15)\n",
    "\n",
    "### Checkpoints:\n",
    "- Best model saved automatically when test accuracy improves\n",
    "- Checkpoint folders named `checkpoint_1`, `checkpoint_2`, etc.\n",
    "- Each folder contains: best_model.pth, metrics.json, training_curves.png, config.json, README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "**‚è±Ô∏è Expected Time:** ~2-3 hours on Colab's T4 GPU\n",
    "\n",
    "**Note:** The cell below will run for a long time. You can monitor progress through the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the training command\n",
    "cmd = \"python train.py --epochs 50 --batch-size 256 --model model --scheduler onecycle --lr-finder\"\n",
    "\n",
    "# Add HuggingFace parameters if available\n",
    "if HF_TOKEN and HF_REPO_ID:\n",
    "    cmd += f\" --hf-token {HF_TOKEN} --hf-repo {HF_REPO_ID}\"\n",
    "    print(f\"‚úì HuggingFace upload enabled: {HF_REPO_ID}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Training without HuggingFace upload\")\n",
    "\n",
    "print(f\"\\nRunning command: {cmd.replace(HF_TOKEN or '', '***TOKEN***')}\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run training\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete - View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Find the latest checkpoint directory\n",
    "checkpoint_dirs = sorted(glob.glob('checkpoint_*'), reverse=True)\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LATEST CHECKPOINT: {latest_checkpoint}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load and display metrics\n",
    "    metrics_file = os.path.join(latest_checkpoint, 'metrics.json')\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(f\"üìä TRAINING RESULTS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Best Test Accuracy: {metrics['best_test_accuracy']:.2f}%\")\n",
    "        print(f\"Best Epoch: {metrics['best_epoch']}\")\n",
    "        print(f\"Total Epochs Trained: {len(metrics['epochs'])}\")\n",
    "        print(f\"\\nFinal Metrics:\")\n",
    "        print(f\"  - Train Accuracy: {metrics['train_accuracies'][-1]:.2f}%\")\n",
    "        print(f\"  - Test Accuracy: {metrics['test_accuracies'][-1]:.2f}%\")\n",
    "        print(f\"  - Train Loss: {metrics['train_losses'][-1]:.4f}\")\n",
    "        print(f\"  - Test Loss: {metrics['test_losses'][-1]:.4f}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    # List saved files\n",
    "    print(f\"\\nüìÅ Saved Files in {latest_checkpoint}:\")\n",
    "    for file in sorted(os.listdir(latest_checkpoint)):\n",
    "        file_path = os.path.join(latest_checkpoint, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "else:\n",
    "    print(\"‚ö† No checkpoint directories found. Training may not have completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    \n",
    "    # Display training curves\n",
    "    curves_path = os.path.join(latest_checkpoint, 'training_curves.png')\n",
    "    if os.path.exists(curves_path):\n",
    "        print(\"üìà Training Curves:\")\n",
    "        print(\"=\"*70)\n",
    "        display(Image(filename=curves_path))\n",
    "    else:\n",
    "        print(\"‚ö† Training curves not found.\")\n",
    "    \n",
    "    # Display LR Finder plot\n",
    "    lr_finder_path = os.path.join(latest_checkpoint, 'lr_finder_plot.png')\n",
    "    if os.path.exists(lr_finder_path):\n",
    "        print(\"\\nüîç LR Finder Plot:\")\n",
    "        print(\"=\"*70)\n",
    "        display(Image(filename=lr_finder_path))\n",
    "    else:\n",
    "        print(\"‚ö† LR Finder plot not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Metrics (Interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    metrics_file = os.path.join(latest_checkpoint, 'metrics.json')\n",
    "    \n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        epochs = metrics['epochs']\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Loss\n",
    "        axes[0, 0].plot(epochs, metrics['train_losses'], label='Train Loss', color='blue')\n",
    "        axes[0, 0].plot(epochs, metrics['test_losses'], label='Test Loss', color='red')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Training and Test Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Accuracy\n",
    "        axes[0, 1].plot(epochs, metrics['train_accuracies'], label='Train Accuracy', color='blue')\n",
    "        axes[0, 1].plot(epochs, metrics['test_accuracies'], label='Test Accuracy', color='red')\n",
    "        axes[0, 1].axhline(y=74, color='green', linestyle='--', label='Target (74%)', alpha=0.5)\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 1].set_title('Training and Test Accuracy')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Learning Rate\n",
    "        axes[1, 0].plot(epochs, metrics['learning_rates'], color='green')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Learning Rate')\n",
    "        axes[1, 0].set_title('Learning Rate Schedule')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Accuracy Gap\n",
    "        accuracy_gap = [train - test for train, test in zip(metrics['train_accuracies'], metrics['test_accuracies'])]\n",
    "        axes[1, 1].plot(epochs, accuracy_gap, color='orange')\n",
    "        axes[1, 1].set_xlabel('Epoch')\n",
    "        axes[1, 1].set_ylabel('Gap (%)')\n",
    "        axes[1, 1].set_title('Train-Test Accuracy Gap (Overfitting Indicator)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nüìä TRAINING SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Best Test Accuracy: {max(metrics['test_accuracies']):.2f}% (Epoch {metrics['test_accuracies'].index(max(metrics['test_accuracies'])) + 1})\")\n",
    "        print(f\"Final Test Accuracy: {metrics['test_accuracies'][-1]:.2f}%\")\n",
    "        print(f\"Final Train-Test Gap: {accuracy_gap[-1]:.2f}%\")\n",
    "        print(f\"Min Test Loss: {min(metrics['test_losses']):.4f}\")\n",
    "        print(f\"Max Learning Rate: {max(metrics['learning_rates']):.6f}\")\n",
    "        print(f\"Min Learning Rate: {min(metrics['learning_rates']):.6f}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    else:\n",
    "        print(\"‚ö† Metrics file not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Best Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    best_model_path = os.path.join(latest_checkpoint, 'best_model.pth')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"BEST MODEL CHECKPOINT INFORMATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"Train Accuracy: {checkpoint['train_accuracy']:.2f}%\")\n",
    "        print(f\"Test Accuracy: {checkpoint['test_accuracy']:.2f}%\")\n",
    "        print(f\"Train Loss: {checkpoint['train_loss']:.4f}\")\n",
    "        print(f\"Test Loss: {checkpoint['test_loss']:.4f}\")\n",
    "        print(f\"Timestamp: {checkpoint['timestamp']}\")\n",
    "        \n",
    "        print(f\"\\nModel Configuration:\")\n",
    "        for key, value in checkpoint['config'].items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        print(\"‚úì Checkpoint loaded successfully\")\n",
    "        print(f\"  Model state dict keys: {len(checkpoint['model_state_dict'])} layers\")\n",
    "        print(f\"  Optimizer state dict available: {'optimizer_state_dict' in checkpoint}\")\n",
    "        \n",
    "        # Optional: Load model for inference\n",
    "        # Uncomment to actually load the model\n",
    "        # import importlib\n",
    "        # model_module = importlib.import_module('model')\n",
    "        # model = model_module.Net()\n",
    "        # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        # model.eval()\n",
    "        # print(\"‚úì Model loaded and ready for inference\")\n",
    "    else:\n",
    "        print(\"‚ö† Best model checkpoint not found.\")\n",
    "else:\n",
    "    print(\"‚ö† No checkpoint directories found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Checkpoint Files (Optional)\n",
    "\n",
    "Download the trained model and artifacts to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    \n",
    "    # Create a zip file of the checkpoint directory\n",
    "    zip_filename = f\"{latest_checkpoint}.zip\"\n",
    "    shutil.make_archive(latest_checkpoint, 'zip', latest_checkpoint)\n",
    "    \n",
    "    print(f\"‚úì Created {zip_filename}\")\n",
    "    print(f\"  Size: {os.path.getsize(zip_filename) / (1024*1024):.2f} MB\")\n",
    "    print(\"\\nDownloading...\")\n",
    "    \n",
    "    # Download the zip file\n",
    "    files.download(zip_filename)\n",
    "    \n",
    "    print(\"‚úì Download complete!\")\n",
    "else:\n",
    "    print(\"‚ö† No checkpoint directories found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training is complete! Here's what was saved:\n",
    "\n",
    "### üìÅ Checkpoint Files:\n",
    "- `best_model.pth` - The model with the best test accuracy\n",
    "- `training_curves.png` - Visualization of training progress\n",
    "- `lr_finder_plot.png` - Learning rate range test results\n",
    "- `metrics.json` - Complete training history (all epochs)\n",
    "- `config.json` - Hyperparameter configuration\n",
    "- `README.md` - Model card with detailed documentation\n",
    "\n",
    "### üîÑ Next Steps:\n",
    "1. Review the training curves to understand model performance\n",
    "2. Check the LR Finder plot to see the learning rate selection\n",
    "3. Download the checkpoint files using the cell above\n",
    "4. (Optional) Upload to HuggingFace Hub if you added credentials\n",
    "\n",
    "### üéØ Model Usage:\n",
    "To use the trained model:\n",
    "```python\n",
    "import torch\n",
    "from model import Net\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('checkpoint_1/best_model.pth')\n",
    "model = Net()\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for using this training notebook!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
