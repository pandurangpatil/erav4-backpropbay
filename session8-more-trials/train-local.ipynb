{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CIFAR-100 Model (Local Mac)\n",
    "This notebook trains a model on CIFAR-100 locally on Mac with the **new modular codebase**.\n",
    "\n",
    "**Training Command:**\n",
    "```bash\n",
    "python train.py --epochs 50 --batch-size 256 --model resnet50 --scheduler onecycle --lr-finder\n",
    "```\n",
    "\n",
    "**Modular Structure:**\n",
    "- Datasets in `datasets/` - Easy to add new datasets\n",
    "- Models in `models/` - Clean separation of architectures  \n",
    "- Training components in `training/` - Reusable optimizer, scheduler, LR finder\n",
    "- Utilities in `utils/` - Checkpointing, metrics, HuggingFace upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENVIRONMENT INFORMATION\n",
      "======================================================================\n",
      "Python Version: 3.12.3 (main, Oct  7 2025, 19:27:29) [Clang 17.0.0 (clang-1700.0.13.5)]\n",
      "Platform: macOS-15.5-arm64-arm-64bit\n",
      "Processor: arm\n",
      "Working Directory: /Users/pandurang/projects/pandurang/erav4-backpropbay/session8-more-trials\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU/MPS Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PYTORCH & GPU DETECTION\n",
      "======================================================================\n",
      "PyTorch Version: 2.9.0\n",
      "‚úì Apple MPS (Metal Performance Shaders) is available\n",
      "  This Mac has Apple Silicon GPU acceleration\n",
      "‚úì Using device: mps\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PYTORCH & GPU DETECTION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Check for CUDA\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úì CUDA is available\")\n",
    "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "        device = torch.device('cuda')\n",
    "    # Check for MPS (Apple Silicon)\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        print(f\"‚úì Apple MPS (Metal Performance Shaders) is available\")\n",
    "        print(f\"  This Mac has Apple Silicon GPU acceleration\")\n",
    "        device = torch.device('mps')\n",
    "    else:\n",
    "        print(f\"‚ö† No GPU detected. Training will use CPU\")\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"‚úì Using device: {device}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö† PyTorch not installed. Will install dependencies in next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.0.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: torchsummary>=0.1.5 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: torchinfo>=1.8.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (3.10.7)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (2.2.6)\n",
      "Requirement already satisfied: plotille>=5.0.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (5.0.0)\n",
      "Requirement already satisfied: albumentations>=1.3.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (2.0.8)\n",
      "Requirement already satisfied: huggingface_hub>=0.20.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.35.3)\n",
      "Requirement already satisfied: filelock in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from torchvision>=0.15.0->-r requirements.txt (line 2)) (12.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 6)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 6)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 6)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from albumentations>=1.3.0->-r requirements.txt (line 9)) (1.16.2)\n",
      "Requirement already satisfied: PyYAML in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from albumentations>=1.3.0->-r requirements.txt (line 9)) (6.0.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from albumentations>=1.3.0->-r requirements.txt (line 9)) (2.12.3)\n",
      "Requirement already satisfied: albucore==0.0.24 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from albumentations>=1.3.0->-r requirements.txt (line 9)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from albumentations>=1.3.0->-r requirements.txt (line 9)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from albucore==0.0.24->albumentations>=1.3.0->-r requirements.txt (line 9)) (4.2.1)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from albucore==0.0.24->albumentations>=1.3.0->-r requirements.txt (line 9)) (6.5.3)\n",
      "Requirement already satisfied: requests in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from huggingface_hub>=0.20.0->-r requirements.txt (line 10)) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from huggingface_hub>=0.20.0->-r requirements.txt (line 10)) (1.1.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations>=1.3.0->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations>=1.3.0->-r requirements.txt (line 9)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from pydantic>=2.9.2->albumentations>=1.3.0->-r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from requests->huggingface_hub>=0.20.0->-r requirements.txt (line 10)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from requests->huggingface_hub>=0.20.0->-r requirements.txt (line 10)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from requests->huggingface_hub>=0.20.0->-r requirements.txt (line 10)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pandurang/.pyenv/versions/3.12.3/envs/session8-more-trials/lib/python3.12/site-packages (from requests->huggingface_hub>=0.20.0->-r requirements.txt (line 10)) (2025.10.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Verify Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LIBRARY VERSIONS\n",
      "======================================================================\n",
      "PyTorch: 2.9.0\n",
      "TorchVision: 0.24.0\n",
      "NumPy: 2.2.6\n",
      "Matplotlib: 3.10.7\n",
      "Albumentations: 2.0.8\n",
      "TQDM: 4.67.1\n",
      "======================================================================\n",
      "‚úì All dependencies successfully imported\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchsummary\n",
    "import torchinfo\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import numpy\n",
    "import plotille\n",
    "import albumentations\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LIBRARY VERSIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"TorchVision: {torchvision.__version__}\")\n",
    "print(f\"NumPy: {numpy.__version__}\")\n",
    "print(f\"Matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"Albumentations: {albumentations.__version__}\")\n",
    "print(f\"TQDM: {tqdm.__version__}\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚úì All dependencies successfully imported\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Training Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "VERIFYING TRAINING FILES AND MODULAR STRUCTURE\n",
      "======================================================================\n",
      "\n",
      "Required Files:\n",
      "‚úì train.py\n",
      "‚úì config.json\n",
      "‚úì requirements.txt\n",
      "\n",
      "Modular Directories:\n",
      "‚úì datasets/\n",
      "‚úì models/\n",
      "‚úì training/\n",
      "‚úì utils/\n",
      "======================================================================\n",
      "‚úì All required files and modular structure verified!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFYING TRAINING FILES AND MODULAR STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check required files\n",
    "required_files = [\n",
    "    'train.py',\n",
    "    'config.json',\n",
    "    'requirements.txt'\n",
    "]\n",
    "\n",
    "print(\"\\nRequired Files:\")\n",
    "files_ok = True\n",
    "for file in required_files:\n",
    "    exists = os.path.exists(file)\n",
    "    status = \"‚úì\" if exists else \"‚úó\"\n",
    "    print(f\"{status} {file}\")\n",
    "    if not exists:\n",
    "        files_ok = False\n",
    "\n",
    "# Check modular directories\n",
    "required_dirs = ['datasets', 'models', 'training', 'utils']\n",
    "print(\"\\nModular Directories:\")\n",
    "dirs_ok = True\n",
    "for dir in required_dirs:\n",
    "    exists = os.path.isdir(dir)\n",
    "    status = \"‚úì\" if exists else \"‚úó\"\n",
    "    print(f\"{status} {dir}/\")\n",
    "    if not exists:\n",
    "        dirs_ok = False\n",
    "\n",
    "print(\"=\"*70)\n",
    "if files_ok and dirs_ok:\n",
    "    print(\"‚úì All required files and modular structure verified!\")\n",
    "else:\n",
    "    print(\"‚ö† Some files or directories are missing. Please check your directory.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Training Configuration\n",
    "\n",
    "The training will use the following configuration with the **new modular codebase**:\n",
    "\n",
    "- **Model**: resnet50 (from `models/resnet50.py`)\n",
    "  - Alternative: `wideresnet28-10` (from `models/wideresnet.py`)\n",
    "- **Epochs**: 50\n",
    "- **Batch Size**: 256\n",
    "- **Scheduler**: OneCycle Learning Rate Policy\n",
    "- **LR Finder**: Enabled (will automatically find optimal learning rate)\n",
    "\n",
    "The LR Finder will:\n",
    "1. Run a learning rate range test before training\n",
    "2. Automatically determine the best `max_lr` and `base_lr` for OneCycle scheduler\n",
    "3. Save the LR finder plot to `checkpoint_N/lr_finder_plot.png`\n",
    "\n",
    "**Available Models:**\n",
    "- `resnet50` - ResNet50 (23.5M parameters)\n",
    "- `wideresnet28-10` - WideResNet-28-10 (36.5M parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "**Note:** This will take a significant amount of time depending on your hardware.\n",
    "\n",
    "Training progress will be displayed below with:\n",
    "- Real-time loss and accuracy metrics\n",
    "- Learning rate schedule visualization\n",
    "- Checkpoint saving at key epochs\n",
    "- Early stopping if no improvement\n",
    "\n",
    "**Expected behavior:**\n",
    "1. LR Finder will run first (3 epochs of range testing)\n",
    "2. LR Finder will suggest optimal learning rates\n",
    "3. Main training will begin with the suggested learning rates\n",
    "4. Model checkpoints will be saved to `checkpoint_N/` folder\n",
    "\n",
    "**Using the new modular codebase** - models loaded from `models/` directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded config from: ./config.json\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model: resnet50\n",
      "Dataset: cifar100\n",
      "Epochs: 50\n",
      "Batch Size: 256\n",
      "Optimizer: sgd\n",
      "Scheduler: onecycle\n",
      "Augmentation: strong\n",
      "MixUp: True (alpha=0.2)\n",
      "Label Smoothing: 0.1\n",
      "Mixed Precision: True\n",
      "Gradient Clipping: 1.0\n",
      "LR Finder: True\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "GPU DETECTION AND CONFIGURATION\n",
      "======================================================================\n",
      "‚úì Apple MPS (Metal Performance Shaders) is available\n",
      "‚úì Using device: mps\n",
      "‚úì PyTorch Version: 2.9.0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üìä Dataset: CIFAR-100\n",
      "   Classes: 100\n",
      "   Train samples: 50000\n",
      "   Test samples: 10000\n",
      "\n",
      "Loading datasets...\n",
      "‚úì Train batches: 196\n",
      "‚úì Test batches: 40\n",
      "\n",
      "Creating model: resnet50\n",
      "‚úì Model created\n",
      "\n",
      "‚úì Optimizer: sgd\n",
      "\n",
      "‚úì Scheduler: onecycle\n",
      "\n",
      "üìÅ Checkpoint folder: ./checkpoint_2\n",
      "/Users/pandurang/projects/pandurang/erav4-backpropbay/session8-more-trials/training/trainer.py:71: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler() if self.use_amp else None\n",
      "/Users/pandurang/.pyenv/versions/session8-more-trials/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  super().__init__(\n",
      "\n",
      "======================================================================\n",
      "STARTING LR FINDER\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "LEARNING RATE FINDER - RANGE TEST\n",
      "======================================================================\n",
      "Testing learning rates from 1.00e-06 to 1.00e+00\n",
      "Running for 3 epochs\n",
      "Note: Using clean data (no MixUp, no Label Smoothing)\n",
      "======================================================================\n",
      "\n",
      "LR Finder Epoch 1/3:  35%|‚ñé| 68/196 [00:38<00:57,  2.22it/s, lr=4.83e-06, loss=4^C\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x1031d07c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pandurang/.pyenv/versions/session8-more-trials/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1654, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/pandurang/.pyenv/versions/session8-more-trials/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1618, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/pandurang/.pyenv/versions/3.12.3/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pandurang/.pyenv/versions/3.12.3/lib/python3.12/multiprocessing/popen_fork.py\", line 43, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pandurang/.pyenv/versions/3.12.3/lib/python3.12/multiprocessing/popen_fork.py\", line 27, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pandurang/.pyenv/versions/session8-more-trials/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 64046) is killed by signal: Interrupt: 2. \n",
      "LR Finder Epoch 1/3:  35%|‚ñé| 68/196 [00:38<01:13,  1.75it/s, lr=4.83e-06, loss=4\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Run training with the specified configuration\n",
    "# Note: Using modular structure - model loaded from models/resnet50.py\n",
    "!python train.py --epochs 50 --batch-size 256 --model resnet50 --scheduler onecycle --lr-finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Complete - View Results\n",
    "\n",
    "After training completes, you can view the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List checkpoint directories\n",
    "import glob\n",
    "import json\n",
    "\n",
    "checkpoint_dirs = sorted(glob.glob('checkpoint_*'), reverse=True)\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LATEST CHECKPOINT: {latest_checkpoint}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load and display metrics\n",
    "    metrics_file = os.path.join(latest_checkpoint, 'metrics.json')\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        print(f\"Best Test Accuracy: {metrics['best_test_accuracy']:.2f}%\")\n",
    "        print(f\"Best Epoch: {metrics['best_epoch']}\")\n",
    "        print(f\"Total Epochs Trained: {len(metrics['epochs'])}\")\n",
    "        print(f\"\\nFinal Metrics:\")\n",
    "        print(f\"  - Train Accuracy: {metrics['train_accuracies'][-1]:.2f}%\")\n",
    "        print(f\"  - Test Accuracy: {metrics['test_accuracies'][-1]:.2f}%\")\n",
    "        print(f\"  - Train Loss: {metrics['train_losses'][-1]:.4f}\")\n",
    "        print(f\"  - Test Loss: {metrics['test_losses'][-1]:.4f}\")\n",
    "    \n",
    "    # List saved files\n",
    "    print(f\"\\nSaved Files in {latest_checkpoint}:\")\n",
    "    for file in sorted(os.listdir(latest_checkpoint)):\n",
    "        file_path = os.path.join(latest_checkpoint, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "else:\n",
    "    print(\"No checkpoint directories found. Training may not have completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    \n",
    "    # Display training curves\n",
    "    curves_path = os.path.join(latest_checkpoint, 'training_curves.png')\n",
    "    if os.path.exists(curves_path):\n",
    "        print(\"Training Curves:\")\n",
    "        display(Image(filename=curves_path))\n",
    "    else:\n",
    "        print(\"Training curves not found.\")\n",
    "    \n",
    "    # Display LR Finder plot\n",
    "    lr_finder_path = os.path.join(latest_checkpoint, 'lr_finder_plot.png')\n",
    "    if os.path.exists(lr_finder_path):\n",
    "        print(\"\\nLR Finder Plot:\")\n",
    "        display(Image(filename=lr_finder_path))\n",
    "    else:\n",
    "        print(\"LR Finder plot not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test Best Model\n",
    "\n",
    "You can load the best saved model and use it for inference or further testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[0]\n",
    "    best_model_path = os.path.join(latest_checkpoint, 'best_model.pth')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        # Load the checkpoint with weights_only=False for PyTorch 2.6+\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"BEST MODEL CHECKPOINT INFORMATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"Train Accuracy: {checkpoint['train_accuracy']:.2f}%\")\n",
    "        print(f\"Test Accuracy: {checkpoint['test_accuracy']:.2f}%\")\n",
    "        print(f\"Train Loss: {checkpoint['train_loss']:.4f}\")\n",
    "        print(f\"Test Loss: {checkpoint['test_loss']:.4f}\")\n",
    "        print(f\"Timestamp: {checkpoint['timestamp']}\")\n",
    "        \n",
    "        print(f\"\\nModel Configuration:\")\n",
    "        for key, value in checkpoint['config'].items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "        \n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Load model using the new modular structure\n",
    "        from models import get_model\n",
    "        \n",
    "        model_name = checkpoint['config'].get('model', 'resnet50')\n",
    "        model = get_model(model_name, num_classes=100)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        print(f\"‚úì Model '{model_name}' loaded successfully from modular structure\")\n",
    "        print(\"‚úì Model ready for inference\")\n",
    "    else:\n",
    "        print(\"‚ö† Best model checkpoint not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training is complete using the **new modular codebase**! The following artifacts have been saved:\n",
    "\n",
    "### üìÅ Checkpoint Files:\n",
    "- **Best Model**: `checkpoint_N/best_model.pth` - The model with the best test accuracy\n",
    "- **Training Curves**: `checkpoint_N/training_curves.png` - Visualization of training progress\n",
    "- **LR Finder Plot**: `checkpoint_N/lr_finder_plot.png` - Learning rate range test results\n",
    "- **Metrics**: `checkpoint_N/metrics.json` - Complete training history\n",
    "- **Config**: `checkpoint_N/config.json` - Training configuration\n",
    "- **Model Card**: `checkpoint_N/README.md` - Detailed model documentation\n",
    "\n",
    "### üèóÔ∏è Modular Structure Benefits:\n",
    "- **Datasets** (`datasets/`) - Easy to add CIFAR-10, ImageNet, etc.\n",
    "- **Models** (`models/`) - Clean separation of architectures\n",
    "- **Training** (`training/`) - Reusable optimizer, scheduler, LR finder\n",
    "- **Utils** (`utils/`) - Checkpointing, metrics, HuggingFace upload\n",
    "\n",
    "### üéØ Model Usage (New Modular Way):\n",
    "```python\n",
    "import torch\n",
    "from models import get_model\n",
    "\n",
    "# Load checkpoint (PyTorch 2.6+ requires weights_only=False)\n",
    "checkpoint = torch.load('checkpoint_N/best_model.pth', \n",
    "                       map_location='cpu', weights_only=False)\n",
    "\n",
    "# Get model using modular factory\n",
    "model = get_model('resnet50', num_classes=100)  # or 'wideresnet28-10'\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "### üÜï Available Models:\n",
    "- `resnet50` - ResNet50 (23.5M parameters)\n",
    "- `wideresnet28-10` - WideResNet-28-10 (36.5M parameters)\n",
    "- More models can be easily added to `models/` directory!\n",
    "\n",
    "You can find all checkpoints in the `checkpoint_N/` directories where N is the run number.\n",
    "\n",
    "---\n",
    "\n",
    "**Modular codebase makes it easy to extend and maintain!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session8-more-trials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
