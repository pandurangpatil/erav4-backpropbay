{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-100 Training - Session 05 (Conservative Optimization)\n",
    "\n",
    "## Target: 74-75% Accuracy in 100 Epochs\n",
    "\n",
    "### Conservative Optimization Strategy:\n",
    "1. **Progressive Augmentation** (2 phases): Full \u2192 Reduced\n",
    "2. **Progressive Dropout**: 0.3 \u2192 0.2\n",
    "3. **Two-Phase LR**: Warm restarts (T_0=25) \u2192 Smooth decay\n",
    "4. **Keep Max LR**: 0.1 (same as Session-02)\n",
    "5. **Progressive MixUp**: 0.2 \u2192 0.15\n",
    "6. **Keep Label Smoothing**: 0.1 (no change)\n",
    "\n",
    "### Storage Strategy:\n",
    "- **Google Drive**: All checkpoints (auto-cleanup keeps last 5)\n",
    "- **HuggingFace**: Upload every 10 epochs + best model\n",
    "\n",
    "### Improvements from Session-02 (71.20%):\n",
    "- Expected gain: +3-4% (target: 74-75%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# CIFAR-100 Mean and Std\n",
    "cifar100_mean = (0.5071, 0.4865, 0.4409)\n",
    "cifar100_std = (0.2673, 0.2564, 0.2761)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoint storage\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    GDRIVE_DIR = '/content/drive/MyDrive/cifar100_checkpoints_session05'\n",
    "    os.makedirs(GDRIVE_DIR, exist_ok=True)\n",
    "    print(f\"\u2713 Google Drive mounted: {GDRIVE_DIR}\")\n",
    "    USE_GDRIVE = True\n",
    "except:\n",
    "    print(\"\u26a0 Google Drive not available (not running in Colab)\")\n",
    "    GDRIVE_DIR = './checkpoints_gdrive_backup'\n",
    "    os.makedirs(GDRIVE_DIR, exist_ok=True)\n",
    "    USE_GDRIVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Setup\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"\u2713 HuggingFace token retrieved from Colab secrets\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not retrieve HF_TOKEN: {e}\")\n",
    "    HF_TOKEN = None\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import HfApi, create_repo\n",
    "except ImportError:\n",
    "    !pip install -q huggingface_hub\n",
    "    from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "REPO_ID = 'pandurangpatil/cifar100-wideresnet-session05'\n",
    "api = HfApi()\n",
    "\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True, token=HF_TOKEN)\n",
    "        print(f\"\u2713 HuggingFace repository ready: https://huggingface.co/{REPO_ID}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create repository: {e}\")\n",
    "else:\n",
    "    print(\"\u26a0 HuggingFace upload will be skipped (no token)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive Data Augmentation (2 Phases - Conservative)\n",
    "class ProgressiveAlbumentationsTransforms:\n",
    "    def __init__(self, mean, std, phase='full'):\n",
    "        if phase == 'full':  # Epochs 1-60: Full augmentation\n",
    "            self.aug = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "                A.CoarseDropout(max_holes=1, max_height=8, max_width=8, p=0.5, fill_value=0),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
    "                A.Normalize(mean=mean, std=std),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:  # phase == 'reduced', Epochs 61-100: Reduced augmentation\n",
    "            self.aug = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.07, rotate_limit=10, p=0.35),\n",
    "                A.CoarseDropout(max_holes=1, max_height=6, max_width=6, p=0.35, fill_value=0),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.2),\n",
    "                A.Normalize(mean=mean, std=std),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        self.phase = phase\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image = np.array(img)\n",
    "        return self.aug(image=image)[\"image\"]\n",
    "\n",
    "def get_augmentation_phase(epoch):\n",
    "    \"\"\"Determine augmentation phase based on epoch\"\"\"\n",
    "    if epoch <= 60:\n",
    "        return 'full'\n",
    "    else:\n",
    "        return 'reduced'\n",
    "\n",
    "# Initial transforms (full augmentation)\n",
    "train_transforms = ProgressiveAlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std, phase='full')\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
    "])\n",
    "\n",
    "print(\"\u2713 Progressive augmentation initialized (Phase: full)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 Dataset\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "cifar100_classes = datasets.CIFAR100(root='./data', train=False).classes\n",
    "print(f\"Training batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WideResNet Architecture with Dynamic Dropout\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.equalInOut = in_planes == out_planes\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "        self.shortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, 1, stride=stride, bias=False) or None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.conv1(out if self.equalInOut else x)\n",
    "        out = self.relu2(self.bn2(out))\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return out + (x if self.equalInOut else self.shortcut(x))\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, num_classes=100, widen_factor=10, dropRate=0.3):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) // 6\n",
    "        block = BasicBlock\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)\n",
    "\n",
    "def update_model_dropout(model, new_dropout):\n",
    "    \"\"\"Dynamically update dropout rate during training\"\"\"\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, BasicBlock):\n",
    "            module.dropRate = new_dropout\n",
    "            count += 1\n",
    "    print(f\"\u2713 Updated dropout to {new_dropout} in {count} blocks\")\n",
    "\n",
    "# Initialize model\n",
    "model = WideResNet(depth=28, widen_factor=10, dropRate=0.3, num_classes=100).to(device)\n",
    "print(f\"\u2713 Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MixUp function\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, initial_lr, target_lr, steps_per_epoch):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_epochs * steps_per_epoch\n",
    "        self.initial_lr = initial_lr\n",
    "        self.target_lr = target_lr\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def step(self):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_lr + (self.target_lr - self.initial_lr) * self.current_step / self.warmup_steps\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_step += 1\n",
    "    \n",
    "    def is_warmup(self):\n",
    "        return self.current_step < self.warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Checkpoint Manager\n",
    "class GoogleDriveCheckpointManager:\n",
    "    def __init__(self, gdrive_dir, max_keep=5):\n",
    "        self.gdrive_dir = gdrive_dir\n",
    "        self.max_keep = max_keep\n",
    "        self.checkpoints = []\n",
    "        os.makedirs(gdrive_dir, exist_ok=True)\n",
    "    \n",
    "    def save(self, checkpoint_path, epoch, is_best=False):\n",
    "        \"\"\"Save checkpoint to Google Drive\"\"\"\n",
    "        try:\n",
    "            if is_best:\n",
    "                # Always keep best model\n",
    "                best_path = os.path.join(self.gdrive_dir, 'best_model.pth')\n",
    "                shutil.copy(checkpoint_path, best_path)\n",
    "                print(f\"  \u2192 Saved to GDrive: best_model.pth\")\n",
    "            else:\n",
    "                # Save regular checkpoint\n",
    "                gdrive_path = os.path.join(self.gdrive_dir, f'checkpoint_epoch{epoch:03d}.pth')\n",
    "                shutil.copy(checkpoint_path, gdrive_path)\n",
    "                self.checkpoints.append((epoch, gdrive_path))\n",
    "                \n",
    "                # Cleanup: keep only last max_keep checkpoints\n",
    "                if len(self.checkpoints) > self.max_keep:\n",
    "                    old_epoch, old_path = self.checkpoints.pop(0)\n",
    "                    if os.path.exists(old_path):\n",
    "                        os.remove(old_path)\n",
    "                        print(f\"  \u2192 Cleaned up old checkpoint: epoch{old_epoch:03d}\")\n",
    "                \n",
    "                print(f\"  \u2192 Saved to GDrive: checkpoint_epoch{epoch:03d}.pth\")\n",
    "        except Exception as e:\n",
    "            print(f\"  \u2717 GDrive save failed: {e}\")\n",
    "    \n",
    "    def list_checkpoints(self):\n",
    "        \"\"\"List all checkpoints in Google Drive\"\"\"\n",
    "        files = [f for f in os.listdir(self.gdrive_dir) if f.endswith('.pth')]\n",
    "        return sorted(files)\n",
    "\n",
    "gdrive_manager = GoogleDriveCheckpointManager(GDRIVE_DIR, max_keep=5)\n",
    "print(f\"\u2713 Google Drive checkpoint manager initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Upload Functions\n",
    "def upload_to_huggingface(file_path, path_in_repo, commit_message=\"Upload checkpoint\"):\n",
    "    \"\"\"Upload file to HuggingFace (only every 10 epochs)\"\"\"\n",
    "    if not HF_TOKEN:\n",
    "        return\n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=path_in_repo,\n",
    "            repo_id=REPO_ID,\n",
    "            repo_type=\"model\",\n",
    "            token=HF_TOKEN,\n",
    "            commit_message=commit_message\n",
    "        )\n",
    "        print(f\"  \u2192 Uploaded to HF: {path_in_repo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  \u2717 HF upload failed: {e}\")\n",
    "\n",
    "# HuggingFace upload schedule (every 10 epochs)\n",
    "HF_UPLOAD_EPOCHS = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "print(f\"\u2713 HuggingFace uploads scheduled for epochs: {HF_UPLOAD_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing Functions\n",
    "def train(model, device, train_loader, optimizer, scheduler, warmup_scheduler, scaler, epoch,\n",
    "          use_mixup=True, mixup_alpha=0.2, label_smoothing=0.1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            if use_mixup:\n",
    "                inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
    "                outputs = model(inputs)\n",
    "                loss = lam * F.cross_entropy(outputs, targets_a, label_smoothing=label_smoothing) + \\\n",
    "                       (1 - lam) * F.cross_entropy(outputs, targets_b, label_smoothing=label_smoothing)\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "                loss = F.cross_entropy(outputs, target, label_smoothing=label_smoothing)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if warmup_scheduler.is_warmup():\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        _, pred = outputs.max(1)\n",
    "        if use_mixup:\n",
    "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
    "        else:\n",
    "            correct += pred.eq(target).sum().item()\n",
    "        processed += len(data)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100*correct/processed:.2f}% LR={current_lr:.6f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / processed\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n\")\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CONFIGURATION - SESSION 05 (CONSERVATIVE)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: WideResNet-28-10 (36.5M parameters)\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Dropout: 0.3 \u2192 0.2 (epoch 61)\")\n",
    "print(f\"Augmentation: Full (1-60) \u2192 Reduced (61-100)\")\n",
    "print(f\"MixUp: 0.2 (1-60) \u2192 0.15 (61-100)\")\n",
    "print(f\"Label Smoothing: 0.1 (constant)\")\n",
    "print(f\"LR Schedule: Phase1 (1-50) T_0=25, Phase2 (51-100) CosineDecay\")\n",
    "print(f\"Max LR: 0.1 (same as Session-02)\")\n",
    "print(f\"Storage: GDrive (all) + HuggingFace (every 10 epochs)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "EPOCHS = 100\n",
    "WARMUP_EPOCHS = 5\n",
    "INITIAL_LR = 0.01\n",
    "MAX_LR = 0.1  # Conservative - same as Session-02\n",
    "MIN_LR = 1e-4\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=0.9, weight_decay=1e-3)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=25, T_mult=1, eta_min=1e-4)\n",
    "warmup_scheduler = WarmupScheduler(optimizer, WARMUP_EPOCHS, INITIAL_LR, MAX_LR, len(train_loader))\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Tracking\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "learning_rates = []\n",
    "dropout_history = []\n",
    "mixup_history = []\n",
    "aug_phase_history = []\n",
    "\n",
    "best_test_acc = 0.0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Conservative Progressive Strategies\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "current_aug_phase = 'full'\n",
    "current_dropout = 0.3\n",
    "current_mixup_alpha = 0.2\n",
    "current_label_smoothing = 0.1\n",
    "current_use_mixup = True\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # === PHASE TRANSITIONS (Conservative) ===\n",
    "    \n",
    "    # LR scheduler transition (epoch 51)\n",
    "    if epoch == 51:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"\ud83d\udccd LR SCHEDULER TRANSITION AT EPOCH {epoch}\")\n",
    "        print(f\"   CosineAnnealingWarmRestarts \u2192 CosineAnnealingLR\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-5)\n",
    "    \n",
    "    # Combined transition at epoch 61\n",
    "    if epoch == 61:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"\ud83d\udccd MAJOR PHASE TRANSITION AT EPOCH {epoch}\")\n",
    "        print(f\"   \u2022 Augmentation: Full \u2192 Reduced\")\n",
    "        print(f\"   \u2022 Dropout: 0.3 \u2192 0.2\")\n",
    "        print(f\"   \u2022 MixUp: 0.2 \u2192 0.15\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # Update augmentation\n",
    "        current_aug_phase = 'reduced'\n",
    "        train_transforms = ProgressiveAlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std, phase=current_aug_phase)\n",
    "        train_dataset.transform = train_transforms\n",
    "        \n",
    "        # Update dropout\n",
    "        current_dropout = 0.2\n",
    "        update_model_dropout(model, current_dropout)\n",
    "        \n",
    "        # Update MixUp\n",
    "        current_mixup_alpha = 0.15\n",
    "    \n",
    "    # === TRAINING ===\n",
    "    train_loss, train_acc = train(\n",
    "        model, device, train_loader, optimizer, scheduler, warmup_scheduler, scaler, epoch,\n",
    "        use_mixup=current_use_mixup, mixup_alpha=current_mixup_alpha, label_smoothing=current_label_smoothing\n",
    "    )\n",
    "    \n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    \n",
    "    # Record metrics\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "    dropout_history.append(current_dropout)\n",
    "    mixup_history.append(current_mixup_alpha if current_use_mixup else 0.0)\n",
    "    aug_phase_history.append(current_aug_phase)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config': {\n",
    "            'dropout': current_dropout,\n",
    "            'mixup_alpha': current_mixup_alpha if current_use_mixup else 0.0,\n",
    "            'label_smoothing': current_label_smoothing,\n",
    "            'aug_phase': current_aug_phase\n",
    "        }\n",
    "    }\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}.pth')\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Save to Google Drive every epoch\n",
    "    is_best = test_acc > best_test_acc\n",
    "    gdrive_manager.save(checkpoint_path, epoch, is_best=is_best)\n",
    "    \n",
    "    # Upload to HuggingFace every 10 epochs\n",
    "    if epoch in HF_UPLOAD_EPOCHS:\n",
    "        print(f\"\\n\ud83d\udce4 Uploading to HuggingFace (epoch {epoch})...\")\n",
    "        upload_to_huggingface(checkpoint_path, f'checkpoint_epoch{epoch}.pth', f\"Epoch {epoch}: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Upload metrics\n",
    "        metrics = {\n",
    "            'epochs': list(range(1, epoch + 1)),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'test_accuracies': test_accuracies,\n",
    "            'learning_rates': learning_rates,\n",
    "            'dropout_history': dropout_history,\n",
    "            'mixup_history': mixup_history,\n",
    "            'aug_phase_history': aug_phase_history,\n",
    "            'best_test_accuracy': best_test_acc\n",
    "        }\n",
    "        metrics_path = os.path.join(checkpoint_dir, 'metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        upload_to_huggingface(metrics_path, 'metrics.json', f\"Metrics update (epoch {epoch})\")\n",
    "    \n",
    "    # Best model handling\n",
    "    if is_best:\n",
    "        best_test_acc = test_acc\n",
    "        patience_counter = 0\n",
    "        print(f\"\\n*** NEW BEST MODEL! Test Accuracy: {best_test_acc:.2f}% ***\")\n",
    "        \n",
    "        # Upload best model to HF immediately\n",
    "        best_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "        torch.save(checkpoint, best_path)\n",
    "        upload_to_huggingface(best_path, 'best_model.pth', f\"New best: {best_test_acc:.2f}% (epoch {epoch})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    # Target reached\n",
    "    if test_acc >= 74.0:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TARGET REACHED! Test Accuracy: {test_acc:.2f}% at epoch {epoch}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Best: {best_test_acc:.2f}% | Patience: {patience_counter}/{patience}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TRAINING COMPLETED\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training Curves\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].plot(train_accuracies, label='Train', linewidth=2)\n",
    "axes[0, 0].plot(test_accuracies, label='Test', linewidth=2)\n",
    "axes[0, 0].axhline(y=74, color='r', linestyle='--', label='Target (74%)')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].set_title('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(train_losses, label='Train', linewidth=2)\n",
    "axes[0, 1].plot(test_losses, label='Test', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning Rate\n",
    "axes[0, 2].plot(learning_rates, linewidth=2, color='green')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Learning Rate')\n",
    "axes[0, 2].set_title('Learning Rate Schedule')\n",
    "axes[0, 2].grid(True)\n",
    "\n",
    "# Dropout History\n",
    "axes[1, 0].plot(dropout_history, linewidth=2, color='purple')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Dropout Rate')\n",
    "axes[1, 0].set_title('Progressive Dropout')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# MixUp History\n",
    "axes[1, 1].plot(mixup_history, linewidth=2, color='orange')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('MixUp Alpha')\n",
    "axes[1, 1].set_title('Progressive MixUp')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# Train-Test Gap\n",
    "accuracy_gap = [train - test for train, test in zip(train_accuracies, test_accuracies)]\n",
    "axes[1, 2].plot(accuracy_gap, linewidth=2, color='red')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Accuracy Gap (%)')\n",
    "axes[1, 2].set_title('Train-Test Gap (Overfitting Indicator)')\n",
    "axes[1, 2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "curves_path = os.path.join(checkpoint_dir, 'training_curves.png')\n",
    "plt.savefig(curves_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "upload_to_huggingface(curves_path, 'training_curves.png', \"Training curves\")\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "print(f\"Final Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Train-Test Gap: {accuracy_gap[-1]:.2f}%\")\n",
    "print(f\"\\nGoogle Drive checkpoints: {len(gdrive_manager.list_checkpoints())} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and final evaluation\n",
    "best_checkpoint = torch.load(os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {best_checkpoint['epoch']}\")\n",
    "print(f\"Best test accuracy: {best_checkpoint['test_accuracy']:.2f}%\")\n",
    "\n",
    "test_loss, test_acc = test(model, device, test_loader)\n",
    "print(f\"\\nFinal evaluation: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}