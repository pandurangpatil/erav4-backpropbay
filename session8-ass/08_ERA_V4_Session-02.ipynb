{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-100 Training - Optimized for 74% Accuracy with HuggingFace Integration\n",
    "\n",
    "## Key Improvements:\n",
    "- Fixed data augmentation (proper Cutout implementation)\n",
    "- Cosine annealing scheduler with warmup\n",
    "- Optimized hyperparameters (batch size 256, MixUp alpha 0.2)\n",
    "- Label smoothing (0.1)\n",
    "- Gradient clipping\n",
    "- Mixed precision training\n",
    "- Model checkpointing and early stopping\n",
    "- WideResNet-28-10 (36.5M parameters)\n",
    "- **HuggingFace Hub integration with checkpoint uploads at breakpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# CIFAR-100 Mean and Std\n",
    "cifar100_mean = (0.5071, 0.4865, 0.4409)\n",
    "cifar100_std = (0.2673, 0.2564, 0.2761)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Setup - Get token from Colab secrets\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"✓ HuggingFace token retrieved from Colab secrets\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not retrieve HF_TOKEN from secrets: {e}\")\n",
    "    print(\"Please add HF_TOKEN to Colab secrets for model upload\")\n",
    "    HF_TOKEN = None\n",
    "\n",
    "# Install huggingface_hub if needed\n",
    "try:\n",
    "    from huggingface_hub import HfApi, create_repo, upload_file\n",
    "except ImportError:\n",
    "    !pip install -q huggingface_hub\n",
    "    from huggingface_hub import HfApi, create_repo, upload_file\n",
    "\n",
    "# Configure HuggingFace repository\n",
    "REPO_ID = 'pandurangpatil/cifar100-wideresnet-session8'\n",
    "api = HfApi()\n",
    "\n",
    "# Login and create repository\n",
    "if HF_TOKEN:\n",
    "    try:\n",
    "        api.set_access_token(HF_TOKEN)\n",
    "        create_repo(repo_id=REPO_ID, repo_type=\"model\", exist_ok=True, token=HF_TOKEN)\n",
    "        print(f\"✓ Repository ready: https://huggingface.co/{REPO_ID}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not create repository: {e}\")\n",
    "else:\n",
    "    print(\"⚠ HuggingFace upload will be skipped (no token available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Data Augmentation\n",
    "class ImprovedAlbumentationsTransforms:\n",
    "    def __init__(self, mean, std):\n",
    "        self.aug = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            A.CoarseDropout(max_holes=1, max_height=8, max_width=8, p=0.5, fill_value=0),  # Cutout\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image = np.array(img)\n",
    "        return self.aug(image=image)[\"image\"]\n",
    "\n",
    "# Instantiate transforms\n",
    "train_transforms = ImprovedAlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std)\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
    "])\n",
    "\n",
    "# CIFAR-100 Dataset with increased batch size\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "# DataLoaders with batch size 256\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "cifar100_classes = datasets.CIFAR100(root='./data', train=False).classes\n",
    "print(f\"Training batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented samples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_data, batch_label = next(iter(train_loader))\n",
    "batch_data = batch_data.cpu().detach()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "for i in range(12):\n",
    "    img = batch_data[i].numpy().transpose((1, 2, 0))\n",
    "    # Denormalize\n",
    "    img = img * np.array(cifar100_std) + np.array(cifar100_mean)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Class: {cifar100_classes[batch_label[i]]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_augmentations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WideResNet Architecture\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.equalInOut = in_planes == out_planes\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "        self.shortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, 1, stride=stride, bias=False) or None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.conv1(out if self.equalInOut else x)\n",
    "        out = self.relu2(self.bn2(out))\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return out + (x if self.equalInOut else self.shortcut(x))\n",
    "\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
    "                                i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, num_classes=100, widen_factor=10, dropRate=0.3):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) // 6\n",
    "        block = BasicBlock\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = WideResNet(depth=28, widen_factor=10, dropRate=0.3, num_classes=100).to(device)\n",
    "summary(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MixUp function\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, initial_lr, target_lr, steps_per_epoch):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_epochs * steps_per_epoch\n",
    "        self.initial_lr = initial_lr\n",
    "        self.target_lr = target_lr\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def step(self):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_lr + (self.target_lr - self.initial_lr) * self.current_step / self.warmup_steps\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_step += 1\n",
    "        \n",
    "    def is_warmup(self):\n",
    "        return self.current_step < self.warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Upload Functions\n",
    "def upload_to_huggingface(file_path, path_in_repo, repo_id=REPO_ID, commit_message=\"Upload checkpoint\"):\n",
    "    \"\"\"Upload a file to HuggingFace Hub\"\"\"\n",
    "    if not HF_TOKEN:\n",
    "        print(\"⚠ Skipping upload (no HF token)\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=path_in_repo,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",\n",
    "            token=HF_TOKEN,\n",
    "            commit_message=commit_message\n",
    "        )\n",
    "        print(f\"✓ Uploaded: {path_in_repo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Upload failed for {path_in_repo}: {e}\")\n",
    "\n",
    "def save_and_upload_checkpoint(model, optimizer, epoch, train_acc, test_acc, train_loss, test_loss, \n",
    "                                checkpoint_dir, checkpoint_name, is_best=False):\n",
    "    \"\"\"Save checkpoint locally and upload to HuggingFace\"\"\"\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    \n",
    "    # Save checkpoint with metadata\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc,\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'config': {\n",
    "            'model': 'WideResNet-28-10',\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'mixup_alpha': 0.2,\n",
    "            'label_smoothing': 0.1,\n",
    "            'weight_decay': 1e-3,\n",
    "            'dropout': 0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    # Upload to HuggingFace\n",
    "    commit_msg = f\"Epoch {epoch}: Test Acc {test_acc:.2f}%\"\n",
    "    if is_best:\n",
    "        commit_msg = f\"New best model! \" + commit_msg\n",
    "    \n",
    "    upload_to_huggingface(checkpoint_path, checkpoint_name, commit_message=commit_msg)\n",
    "    \n",
    "    return checkpoint_path\n",
    "\n",
    "def create_and_upload_model_card(best_acc, total_epochs, train_accuracies, test_accuracies, checkpoint_dir):\n",
    "    \"\"\"Create and upload README.md model card\"\"\"\n",
    "    model_card = f\"\"\"---\n",
    "tags:\n",
    "- image-classification\n",
    "- cifar100\n",
    "- wideresnet\n",
    "- pytorch\n",
    "datasets:\n",
    "- cifar100\n",
    "metrics:\n",
    "- accuracy\n",
    "---\n",
    "\n",
    "# CIFAR-100 WideResNet-28-10\n",
    "\n",
    "## Model Description\n",
    "\n",
    "WideResNet-28-10 trained on CIFAR-100 dataset with advanced augmentation techniques.\n",
    "\n",
    "### Model Architecture\n",
    "- **Architecture**: WideResNet-28-10\n",
    "- **Parameters**: 36.5M\n",
    "- **Depth**: 28 layers\n",
    "- **Width Factor**: 10\n",
    "- **Dropout**: 0.3\n",
    "\n",
    "### Training Configuration\n",
    "- **Batch Size**: 256\n",
    "- **Optimizer**: SGD (momentum=0.9, weight_decay=1e-3)\n",
    "- **Learning Rate**: Cosine annealing with warmup (0.01→0.1, min=1e-4)\n",
    "- **Scheduler**: CosineAnnealingWarmRestarts (T_0=25)\n",
    "- **Augmentation**: HorizontalFlip, ShiftScaleRotate, Cutout, ColorJitter\n",
    "- **MixUp**: Alpha=0.2\n",
    "- **Label Smoothing**: 0.1\n",
    "- **Mixed Precision**: Enabled\n",
    "- **Gradient Clipping**: 1.0\n",
    "\n",
    "### Performance\n",
    "- **Best Test Accuracy**: {best_acc:.2f}%\n",
    "- **Total Epochs Trained**: {total_epochs}\n",
    "- **Final Train Accuracy**: {train_accuracies[-1]:.2f}%\n",
    "- **Final Test Accuracy**: {test_accuracies[-1]:.2f}%\n",
    "\n",
    "### Available Checkpoints\n",
    "- `best_model.pth` - Best performing model\n",
    "- `checkpoint_epoch10.pth` - Epoch 10 checkpoint\n",
    "- `checkpoint_epoch25.pth` - Epoch 25 checkpoint (end of cycle 1)\n",
    "- `checkpoint_epoch50.pth` - Epoch 50 checkpoint (mid-training)\n",
    "- `checkpoint_epoch75.pth` - Epoch 75 checkpoint (late training)\n",
    "- `final_model.pth` - Final epoch model\n",
    "\n",
    "### Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download model\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=\"{REPO_ID}\",\n",
    "    filename=\"best_model.pth\"\n",
    ")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "# Load model (define WideResNet class first)\n",
    "model = WideResNet(depth=28, widen_factor=10, num_classes=100)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "### Training Details\n",
    "- **Dataset**: CIFAR-100 (50,000 train, 10,000 test)\n",
    "- **Classes**: 100\n",
    "- **Image Size**: 32×32\n",
    "- **Normalization**: mean=(0.5071, 0.4865, 0.4409), std=(0.2673, 0.2564, 0.2761)\n",
    "\n",
    "### Files\n",
    "- `training_curves.png` - Training/test accuracy and loss curves\n",
    "- `metrics.json` - Complete training history\n",
    "- `config.json` - Hyperparameter configuration\n",
    "\n",
    "### License\n",
    "MIT\n",
    "\n",
    "### Citation\n",
    "```bibtex\n",
    "@misc{{wideresnet-cifar100,\n",
    "  author = {{Pandurang Patil}},\n",
    "  title = {{CIFAR-100 WideResNet-28-10}},\n",
    "  year = {{2025}},\n",
    "  publisher = {{HuggingFace}},\n",
    "  url = {{https://huggingface.co/{REPO_ID}}}\n",
    "}}\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    # Save README locally\n",
    "    readme_path = os.path.join(checkpoint_dir, 'README.md')\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(model_card)\n",
    "    \n",
    "    # Upload to HuggingFace\n",
    "    upload_to_huggingface(readme_path, 'README.md', commit_message=\"Update model card\")\n",
    "    print(\"✓ Model card created and uploaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision and gradient clipping\n",
    "def train(model, device, train_loader, optimizer, scheduler, warmup_scheduler, scaler, epoch, \n",
    "          use_mixup=True, mixup_alpha=0.2, label_smoothing=0.1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            if use_mixup:\n",
    "                inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
    "                outputs = model(inputs)\n",
    "                loss = lam * F.cross_entropy(outputs, targets_a, label_smoothing=label_smoothing) + \\\n",
    "                       (1 - lam) * F.cross_entropy(outputs, targets_b, label_smoothing=label_smoothing)\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "                loss = F.cross_entropy(outputs, target, label_smoothing=label_smoothing)\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update learning rate\n",
    "        if warmup_scheduler.is_warmup():\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Accuracy tracking\n",
    "        _, pred = outputs.max(1)\n",
    "        if use_mixup:\n",
    "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
    "        else:\n",
    "            correct += pred.eq(target).sum().item()\n",
    "        processed += len(data)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100*correct/processed:.2f}% LR={current_lr:.6f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / processed\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n\")\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "print(\"=\" * 70)\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: WideResNet-28-10 (36.5M parameters)\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"MixUp Alpha: 0.2\")\n",
    "print(f\"Label Smoothing: 0.1\")\n",
    "print(f\"Weight Decay: 1e-3\")\n",
    "print(f\"Gradient Clipping: 1.0\")\n",
    "print(f\"Scheduler: CosineAnnealingWarmRestarts\")\n",
    "print(f\"Warmup: 5 epochs (0.01 -> 0.1)\")\n",
    "print(f\"Mixed Precision: Enabled\")\n",
    "print(f\"HuggingFace Upload: {'Enabled' if HF_TOKEN else 'Disabled'}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "EPOCHS = 100\n",
    "WARMUP_EPOCHS = 5\n",
    "INITIAL_LR = 0.01\n",
    "MAX_LR = 0.1\n",
    "MIN_LR = 1e-4\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=0.9, weight_decay=1e-3)\n",
    "\n",
    "# Cosine annealing with warm restarts (T_0 = 25 epochs per cycle)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=25, T_mult=1, eta_min=MIN_LR)\n",
    "\n",
    "# Warmup scheduler\n",
    "warmup_scheduler = WarmupScheduler(optimizer, WARMUP_EPOCHS, INITIAL_LR, MAX_LR, len(train_loader))\n",
    "\n",
    "# Gradient scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Tracking variables\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "learning_rates = []\n",
    "\n",
    "# Early stopping and checkpointing\n",
    "best_test_acc = 0.0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define checkpoint breakpoints\n",
    "CHECKPOINT_EPOCHS = [10, 20, 25, 30, 40, 50, 60, 75, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with HuggingFace Checkpoint Uploads\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train\n",
    "    train_loss, train_acc = train(\n",
    "        model, device, train_loader, optimizer, scheduler, warmup_scheduler, scaler, epoch,\n",
    "        use_mixup=True, mixup_alpha=0.2, label_smoothing=0.1\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    \n",
    "    # Record metrics\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save and upload best model\n",
    "        print(f\"*** New best model! Test Accuracy: {best_test_acc:.2f}% ***\")\n",
    "        save_and_upload_checkpoint(\n",
    "            model, optimizer, epoch, train_acc, test_acc, train_loss, test_loss,\n",
    "            checkpoint_dir, 'best_model.pth', is_best=True\n",
    "        )\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Upload checkpoint at breakpoints\n",
    "    if epoch in CHECKPOINT_EPOCHS:\n",
    "        checkpoint_name = f'checkpoint_epoch{epoch}.pth'\n",
    "        print(f\"📍 Breakpoint checkpoint at epoch {epoch}\")\n",
    "        save_and_upload_checkpoint(\n",
    "            model, optimizer, epoch, train_acc, test_acc, train_loss, test_loss,\n",
    "            checkpoint_dir, checkpoint_name\n",
    "        )\n",
    "    \n",
    "    # Save metrics periodically\n",
    "    if epoch % 10 == 0 or epoch in CHECKPOINT_EPOCHS:\n",
    "        metrics = {\n",
    "            'epochs': list(range(1, epoch + 1)),\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'train_accuracies': train_accuracies,\n",
    "            'test_accuracies': test_accuracies,\n",
    "            'learning_rates': learning_rates,\n",
    "            'best_test_accuracy': best_test_acc\n",
    "        }\n",
    "        metrics_path = os.path.join(checkpoint_dir, 'metrics.json')\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        \n",
    "        upload_to_huggingface(metrics_path, 'metrics.json', commit_message=f\"Update metrics (epoch {epoch})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch}. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "    \n",
    "    # Check if target reached\n",
    "    if test_acc >= 74.0:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"Target accuracy of 74% reached at epoch {epoch}!\")\n",
    "        print(f\"Final test accuracy: {test_acc:.2f}%\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Best Test Accuracy so far: {best_test_acc:.2f}% | Patience: {patience_counter}/{patience}\\n\")\n",
    "\n",
    "# Save final model\n",
    "print(\"\\n📦 Saving final model...\")\n",
    "save_and_upload_checkpoint(\n",
    "    model, optimizer, epoch, train_acc, test_acc, train_loss, test_loss,\n",
    "    checkpoint_dir, 'final_model.pth'\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed. Best test accuracy: {best_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(test_losses, label='Test Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Test Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0, 1].plot(train_accuracies, label='Train Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(test_accuracies, label='Test Accuracy', linewidth=2)\n",
    "axes[0, 1].axhline(y=74, color='r', linestyle='--', label='Target (74%)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Training and Test Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 0].plot(learning_rates, linewidth=2, color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Gap between train and test accuracy (overfitting indicator)\n",
    "accuracy_gap = [train - test for train, test in zip(train_accuracies, test_accuracies)]\n",
    "axes[1, 1].plot(accuracy_gap, linewidth=2, color='orange')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy Gap (%)')\n",
    "axes[1, 1].set_title('Train-Test Accuracy Gap (Overfitting Indicator)')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "curves_path = os.path.join(checkpoint_dir, 'training_curves.png')\n",
    "plt.savefig(curves_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Upload training curves\n",
    "upload_to_huggingface(curves_path, 'training_curves.png', commit_message=\"Upload training curves\")\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "print(f\"Final Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Train-Test Gap: {accuracy_gap[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload model card\n",
    "create_and_upload_model_card(\n",
    "    best_test_acc, \n",
    "    len(train_accuracies), \n",
    "    train_accuracies, \n",
    "    test_accuracies, \n",
    "    checkpoint_dir\n",
    ")\n",
    "\n",
    "# Save final config\n",
    "config = {\n",
    "    'model': 'WideResNet-28-10',\n",
    "    'depth': 28,\n",
    "    'widen_factor': 10,\n",
    "    'dropout': 0.3,\n",
    "    'num_classes': 100,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': len(train_accuracies),\n",
    "    'optimizer': 'SGD',\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-3,\n",
    "    'initial_lr': INITIAL_LR,\n",
    "    'max_lr': MAX_LR,\n",
    "    'min_lr': MIN_LR,\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts',\n",
    "    'T_0': 25,\n",
    "    'warmup_epochs': WARMUP_EPOCHS,\n",
    "    'mixup_alpha': 0.2,\n",
    "    'label_smoothing': 0.1,\n",
    "    'gradient_clipping': 1.0,\n",
    "    'mixed_precision': True,\n",
    "    'best_test_accuracy': best_test_acc,\n",
    "    'final_train_accuracy': train_accuracies[-1],\n",
    "    'final_test_accuracy': test_accuracies[-1]\n",
    "}\n",
    "\n",
    "config_path = os.path.join(checkpoint_dir, 'config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "upload_to_huggingface(config_path, 'config.json', commit_message=\"Upload training configuration\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"All files uploaded to: https://huggingface.co/{REPO_ID}\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} with test accuracy: {checkpoint['test_accuracy']:.2f}%\")\n",
    "\n",
    "# Final evaluation\n",
    "test_loss, test_acc = test(model, device, test_loader)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download and resume from checkpoint\n",
    "# Uncomment to use\n",
    "\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# # Download a specific checkpoint\n",
    "# checkpoint_path = hf_hub_download(\n",
    "#     repo_id=REPO_ID,\n",
    "#     filename=\"checkpoint_epoch50.pth\",\n",
    "#     token=HF_TOKEN\n",
    "# )\n",
    "\n",
    "# # Load checkpoint\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# print(f\"Resumed from epoch {checkpoint['epoch']} with test accuracy {checkpoint['test_accuracy']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
