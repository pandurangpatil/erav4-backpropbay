{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "from torchsummary import summary\n",
        "\n",
        "# CIFAR-100 Mean and Std\n",
        "cifar100_mean = (0.5071, 0.4865, 0.4409)\n",
        "cifar100_std = (0.2673, 0.2564, 0.2761)\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "jr2KC0BGbtME",
        "outputId": "81c434c6-4580-4e3b-816a-00b423d2441f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e548edab-d2c4-4967-cbb3-4a0d7a01616c"
      },
      "source": [
        "\n",
        "\n",
        "# Custom Albumentations Transform Wrapper\n",
        "class AlbumentationsTransforms:\n",
        "    def __init__(self, mean, std):\n",
        "        self.aug = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "            A.Affine(translate_percent=0.0625, scale=(0.9, 1.1), rotate=(-7, 7), p=0.5),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),\n",
        "            A.RandomCrop(height=28, width=28, p=0.5),\n",
        "            A.Resize(height=32, width=32),\n",
        "            A.CoarseDropout(\n",
        "                max_holes=1,\n",
        "                max_height=16, max_width=16,\n",
        "                fill_value=tuple(int(m * 255) for m in mean),  # Convert mean to 0-255\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.Normalize(mean=mean, std=std),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        image = np.array(img)\n",
        "        return self.aug(image=image)[\"image\"]\n",
        "\n",
        "# Instantiate transforms\n",
        "train_transforms = AlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std)\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
        "])\n",
        "\n",
        "# CIFAR-100 Dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transforms)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=5, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=5, pin_memory=True)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2303470777.py:12: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(\n",
            "100%|██████████| 169M/169M [00:03<00:00, 45.3MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Get a batch\n",
        "batch_data, batch_label = next(iter(train_loader))\n",
        "\n",
        "# Move to CPU and detach the computation graph\n",
        "batch_data = batch_data.cpu().detach()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(12):\n",
        "    img = batch_data[i]  # shape: [3, 32, 32]\n",
        "    img = img.numpy().transpose((1, 2, 0))  # to shape [32, 32, 3]\n",
        "    img = np.clip(img, 0, 1)  # ensure valid range for display\n",
        "\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Label: {batch_label[i].item()}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "7MwdNs7Nrngt",
        "outputId": "2a5a1a89-5b90-401e-8e04-395b11687137"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 12 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Load ResNet18 without pretrained weights\n",
        "model = resnet18(weights=None, num_classes=100).to(device)\n",
        "\n",
        "# Modify the final fully connected layer for CIFAR-100 (100 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the summary (for CIFAR-100 input size)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_JMSwm0vQiV",
        "outputId": "e1459331-a7b9-43f1-9034-bd0431d7ac1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
            "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
            "             ReLU-26            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
            "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
            "             ReLU-30            [-1, 128, 4, 4]               0\n",
            "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
            "             ReLU-37            [-1, 256, 2, 2]               0\n",
            "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
            "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
            "             ReLU-42            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
            "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
            "             ReLU-46            [-1, 256, 2, 2]               0\n",
            "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
            "             ReLU-49            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
            "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-53            [-1, 512, 1, 1]               0\n",
            "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
            "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-58            [-1, 512, 1, 1]               0\n",
            "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
            "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-62            [-1, 512, 1, 1]               0\n",
            "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-65            [-1, 512, 1, 1]               0\n",
            "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 11,227,812\n",
            "Trainable params: 11,227,812\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.29\n",
            "Params size (MB): 42.83\n",
            "Estimated Total Size (MB): 44.13\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.0\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n"
      ],
      "metadata": {
        "id": "W8kyBS2C8iKL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Training & Testing\n",
        "# ------------------------------\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True, mixup_alpha=0.4):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_mixup:\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
        "            outputs = model(inputs)\n",
        "            loss = lam * F.cross_entropy(outputs, targets_a) + (1 - lam) * F.cross_entropy(outputs, targets_b)\n",
        "        else:\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Accuracy tracking (still approximate when using MixUp)\n",
        "        _, pred = outputs.max(1)\n",
        "        if use_mixup:\n",
        "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
        "        else:\n",
        "            correct += pred.eq(target).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100*correct/processed:.2f}\")\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({acc:.2f}%)\\n\")\n",
        "    return acc\n",
        "\n",
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS = 40\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xY1f5OOvvaAG",
        "outputId": "3ac4ea06-8d71-42bb-811f-ea833d9955e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=3.4221 Acc=14.92: 100%|██████████| 391/391 [00:30<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2998, Accuracy: 2028/10000 (20.28%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=3.4910 Acc=18.87: 100%|██████████| 391/391 [00:30<00:00, 12.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.1236, Accuracy: 2349/10000 (23.49%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.3535 Acc=22.10: 100%|██████████| 391/391 [00:30<00:00, 12.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0261, Accuracy: 2502/10000 (25.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.2286 Acc=24.60: 100%|██████████| 391/391 [00:32<00:00, 11.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9580, Accuracy: 2744/10000 (27.44%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=2.6924 Acc=27.50: 100%|██████████| 391/391 [00:30<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7870, Accuracy: 2991/10000 (29.91%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=2.7445 Acc=30.46: 100%|██████████| 391/391 [00:30<00:00, 12.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6035, Accuracy: 3402/10000 (34.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=2.4460 Acc=32.97: 100%|██████████| 391/391 [00:30<00:00, 12.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4771, Accuracy: 3616/10000 (36.16%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=2.3339 Acc=35.59: 100%|██████████| 391/391 [00:31<00:00, 12.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3704, Accuracy: 3859/10000 (38.59%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=2.5520 Acc=38.20: 100%|██████████| 391/391 [00:30<00:00, 12.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3657, Accuracy: 3872/10000 (38.72%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=2.2250 Acc=40.52: 100%|██████████| 391/391 [00:30<00:00, 13.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2985, Accuracy: 4087/10000 (40.87%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.3408 Acc=42.45: 100%|██████████| 391/391 [00:29<00:00, 13.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1994, Accuracy: 4266/10000 (42.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=2.0510 Acc=44.70: 100%|██████████| 391/391 [00:30<00:00, 13.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1202, Accuracy: 4453/10000 (44.53%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=2.1098 Acc=46.50: 100%|██████████| 391/391 [00:30<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1382, Accuracy: 4423/10000 (44.23%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=2.0236 Acc=48.29: 100%|██████████| 391/391 [00:30<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1053, Accuracy: 4550/10000 (45.50%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=1.8044 Acc=49.52:  64%|██████▍   | 251/391 [00:20<00:11, 12.30it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3846335632.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3846335632.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Training & Testing\n",
        "# ------------------------------\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True, mixup_alpha=0.4):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_mixup:\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
        "            outputs = model(inputs)\n",
        "            loss = lam * F.cross_entropy(outputs, targets_a) + (1 - lam) * F.cross_entropy(outputs, targets_b)\n",
        "        else:\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Accuracy tracking (still approximate when using MixUp)\n",
        "        _, pred = outputs.max(1)\n",
        "        if use_mixup:\n",
        "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
        "        else:\n",
        "            correct += pred.eq(target).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100*correct/processed:.2f}\")\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({acc:.2f}%)\\n\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "33CJfErigRHM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS = 40\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zXkvj1v88YJ",
        "outputId": "2e06218c-dcfb-4120-8522-56fc2ae08839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.1080 Acc=4.93: 100%|██████████| 391/391 [00:35<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.8700, Accuracy: 1146/10000 (11.46%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=3.7708 Acc=8.33: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.6858, Accuracy: 1371/10000 (13.71%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.8650 Acc=10.80: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4331, Accuracy: 1931/10000 (19.31%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.4393 Acc=13.67: 100%|██████████| 391/391 [00:33<00:00, 11.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2391, Accuracy: 2208/10000 (22.08%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=3.2511 Acc=15.54: 100%|██████████| 391/391 [00:33<00:00, 11.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0029, Accuracy: 2696/10000 (26.96%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=4.1449 Acc=17.31: 100%|██████████| 391/391 [00:33<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.1769, Accuracy: 2445/10000 (24.45%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=3.7653 Acc=19.57: 100%|██████████| 391/391 [00:33<00:00, 11.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9067, Accuracy: 2835/10000 (28.35%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=2.9900 Acc=21.15: 100%|██████████| 391/391 [00:33<00:00, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7548, Accuracy: 3121/10000 (31.21%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.0817 Acc=23.03: 100%|██████████| 391/391 [00:33<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6914, Accuracy: 3391/10000 (33.91%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=3.1194 Acc=24.09: 100%|██████████| 391/391 [00:33<00:00, 11.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6683, Accuracy: 3266/10000 (32.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.6397 Acc=25.52: 100%|██████████| 391/391 [00:33<00:00, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5892, Accuracy: 3535/10000 (35.35%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=3.4141 Acc=27.16: 100%|██████████| 391/391 [00:34<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5892, Accuracy: 3435/10000 (34.35%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=3.0038 Acc=28.26: 100%|██████████| 391/391 [00:34<00:00, 11.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5131, Accuracy: 3670/10000 (36.70%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=2.5250 Acc=30.06: 100%|██████████| 391/391 [00:34<00:00, 11.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4048, Accuracy: 3911/10000 (39.11%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=3.3204 Acc=29.31: 100%|██████████| 391/391 [00:33<00:00, 11.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3810, Accuracy: 3953/10000 (39.53%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=2.6870 Acc=31.37: 100%|██████████| 391/391 [00:33<00:00, 11.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2785, Accuracy: 4209/10000 (42.09%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss=2.0350 Acc=32.29: 100%|██████████| 391/391 [00:34<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2638, Accuracy: 4195/10000 (41.95%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss=3.5353 Acc=31.85: 100%|██████████| 391/391 [00:37<00:00, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2711, Accuracy: 4211/10000 (42.11%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss=2.3673 Acc=34.05: 100%|██████████| 391/391 [00:34<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2630, Accuracy: 4185/10000 (41.85%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss=2.4790 Acc=35.21: 100%|██████████| 391/391 [00:33<00:00, 11.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2004, Accuracy: 4419/10000 (44.19%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss=2.1419 Acc=34.65: 100%|██████████| 391/391 [00:34<00:00, 11.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1757, Accuracy: 4424/10000 (44.24%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss=2.7917 Acc=36.62: 100%|██████████| 391/391 [00:33<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1677, Accuracy: 4446/10000 (44.46%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss=2.2937 Acc=36.55: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1410, Accuracy: 4523/10000 (45.23%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss=3.8063 Acc=37.75: 100%|██████████| 391/391 [00:34<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1330, Accuracy: 4539/10000 (45.39%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss=2.1359 Acc=38.65: 100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0854, Accuracy: 4642/10000 (46.42%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss=2.4483 Acc=39.53: 100%|██████████| 391/391 [00:34<00:00, 11.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0427, Accuracy: 4759/10000 (47.59%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss=3.5080 Acc=39.19: 100%|██████████| 391/391 [00:33<00:00, 11.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0551, Accuracy: 4742/10000 (47.42%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss=3.5178 Acc=41.78: 100%|██████████| 391/391 [00:33<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9689, Accuracy: 4921/10000 (49.21%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Loss=2.9985 Acc=42.47: 100%|██████████| 391/391 [00:34<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0152, Accuracy: 4847/10000 (48.47%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Loss=2.8367 Acc=43.20: 100%|██████████| 391/391 [00:35<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9498, Accuracy: 4943/10000 (49.43%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Loss=2.3514 Acc=42.87: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0660, Accuracy: 4857/10000 (48.57%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Loss=1.5283 Acc=45.01: 100%|██████████| 391/391 [00:33<00:00, 11.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9240, Accuracy: 5052/10000 (50.52%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Loss=1.6636 Acc=47.21: 100%|██████████| 391/391 [00:33<00:00, 11.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9262, Accuracy: 5089/10000 (50.89%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Loss=2.5289 Acc=45.95: 100%|██████████| 391/391 [00:35<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8795, Accuracy: 5163/10000 (51.63%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Loss=3.3824 Acc=48.72: 100%|██████████| 391/391 [00:35<00:00, 11.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8244, Accuracy: 5267/10000 (52.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Loss=3.0239 Acc=50.60: 100%|██████████| 391/391 [00:34<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8451, Accuracy: 5234/10000 (52.34%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Loss=2.1382 Acc=52.40: 100%|██████████| 391/391 [00:34<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8164, Accuracy: 5307/10000 (53.07%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Loss=1.9740 Acc=53.36: 100%|██████████| 391/391 [00:33<00:00, 11.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7984, Accuracy: 5315/10000 (53.15%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Loss=3.7840 Acc=52.19: 100%|██████████| 391/391 [00:35<00:00, 10.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8411, Accuracy: 5287/10000 (52.87%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Loss=1.4055 Acc=53.01: 100%|██████████| 391/391 [00:35<00:00, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8010, Accuracy: 5344/10000 (53.44%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Load ResNet18 without pretrained weights\n",
        "model = resnet34(weights=None, num_classes=100).to(device)\n",
        "\n",
        "# Modify the final fully connected layer for CIFAR-100 (100 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the summary (for CIFAR-100 input size)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS = 40\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "qH4Wf9ikkJ_G",
        "outputId": "7b5f549e-67c0-4466-d7ba-4e31484627e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-25             [-1, 64, 8, 8]               0\n",
            "           Conv2d-26            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-27            [-1, 128, 4, 4]             256\n",
            "             ReLU-28            [-1, 128, 4, 4]               0\n",
            "           Conv2d-29            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-30            [-1, 128, 4, 4]             256\n",
            "           Conv2d-31            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-36            [-1, 128, 4, 4]             256\n",
            "             ReLU-37            [-1, 128, 4, 4]               0\n",
            "           Conv2d-38            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-39            [-1, 128, 4, 4]             256\n",
            "             ReLU-40            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-41            [-1, 128, 4, 4]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "             ReLU-44            [-1, 128, 4, 4]               0\n",
            "           Conv2d-45            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-46            [-1, 128, 4, 4]             256\n",
            "             ReLU-47            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-48            [-1, 128, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-55            [-1, 128, 4, 4]               0\n",
            "           Conv2d-56            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-57            [-1, 256, 2, 2]             512\n",
            "             ReLU-58            [-1, 256, 2, 2]               0\n",
            "           Conv2d-59            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 2, 2]             512\n",
            "           Conv2d-61            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-62            [-1, 256, 2, 2]             512\n",
            "             ReLU-63            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-64            [-1, 256, 2, 2]               0\n",
            "           Conv2d-65            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-66            [-1, 256, 2, 2]             512\n",
            "             ReLU-67            [-1, 256, 2, 2]               0\n",
            "           Conv2d-68            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-69            [-1, 256, 2, 2]             512\n",
            "             ReLU-70            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-71            [-1, 256, 2, 2]               0\n",
            "           Conv2d-72            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-73            [-1, 256, 2, 2]             512\n",
            "             ReLU-74            [-1, 256, 2, 2]               0\n",
            "           Conv2d-75            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-76            [-1, 256, 2, 2]             512\n",
            "             ReLU-77            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-78            [-1, 256, 2, 2]               0\n",
            "           Conv2d-79            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-80            [-1, 256, 2, 2]             512\n",
            "             ReLU-81            [-1, 256, 2, 2]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-85            [-1, 256, 2, 2]               0\n",
            "           Conv2d-86            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-87            [-1, 256, 2, 2]             512\n",
            "             ReLU-88            [-1, 256, 2, 2]               0\n",
            "           Conv2d-89            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-90            [-1, 256, 2, 2]             512\n",
            "             ReLU-91            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-92            [-1, 256, 2, 2]               0\n",
            "           Conv2d-93            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-94            [-1, 256, 2, 2]             512\n",
            "             ReLU-95            [-1, 256, 2, 2]               0\n",
            "           Conv2d-96            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-97            [-1, 256, 2, 2]             512\n",
            "             ReLU-98            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-99            [-1, 256, 2, 2]               0\n",
            "          Conv2d-100            [-1, 512, 1, 1]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-102            [-1, 512, 1, 1]               0\n",
            "          Conv2d-103            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 1, 1]           1,024\n",
            "          Conv2d-105            [-1, 512, 1, 1]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-107            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-108            [-1, 512, 1, 1]               0\n",
            "          Conv2d-109            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-111            [-1, 512, 1, 1]               0\n",
            "          Conv2d-112            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-114            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-115            [-1, 512, 1, 1]               0\n",
            "          Conv2d-116            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-118            [-1, 512, 1, 1]               0\n",
            "          Conv2d-119            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-121            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-122            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "          Linear-124                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 21,335,972\n",
            "Trainable params: 21,335,972\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.97\n",
            "Params size (MB): 81.39\n",
            "Estimated Total Size (MB): 83.37\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.4827 Acc=3.83: 100%|██████████| 391/391 [00:39<00:00,  9.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 4.1112, Accuracy: 856/10000 (8.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=4.6035 Acc=6.69: 100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.9845, Accuracy: 1241/10000 (12.41%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.6625 Acc=8.88: 100%|██████████| 391/391 [00:39<00:00,  9.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.7173, Accuracy: 1471/10000 (14.71%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=4.0104 Acc=11.02: 100%|██████████| 391/391 [00:39<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.5019, Accuracy: 1825/10000 (18.25%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=3.3945 Acc=13.36: 100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4572, Accuracy: 2072/10000 (20.72%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=3.4253 Acc=14.88: 100%|██████████| 391/391 [00:40<00:00,  9.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0861, Accuracy: 2471/10000 (24.71%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=3.4863 Acc=16.61: 100%|██████████| 391/391 [00:39<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2807, Accuracy: 2569/10000 (25.69%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=3.4991 Acc=18.12: 100%|██████████| 391/391 [00:40<00:00,  9.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9569, Accuracy: 2661/10000 (26.61%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.2083 Acc=19.63: 100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9831, Accuracy: 2732/10000 (27.32%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=3.6236 Acc=19.59: 100%|██████████| 391/391 [00:39<00:00,  9.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9725, Accuracy: 2794/10000 (27.94%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=3.0361 Acc=21.70: 100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7515, Accuracy: 3199/10000 (31.99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=2.8610 Acc=23.21: 100%|██████████| 391/391 [00:39<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8382, Accuracy: 3185/10000 (31.85%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=2.6759 Acc=24.88: 100%|██████████| 391/391 [00:39<00:00,  9.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7116, Accuracy: 3302/10000 (33.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=3.2159 Acc=24.52: 100%|██████████| 391/391 [00:39<00:00,  9.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6344, Accuracy: 3486/10000 (34.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=2.8031 Acc=26.30: 100%|██████████| 391/391 [00:39<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5478, Accuracy: 3614/10000 (36.14%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=2.4857 Acc=26.85: 100%|██████████| 391/391 [00:40<00:00,  9.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4559, Accuracy: 3714/10000 (37.14%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss=2.7070 Acc=28.73: 100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3927, Accuracy: 3927/10000 (39.27%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss=3.4180 Acc=29.12: 100%|██████████| 391/391 [00:39<00:00,  9.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4061, Accuracy: 4018/10000 (40.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss=3.7059 Acc=30.07: 100%|██████████| 391/391 [00:39<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3484, Accuracy: 4032/10000 (40.32%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss=3.2391 Acc=30.30: 100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4004, Accuracy: 3938/10000 (39.38%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss=2.7848 Acc=31.44: 100%|██████████| 391/391 [00:39<00:00,  9.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2915, Accuracy: 4156/10000 (41.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss=3.8998 Acc=28.61: 100%|██████████| 391/391 [00:39<00:00,  9.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4277, Accuracy: 3855/10000 (38.55%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss=2.7942 Acc=30.66: 100%|██████████| 391/391 [00:40<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5103, Accuracy: 3768/10000 (37.68%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss=2.6582 Acc=31.77: 100%|██████████| 391/391 [00:39<00:00,  9.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3494, Accuracy: 4086/10000 (40.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss=3.1091 Acc=33.37: 100%|██████████| 391/391 [00:40<00:00,  9.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2785, Accuracy: 4204/10000 (42.04%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss=3.1837 Acc=35.63: 100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1604, Accuracy: 4483/10000 (44.83%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss=2.1554 Acc=35.23: 100%|██████████| 391/391 [00:40<00:00,  9.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1305, Accuracy: 4489/10000 (44.89%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss=3.1482 Acc=36.86: 100%|██████████| 391/391 [00:40<00:00,  9.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1319, Accuracy: 4561/10000 (45.61%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Loss=2.4366 Acc=36.86: 100%|██████████| 391/391 [00:40<00:00,  9.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1457, Accuracy: 4558/10000 (45.58%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Loss=1.7643 Acc=39.06: 100%|██████████| 391/391 [00:41<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0669, Accuracy: 4653/10000 (46.53%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Loss=3.3755 Acc=39.15: 100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0430, Accuracy: 4758/10000 (47.58%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Loss=2.5172 Acc=40.66: 100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0140, Accuracy: 4856/10000 (48.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Loss=1.9585 Acc=41.54: 100%|██████████| 391/391 [00:39<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0351, Accuracy: 4855/10000 (48.55%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Loss=3.9218 Acc=42.01: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0098, Accuracy: 4784/10000 (47.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Loss=3.9070 Acc=44.67: 100%|██████████| 391/391 [00:39<00:00,  9.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9673, Accuracy: 4930/10000 (49.30%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Loss=1.7384 Acc=44.02: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9282, Accuracy: 5067/10000 (50.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Loss=2.1711 Acc=45.93: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8829, Accuracy: 5118/10000 (51.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Loss=1.4311 Acc=46.15: 100%|██████████| 391/391 [00:39<00:00,  9.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9013, Accuracy: 5101/10000 (51.01%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Loss=2.9695 Acc=46.72: 100%|██████████| 391/391 [00:40<00:00,  9.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9120, Accuracy: 5087/10000 (50.87%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Loss=1.8613 Acc=48.24: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8892, Accuracy: 5117/10000 (51.17%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Load ResNet34 without pretrained weights\n",
        "model = resnet34(weights=None, num_classes=100).to(device)\n",
        "\n",
        "# Modify the final fully connected layer for CIFAR-100 (100 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the summary (for CIFAR-100 input size)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "EPOCHS = 100\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "zxEJColutNuL",
        "outputId": "1a30e22e-bca6-4959-e5ce-aaca7d8d4066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-25             [-1, 64, 8, 8]               0\n",
            "           Conv2d-26            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-27            [-1, 128, 4, 4]             256\n",
            "             ReLU-28            [-1, 128, 4, 4]               0\n",
            "           Conv2d-29            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-30            [-1, 128, 4, 4]             256\n",
            "           Conv2d-31            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-36            [-1, 128, 4, 4]             256\n",
            "             ReLU-37            [-1, 128, 4, 4]               0\n",
            "           Conv2d-38            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-39            [-1, 128, 4, 4]             256\n",
            "             ReLU-40            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-41            [-1, 128, 4, 4]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "             ReLU-44            [-1, 128, 4, 4]               0\n",
            "           Conv2d-45            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-46            [-1, 128, 4, 4]             256\n",
            "             ReLU-47            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-48            [-1, 128, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-55            [-1, 128, 4, 4]               0\n",
            "           Conv2d-56            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-57            [-1, 256, 2, 2]             512\n",
            "             ReLU-58            [-1, 256, 2, 2]               0\n",
            "           Conv2d-59            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 2, 2]             512\n",
            "           Conv2d-61            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-62            [-1, 256, 2, 2]             512\n",
            "             ReLU-63            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-64            [-1, 256, 2, 2]               0\n",
            "           Conv2d-65            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-66            [-1, 256, 2, 2]             512\n",
            "             ReLU-67            [-1, 256, 2, 2]               0\n",
            "           Conv2d-68            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-69            [-1, 256, 2, 2]             512\n",
            "             ReLU-70            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-71            [-1, 256, 2, 2]               0\n",
            "           Conv2d-72            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-73            [-1, 256, 2, 2]             512\n",
            "             ReLU-74            [-1, 256, 2, 2]               0\n",
            "           Conv2d-75            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-76            [-1, 256, 2, 2]             512\n",
            "             ReLU-77            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-78            [-1, 256, 2, 2]               0\n",
            "           Conv2d-79            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-80            [-1, 256, 2, 2]             512\n",
            "             ReLU-81            [-1, 256, 2, 2]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-85            [-1, 256, 2, 2]               0\n",
            "           Conv2d-86            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-87            [-1, 256, 2, 2]             512\n",
            "             ReLU-88            [-1, 256, 2, 2]               0\n",
            "           Conv2d-89            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-90            [-1, 256, 2, 2]             512\n",
            "             ReLU-91            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-92            [-1, 256, 2, 2]               0\n",
            "           Conv2d-93            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-94            [-1, 256, 2, 2]             512\n",
            "             ReLU-95            [-1, 256, 2, 2]               0\n",
            "           Conv2d-96            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-97            [-1, 256, 2, 2]             512\n",
            "             ReLU-98            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-99            [-1, 256, 2, 2]               0\n",
            "          Conv2d-100            [-1, 512, 1, 1]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-102            [-1, 512, 1, 1]               0\n",
            "          Conv2d-103            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 1, 1]           1,024\n",
            "          Conv2d-105            [-1, 512, 1, 1]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-107            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-108            [-1, 512, 1, 1]               0\n",
            "          Conv2d-109            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-111            [-1, 512, 1, 1]               0\n",
            "          Conv2d-112            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-114            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-115            [-1, 512, 1, 1]               0\n",
            "          Conv2d-116            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-118            [-1, 512, 1, 1]               0\n",
            "          Conv2d-119            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-121            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-122            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "          Linear-124                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 21,335,972\n",
            "Trainable params: 21,335,972\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.97\n",
            "Params size (MB): 81.39\n",
            "Estimated Total Size (MB): 83.37\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.6701 Acc=1.42:  14%|█▍        | 54/391 [00:09<00:57,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3436661336.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-174365245.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, scheduler, epoch, use_mixup, mixup_alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixup_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2815\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2818\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2819\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvements to Reach 74% Test Accuracy\n",
        "\n",
        "## Analysis of Previous Runs:\n",
        "- **40 epochs run (cell-5)**: Peak test accuracy 53.44% without proper MixUp training\n",
        "- **150 epochs run (new_trial)**: Peak test accuracy 55.78% but severe overfitting (train 73%+ vs test 55%)\n",
        "\n",
        "## Key Issues Identified:\n",
        "1. **Model capacity too small** - ResNet18 (11.2M params) insufficient for 100 classes\n",
        "2. **MixUp too aggressive** - Alpha=0.4 causing performance degradation\n",
        "3. **Data augmentation overlap** - ColorJitter + RandomBrightnessContrast + HueSaturationValue are redundant\n",
        "4. **Training plateaued** - Model maxed out at ~56% accuracy\n",
        "\n",
        "## Improvements Implemented Below:\n",
        "\n",
        "### 1. Model Architecture: ResNet18 → ResNet34\n",
        "- **Parameters**: 11.2M → 21.3M (nearly 2x capacity)\n",
        "- **Depth**: 18 layers → 34 layers\n",
        "- Better feature learning for 100-class classification\n",
        "\n",
        "### 2. MixUp Tuning: Alpha 0.4 → 0.2\n",
        "- Less aggressive blending\n",
        "- Previous runs showed MixUp hurt performance\n",
        "- Lower alpha = more focused training\n",
        "\n",
        "### 3. Simplified Data Augmentation\n",
        "**Removed:**\n",
        "- ColorJitter (redundant)\n",
        "- RandomBrightnessContrast (redundant)\n",
        "- HueSaturationValue (too aggressive)\n",
        "\n",
        "**Kept:**\n",
        "- HorizontalFlip (standard)\n",
        "- Affine (translation, scale, rotation)\n",
        "- RandomCrop + Resize\n",
        "- CoarseDropout (cutout)\n",
        "- Normalize\n",
        "\n",
        "### 4. Extended Training: 100 Epochs\n",
        "- More time for convergence\n",
        "- OneCycleLR for learning rate scheduling"
      ],
      "metadata": {
        "id": "iiCLV8_hbj-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified Data Augmentation (Removed redundant color transforms)\n",
        "class SimplifiedAlbumentationsTransforms:\n",
        "    def __init__(self, mean, std):\n",
        "        self.aug = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Affine(translate_percent=0.0625, scale=(0.9, 1.1), rotate=(-7, 7), p=0.5),\n",
        "            A.RandomCrop(height=28, width=28, p=0.5),\n",
        "            A.Resize(height=32, width=32),\n",
        "            A.CoarseDropout(\n",
        "                max_holes=1,\n",
        "                max_height=16, max_width=16,\n",
        "                fill_value=tuple(int(m * 255) for m in mean),\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.Normalize(mean=mean, std=std),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        image = np.array(img)\n",
        "        return self.aug(image=image)[\"image\"]\n",
        "\n",
        "# Create new datasets with simplified augmentations\n",
        "train_transforms_v2 = SimplifiedAlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std)\n",
        "train_dataset_v2 = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms_v2)\n",
        "train_loader_v2 = DataLoader(train_dataset_v2, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
        "])\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transforms)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=5, pin_memory=True)\n",
        "\n",
        "print(\"Simplified augmentation pipeline created!\")\n",
        "print(\"Removed: ColorJitter, RandomBrightnessContrast, HueSaturationValue\")\n",
        "print(\"Kept: HorizontalFlip, Affine, RandomCrop, CoarseDropout\")"
      ],
      "metadata": {
        "id": "SCJgarKrbj-i",
        "outputId": "cb264481-beee-4973-cf7c-8ca1456caf65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-624767531.py:9: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified augmentation pipeline created!\n",
            "Removed: ColorJitter, RandomBrightnessContrast, HueSaturationValue\n",
            "Kept: HorizontalFlip, Affine, RandomCrop, CoarseDropout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ResNet34 for CIFAR-100\n",
        "model_v2 = resnet34(weights=None, num_classes=100).to(device)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Model Upgrade: ResNet18 → ResNet34\")\n",
        "print(\"=\" * 70)\n",
        "summary(model_v2, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "id": "jWET5uENbj-i",
        "outputId": "84a27aa9-7126-4a1e-aa4d-b823f7762fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Model Upgrade: ResNet18 → ResNet34\n",
            "======================================================================\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-25             [-1, 64, 8, 8]               0\n",
            "           Conv2d-26            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-27            [-1, 128, 4, 4]             256\n",
            "             ReLU-28            [-1, 128, 4, 4]               0\n",
            "           Conv2d-29            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-30            [-1, 128, 4, 4]             256\n",
            "           Conv2d-31            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-36            [-1, 128, 4, 4]             256\n",
            "             ReLU-37            [-1, 128, 4, 4]               0\n",
            "           Conv2d-38            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-39            [-1, 128, 4, 4]             256\n",
            "             ReLU-40            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-41            [-1, 128, 4, 4]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "             ReLU-44            [-1, 128, 4, 4]               0\n",
            "           Conv2d-45            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-46            [-1, 128, 4, 4]             256\n",
            "             ReLU-47            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-48            [-1, 128, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-55            [-1, 128, 4, 4]               0\n",
            "           Conv2d-56            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-57            [-1, 256, 2, 2]             512\n",
            "             ReLU-58            [-1, 256, 2, 2]               0\n",
            "           Conv2d-59            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 2, 2]             512\n",
            "           Conv2d-61            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-62            [-1, 256, 2, 2]             512\n",
            "             ReLU-63            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-64            [-1, 256, 2, 2]               0\n",
            "           Conv2d-65            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-66            [-1, 256, 2, 2]             512\n",
            "             ReLU-67            [-1, 256, 2, 2]               0\n",
            "           Conv2d-68            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-69            [-1, 256, 2, 2]             512\n",
            "             ReLU-70            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-71            [-1, 256, 2, 2]               0\n",
            "           Conv2d-72            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-73            [-1, 256, 2, 2]             512\n",
            "             ReLU-74            [-1, 256, 2, 2]               0\n",
            "           Conv2d-75            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-76            [-1, 256, 2, 2]             512\n",
            "             ReLU-77            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-78            [-1, 256, 2, 2]               0\n",
            "           Conv2d-79            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-80            [-1, 256, 2, 2]             512\n",
            "             ReLU-81            [-1, 256, 2, 2]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-85            [-1, 256, 2, 2]               0\n",
            "           Conv2d-86            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-87            [-1, 256, 2, 2]             512\n",
            "             ReLU-88            [-1, 256, 2, 2]               0\n",
            "           Conv2d-89            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-90            [-1, 256, 2, 2]             512\n",
            "             ReLU-91            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-92            [-1, 256, 2, 2]               0\n",
            "           Conv2d-93            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-94            [-1, 256, 2, 2]             512\n",
            "             ReLU-95            [-1, 256, 2, 2]               0\n",
            "           Conv2d-96            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-97            [-1, 256, 2, 2]             512\n",
            "             ReLU-98            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-99            [-1, 256, 2, 2]               0\n",
            "          Conv2d-100            [-1, 512, 1, 1]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-102            [-1, 512, 1, 1]               0\n",
            "          Conv2d-103            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 1, 1]           1,024\n",
            "          Conv2d-105            [-1, 512, 1, 1]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-107            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-108            [-1, 512, 1, 1]               0\n",
            "          Conv2d-109            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-111            [-1, 512, 1, 1]               0\n",
            "          Conv2d-112            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-114            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-115            [-1, 512, 1, 1]               0\n",
            "          Conv2d-116            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-118            [-1, 512, 1, 1]               0\n",
            "          Conv2d-119            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-121            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-122            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "          Linear-124                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 21,335,972\n",
            "Trainable params: 21,335,972\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.97\n",
            "Params size (MB): 81.39\n",
            "Estimated Total Size (MB): 83.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Improved Configuration\n",
        "# Key changes: ResNet34, Reduced MixUp (alpha=0.2), Simplified augmentation, 100 epochs\n",
        "\n",
        "optimizer_v2 = optim.SGD(model_v2.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS_V2 = 40\n",
        "\n",
        "scheduler_v2 = OneCycleLR(\n",
        "    optimizer_v2,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader_v2),\n",
        "    epochs=EPOCHS_V2\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Training Configuration:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Model: ResNet34 (21.3M parameters)\")\n",
        "print(f\"MixUp Alpha: 0.2 (reduced from 0.4)\")\n",
        "print(f\"Epochs: {EPOCHS_V2}\")\n",
        "print(f\"Optimizer: SGD (lr=0.01, momentum=0.9, weight_decay=1e-4)\")\n",
        "print(f\"Scheduler: OneCycleLR (max_lr=0.05)\")\n",
        "print(f\"Data Augmentation: Simplified (removed redundant color transforms)\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, EPOCHS_V2 + 1):\n",
        "    train(model_v2, device, train_loader_v2, optimizer_v2, scheduler_v2, epoch, use_mixup=True, mixup_alpha=0.2)\n",
        "    acc = test(model_v2, device, test_loader)\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        print(f\"*** New best accuracy: {best_acc:.2f}% ***\")\n",
        "\n",
        "    # Early stopping if target reached\n",
        "    if acc >= 74.0:\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"Target accuracy of 74% reached at epoch {epoch}!\")\n",
        "        print(f\"Final test accuracy: {acc:.2f}%\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nTraining completed. Best test accuracy: {best_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "OzwQF7wYbj-i",
        "outputId": "99c61c6b-c28d-4f13-9702-ef20af203648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Training Configuration:\n",
            "======================================================================\n",
            "Model: ResNet34 (21.3M parameters)\n",
            "MixUp Alpha: 0.2 (reduced from 0.4)\n",
            "Epochs: 40\n",
            "Optimizer: SGD (lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
            "Scheduler: OneCycleLR (max_lr=0.05)\n",
            "Data Augmentation: Simplified (removed redundant color transforms)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=3.7487 Acc=6.66: 100%|██████████| 391/391 [00:35<00:00, 10.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.8757, Accuracy: 1100/10000 (11.00%)\n",
            "\n",
            "*** New best accuracy: 11.00% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=3.7514 Acc=12.25: 100%|██████████| 391/391 [00:34<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.6104, Accuracy: 1564/10000 (15.64%)\n",
            "\n",
            "*** New best accuracy: 15.64% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.8730 Acc=15.47: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4190, Accuracy: 1977/10000 (19.77%)\n",
            "\n",
            "*** New best accuracy: 19.77% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.2073 Acc=18.16: 100%|██████████| 391/391 [00:35<00:00, 11.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2032, Accuracy: 2318/10000 (23.18%)\n",
            "\n",
            "*** New best accuracy: 23.18% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=3.1162 Acc=20.67: 100%|██████████| 391/391 [00:34<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4802, Accuracy: 2391/10000 (23.91%)\n",
            "\n",
            "*** New best accuracy: 23.91% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=2.8163 Acc=22.98: 100%|██████████| 391/391 [00:34<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8450, Accuracy: 3056/10000 (30.56%)\n",
            "\n",
            "*** New best accuracy: 30.56% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=2.8000 Acc=25.68: 100%|██████████| 391/391 [00:34<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7288, Accuracy: 3188/10000 (31.88%)\n",
            "\n",
            "*** New best accuracy: 31.88% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=2.5254 Acc=27.74: 100%|██████████| 391/391 [00:34<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8832, Accuracy: 3243/10000 (32.43%)\n",
            "\n",
            "*** New best accuracy: 32.43% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.2784 Acc=30.23: 100%|██████████| 391/391 [00:34<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6290, Accuracy: 3554/10000 (35.54%)\n",
            "\n",
            "*** New best accuracy: 35.54% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=3.0043 Acc=32.03: 100%|██████████| 391/391 [00:34<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5623, Accuracy: 3427/10000 (34.27%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.5393 Acc=33.66: 100%|██████████| 391/391 [00:34<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3158, Accuracy: 4028/10000 (40.28%)\n",
            "\n",
            "*** New best accuracy: 40.28% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=2.8353 Acc=35.59: 100%|██████████| 391/391 [00:35<00:00, 11.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3719, Accuracy: 3867/10000 (38.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=3.8075 Acc=36.64: 100%|██████████| 391/391 [00:34<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4736, Accuracy: 3961/10000 (39.61%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=3.1262 Acc=38.18: 100%|██████████| 391/391 [00:34<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2981, Accuracy: 4110/10000 (41.10%)\n",
            "\n",
            "*** New best accuracy: 41.10% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=3.5747 Acc=39.54: 100%|██████████| 391/391 [00:34<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2715, Accuracy: 4179/10000 (41.79%)\n",
            "\n",
            "*** New best accuracy: 41.79% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=2.4041 Acc=41.79:  17%|█▋        | 65/391 [00:05<00:28, 11.58it/s]Exception in thread Thread-35 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 61, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
            "    c = SocketClient(address)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Epoch 16 Loss=2.4041 Acc=41.79:  17%|█▋        | 66/391 [00:05<00:29, 11.09it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-418396189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_V2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixup_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-174365245.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, scheduler, epoch, use_mixup, mixup_alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixup_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2815\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2818\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2819\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}