{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-7t1Y7-hV4"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "from torchsummary import summary\n",
        "from timm.models import create_model\n",
        "import timm\n",
        "\n",
        "# CIFAR-100 Mean and Std\n",
        "cifar100_mean = (0.5071, 0.4865, 0.4409)\n",
        "cifar100_std = (0.2673, 0.2564, 0.2761)\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr2KC0BGbtME",
        "outputId": "bee5b3d5-7421-4b6b-abeb-6fc84a406949"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kH16rnZ7wt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5e8d2b-e315-4960-cc39-e9e11d1a4a7a"
      },
      "source": [
        "\n",
        "\n",
        "# Custom Albumentations Transform Wrapper\n",
        "class AlbumentationsTransforms:\n",
        "    def __init__(self, mean, std):\n",
        "        self.aug = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "            A.Affine(translate_percent=0.0625, scale=(0.9, 1.1), rotate=(-7, 7), p=0.5),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),\n",
        "            A.RandomCrop(height=28, width=28, p=0.5),\n",
        "            A.Resize(height=32, width=32),\n",
        "            A.CoarseDropout(\n",
        "                max_holes=1,\n",
        "                max_height=16, max_width=16,\n",
        "                fill_value=tuple(int(m * 255) for m in mean),  # Convert mean to 0-255\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.Normalize(mean=mean, std=std),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        image = np.array(img)\n",
        "        return self.aug(image=image)[\"image\"]\n",
        "\n",
        "# Instantiate transforms\n",
        "train_transforms = AlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std)\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
        "])\n",
        "\n",
        "# CIFAR-100 Dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transforms)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=5, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=5, pin_memory=True)\n",
        "\n",
        "cifar100_classes = datasets.CIFAR100(root='./data', train=False).classes\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3247543136.py:12: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(\n",
            "100%|██████████| 169M/169M [00:03<00:00, 48.3MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Get a batch\n",
        "batch_data, batch_label = next(iter(train_loader))\n",
        "\n",
        "# Move to CPU and detach the computation graph\n",
        "batch_data = batch_data.cpu().detach()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(12):\n",
        "    img = batch_data[i]  # shape: [3, 32, 32]\n",
        "    img = img.numpy().transpose((1, 2, 0))  # to shape [32, 32, 3]\n",
        "    img = np.clip(img, 0, 1)  # ensure valid range for display\n",
        "\n",
        "    plt.subplot(3, 4, i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Label: {batch_label[i].item()}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "7MwdNs7Nrngt",
        "outputId": "f6715735-4c63-4a70-ddf3-5a4e229d731f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 12 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAMYCAYAAADW64SBAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAmLZJREFUeJzs3Xt4VOW9/v87ZGAGEk0gkGDQAJGDiJQiiMj2gFpFrbqxImprtVatttpat4e2uyq22tr+PLbSrW21WhUPrRXrqVorKloUFG1UBDmfEggkmIEMJHGS9fuDbb6bwuezcJJFDrxf19Xr2pt71nqemVmHeRicOysIgkAAAAAAAKDVdWnrCQAAAAAA0Fmx6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOjuQFasWKGsrCzdeuutrbbPV199VVlZWXr11VdbbZ8APh/ObaDz4bwGOh/Oa2SKRXfEHnjgAWVlZemdd95p66lEYsaMGZo4caKKi4sVj8e17777avLkyfrwww93eOzjjz+uc845R4MHD1ZWVpYmTJiw+ycMtJLOfm5L0j/+8Q8dffTR6t27t/Lz8zV27Fg99NBD7jZvvPGGsrKylJWVpaqqqt00U6B1dPbzesCAAc3n57//b/Dgwc2P++x1sP43ffr0NnwWwOfT2c/rzzz++OM67LDDlJOTo/z8fI0fP14zZ87c4XH33Xefhg0bpkQiocGDB+uuu+5qg9nueWJtPQF0bB988IF69uypyy+/XL1799a6dev0hz/8QWPHjtWbb76pkSNHNj/27rvv1rx583TIIYeourq6DWcNIMzTTz+tSZMm6bDDDtMNN9ygrKws/elPf9K5556rqqoqXXHFFTts09TUpO9+97vKyclRKpVqg1kD8Nx5552qra3d7s9Wrlypa6+9Vscff3zznx155JE7/Qu2O+64Q2VlZTr22GMjnyuAXXfDDTfopz/9qSZPnqxvfOMb+vTTT/Xhhx+qvLx8u8f99re/1SWXXKLTTz9d//Vf/6XXX39d3/ve97Rlyxb94Ac/aKPZ7xlYdKNFrr/++h3+7MILL9S+++6ru+++W/fcc0/znz/00EPq16+funTpooMOOmh3ThPA5zRt2jTts88+mjlzpuLxuCTp4osv1gEHHKAHHnhgp4vu3/3ud1q9erUuvPBC/epXv9rdUwYQYtKkSTv82U033SRJ+trXvtb8Z6WlpSotLd3ucVu3btV3vvMdHXPMMerbt2+k8wSw69566y399Kc/1W233bbTe/Nntm7dqh//+Mf68pe/rCeeeEKSdNFFF6mpqUk33nijvvWtb6lnz567a9p7HP55eTvQ0NCg66+/XqNHj1ZeXp5ycnJ0xBFH6JVXXjG3ueOOO9S/f391795dRx111E7/OffChQs1efJk9erVS4lEQmPGjNHTTz8dOp8tW7Zo4cKFGf/T0MLCQvXo0UM1NTXb/fl+++2nLl045LDn6Mjn9qZNm9SzZ8/mBbckxWIx9e7dW927d9/h8Rs3btS1116rn/70p8rPzw/dP9BRdeTzemceeeQRDRw4UOPHj3cf98wzz2jz5s3bLc6BzqIjn9d33nmn+vbtq8svv1xBEOzwr1k+88orr6i6ulrf+c53tvvzSy+9VKlUSs8991zoWMgcK6B2YNOmTbr33ns1YcIE/fKXv9QNN9ygDRs2aOLEifrXv/61w+MffPBB/frXv9all16qH/3oR/rwww91zDHHqLKysvkx8+fP17hx47RgwQL98Ic/1G233aacnBxNmjRJM2bMcOczd+5cDRs2TNOmTdvl51BTU6MNGzbogw8+0IUXXqhNmzbxz8+wx+vI5/aECRM0f/58XXfddVqyZImWLl2qG2+8Ue+8846uueaaHR5/3XXXqW/fvrr44ovDXxigA+vI5/W/e++997RgwQJ99atfDX3s9OnT1b17d33lK1/53OMA7V1HPq9ffvllHXLIIfr1r3+tPn36aK+99tI+++yzw7bvvfeeJGnMmDHb/fno0aPVpUuX5hwRCRCp+++/P5AUvP322+Zj0ul0UF9fv92fffLJJ0FRUVHwzW9+s/nPli9fHkgKunfvHqxZs6b5z+fMmRNICq644ormPzv22GODESNGBHV1dc1/1tTUFIwfPz4YPHhw85+98sorgaTglVde2eHPpk6dusvPc+jQoYGkQFKQm5sbXHvttUFjY6P5+OHDhwdHHXXULu8faG86+7ldW1sbTJkyJcjKymo+t3v06BE89dRTOzy2rKwsyM7ODl588cUgCIJg6tSpgaRgw4YNoeMA7UlnP6//3ZVXXhlICj766CP3cdXV1UG3bt2CKVOmfO4xgLbWmc/rjRs3BpKCgoKCIDc3N7jllluCxx9/PDjhhBMCScE999zT/NhLL700yM7O3ul++vTpE5x11lnuWGgZvuluB7Kzs9WtWzdJ236IaOPGjUqn0xozZozefffdHR4/adIk9evXr/n/Hzt2rA499FA9//zzkrb9M8+ZM2dqypQp2rx5s6qqqlRVVaXq6mpNnDhRixcv3uGHFf6vCRMmKAgC3XDDDbv8HO6//3698MIL+p//+R8NGzZMW7duVWNj4y5vD3RGHfncjsfjGjJkiCZPnqxHH31UDz/8sMaMGaNzzjlHb7311naP/d73vqcTTzxxux9iAjqrjnxe/19NTU167LHHNGrUKA0bNsx97BNPPKGGhgb+aTk6rY56Xn/2T8mrq6t177336qqrrtKUKVP03HPP6cADD2z+zQZp23/T/dlz/HeJREJbt251x0LL8ENq7cQf//hH3XbbbVq4cKE+/fTT5j8fOHDgDo/9v7UenxkyZIj+9Kc/SZKWLFmiIAh03XXX6brrrtvpeOvXr9/uYtFShx12WPP/fdZZZzXfwFuzxxDoiDrquX3ZZZfprbfe0rvvvtv8WwxTpkzR8OHDdfnll2vOnDmStlWUzJ49e6f/LRvQWXXU8/r/eu2111ReXu7+8NJnpk+frl69eunEE09s1TkA7UlHPK8/+42Vrl27avLkyc1/3qVLF5155pmaOnWqVq1apZKSEnXv3l0NDQ073U9dXd1Of68FrYdFdzvw8MMP6xvf+IYmTZqkq6++WoWFhcrOztbNN9+spUuXfu79NTU1SZKuuuoqTZw4caePGTRoUIvm7OnZs6eOOeYYTZ8+nUU39mgd9dxuaGjQfffdp2uuuWa7Hz/s2rWrTjzxRE2bNk0NDQ3q1q2brr76ap1xxhnq1q2bVqxYIUnNP6K4evVqNTQ0qLi4uMVzAtqLjnpe/7vp06erS5cuOvvss93HrVq1Sq+//rq+9a1vqWvXrq0+D6A96Kjn9Wc/0Jafn6/s7OztssLCQknSJ598opKSEu2zzz5qbGzU+vXrmzNp2z2/urqae3XEWHS3A0888YRKS0v15JNPKisrq/nPp06dutPHL168eIc/W7RokQYMGCBJzTUfXbt21Ze+9KXWn/Au2Lp1q5LJZJuMDbQXHfXcrq6uVjqd3ul/IvLpp5+qqampOVu9erUeeeQRPfLIIzs89uCDD9bIkSN3+iM0QEfVUc/r/6u+vl5/+ctfNGHChNAP2o8++qiCIOCflqNT66jndZcuXfTFL35Rb7/9dvNfhn+moqJCktSnTx9J0he/+EVJ0jvvvKOTTjqp+XHvvPOOmpqamnNEg/+mux347G+mgiBo/rM5c+bozTff3Onjn3rqqe3+O5C5c+dqzpw5zf/sq7CwUBMmTNBvf/tbrV27doftN2zY4M7n89QUrF+/foc/W7FihV5++eUdfh0R2NN01HO7sLBQ+fn5mjFjxnb/FK22tlbPPPOMDjjggOZ/hjZjxowd/nfmmWdK2vbrrnfccYc7FtDRdNTz+v96/vnnVVNTs0sL6UceeUQlJSU6/PDDd3n/QEfTkc/rM888U42NjfrjH//Y/Gd1dXWaPn26DjzwwOa/WDvmmGPUq1cv3X333dttf/fdd6tHjx768pe/HDoWMsc33bvJH/7wB73wwgs7/Pnll1+uk08+WU8++aROO+00ffnLX9by5ct1zz336MADD9xp196gQYN0+OGH69vf/rbq6+t15513qqCgYLsan9/85jc6/PDDNWLECF100UUqLS1VZWWl3nzzTa1Zs0ZlZWXmXOfOnaujjz5aU6dODf0BhxEjRujYY4/VF7/4RfXs2VOLFy/Wfffdp08//VS/+MUvtnvsrFmzNGvWLEnbLjapVKr5Bx6OPPJIHXnkke5YQHvUGc/t7OxsXXXVVbr22ms1btw4nXvuuWpsbNR9992nNWvW6OGHH25+7KRJk3bY/rNvtk888UT17t3bHAdorzrjef1/TZ8+XfF4XKeffrr7uA8//FDvv/++fvjDH2737R/QEXXW8/riiy/Wvffeq0svvVSLFi1SSUmJHnroIa1cuVLPPPNM8+O6d++uG2+8UZdeeqnOOOMMTZw4Ua+//roefvhh/exnP1OvXr124VVExtrkN9P3IJ/VFFj/W716ddDU1BT8/Oc/D/r37x/E4/Fg1KhRwbPPPhucd955Qf/+/Zv39VlNwS233BLcdtttwX777RfE4/HgiCOOCMrKynYYe+nSpcG5554b9O3bN+jatWvQr1+/4OSTTw6eeOKJ5se0tH5k6tSpwZgxY4KePXsGsVgsKC4uDs4666zg/fff3+ljrdchk6oToC119nM7CIJg+vTpwdixY4P8/Pyge/fuwaGHHrrdGBYqw9BR7QnndTKZDBKJRPCVr3wl9LE//OEPA0k7vacDHcWecF5XVlYG5513XtCrV68gHo8Hhx56aPDCCy/s9LG/+93vgqFDhwbdunUL9t9//+COO+4ImpqadmkcZC4rCP7Pv6MAAAAAAACthv+mGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIhIbFcfmJWVFeU80Fkc7GTe0Ta3tSeyZwmCIKPtOK+B9ivT81ri3N6TeN+eNLVgW4+335Z8m1PiZGknW9OCMTO1V0he7DyZhSu5Z2NP0s/J9nOyfUL2623rZHEnK/L2KamvnQdz/G35phsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiOzyr5cDu2Spk3k/PQqg3dqnaIiZjTnyCDPzTvnXZr3ijrmlclnYtNDOeL8zu7eT9cvJdfebU5Bnh6m1ZlRZbf/GdoU7opQMyc3pOFnYL4lnKsfJvPlI0cypJftc0VqT+BwGOFnxkfl2Nny8u98RBx+X0Xxc/f5uZ+Od8Yr83fbc184KnR+eLnD2W9DXH9PbNttZnRT1cvbpD+leh7wFkZeFfbT18oSTeee1c0WUJBU7mXe1rXSy9SFjetfMOifL9HWXpGwnqw/ZNtMx4xnuV+KbbgAAAAAAIsOiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIUBm2p/KaLF5qwX69zoCW/M5+R+J1OWTaQwO00IlOPcvNDz3hbjvyuNNbeTbh/vzw42Y25etn7caZdD49nWxCyLYjnCzP6euJ98s3s4Lho90x47l2GVlyqV0Z9sGcl81sQUiXVrWTeVU03iV+hT9kxrz5RFVT1tE4jViacNzhZjbsVPva12voWHfMfn0Hhk3rcztrrv3h7XCnJyrKGqRMtcUCJNPqKu/18aq9JP/89OrEvGtJWBWgN6Y3X28+3msn+fVdXjVapvuUMj+GvO28ujVJGpXhmBLfdAMAAAAAEBkW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARoaf734w8P9/MKlbWmNmGf/r73au3nW0u97fNmFdw966TOX2+qgwZ0ykz7DHUzrZ45YBhr49XHpjrZP1bMKann5PVOlnYa0vHN0Ic55x/0//2NzPrOeqECGbTMmecc6aZ3TanzMyunHZzFNPpcJzLbcZ9tGHqnHLrupoae8zYcne/8f72rNJVdiNtvVNm25KeW+925XXgDggZc5WT0bfdMqNKhpjZuKPsLu6Bo+1rY27v/fxB063/ETvpnGMxp2A47Lz2ZprpswjrWPZE0b8sSQVONszJvO7mbiFjeufusgyzsI+D3sfiTLu4G0PG9I6x9rbYzHOyESHbhr3fHr7pBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIhIe/sV9zZXdn+NHTq/Mb+vV7MlKel1gUTF+31/p3aiRZyelS3vZ7jPkpDcq/7y+g+8+q6wM8Pbr1fH5vE6DIBdcODIUjNrj7Vgmfr+1T82sxceesTMXkqujGI67ZJ3OdnoZBUt2K9Xx5N27kf173rlOFKjU71UV20X4Hi1YN4lXMq8Fsx7DRIhvU0x5/65JMP5NPhDdhqj4/lufvhXvmlmJSPHm1ksnTCz6qVr3TGra+wjcPywke62JudzS6/M9hgq0zqxsI9RmVaKefv1zgXJb8Qd8Pmnsku8bzcHOZl3LfkgZMxMq7/soz2cN1+P91E8rKYsU95x0COiMSW+6QYAAAAAIDIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICJVhn0fSjtY4GVoorG4t0zq2A50spAJOlU7mdep4WVvUyqFTWfXxiraewm6x7P2FZtaY5GIsZX5zD6sM81qvvLoZr8on7B1Lv7/IzGJO75BX+9WSahzvMl7s9SCFDJp2KsMyNTQk944Tr1rOL8uKxl5OdupF33e3HXPS6WZWH7NfhX+V2deajz5a7I65vsp+Bb9xVmaVYUXOGzbE2a4lV0Xv3PWOH+/8C8szvX6FnddhlWLtiTfXwSHbetfx9RmOWRwy5gEhucWrQ/RqH6XMj7+wJuKo8E03AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQESoDMOe6yMn83pxwni1L2EdGkALvJFsMrM1z75iZvuefHQU02mZLXb0i3PON7NXVdP6cwnRw8nCKmqiugl71Tle5VVYtZBXRZPrZMOcrFfImN6cUo125l1uC0LGrHMyb78VznzC2uxW+LHpEycbFXIAFjudYsuW2tnaNmjmm3DwGDMbdexx7rax3vZR9k5ZmZk9N2uWvd2cee6YW6q90rWp7raWQufE9s6xsCpA7zDpE7KtZU1I7h1CXvOeJ+x6GlZBZfGup2FzjeIjYdjz9K5v3rZelmklWJhBEe23PeKbbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAi9HS3kn2yu7p5XeOnZvbVE0eZWcVKu+fx1Y9WumN63Z0I4RUrtkSmJZHALtjgZMvmfWhm7bGne84td5nZa8kPzMxuKo9OYYaZFN1NOC37nuTNqVj2vUqSqp2s3Mm8LtsD3RH9XvFVTuZdxsP6070O4beczKmXbxOVIfecwho7q26DLu6eTjZm9Mlmlqrx9/vOUrtv+4XXXjezuXOdLu7K1f6gEXyQSHoF8g7vHJL81z1TYWN6HdXeddG7loRdT73zPtNrcaZd22G8+cRDtvV6xb3XIKrngm34phsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgItFXhhWE5N7v07dBXUWm1jqVYGGe+9t7ZrYi470CwP9T8fGStp7C9kK6vf714O/NLKwuZXdbkWEWpVVO9Vep8/ftI4YMcfcbk11juWxRlZmtd/Zp3wH/d07qbmbebFdpq5lVhozZkWrBPMtD8rxldpbrbOd9Y9OS2r5ji8aYWaLGLkJ67v5n3f2+vHS+ma1d57xKjV7nWthH6Na/Ui2wn4Z0yO6cSbiwWj5vTt4r296+LewWkvfYLbPYdXu19QT2YO3t2AUAAAAAoNNg0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARKKvDKsOyYucrANVhrXEiraeAIBO762ZL5jZV95b4W7bbdSA1p2MpDV33e3mHyz7wMy8G1dPJ/vEn1KnssLJUk6p07CUXQkmScMO3M/M8nJ7mdl77y4yM/ud3ibtVH8NU28zq3a2WxAyZkeqBfNkh+Rea6tX+VTiZCtCxtzXyRLOhD6YPcfMFlQ1umOurV/tpE4tWE6hGQ3tt487Zm6s9Yu6ctL2+dlN9vkXVmsVhbBv9dpiTkBb4ptuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIhEXxkWptLJxjtZnZOtDBnT68jYQ2rKAHQ+Xl1WeaVd2fTk73/n7nfcaWeaWU6+XVOzbM5sM3vxf37vjunVSDkFP0o4WdjfMttFWp2L1+T5dnmVu23aqSwqyuljZgXxfDPLq69xx1zlZAXOhwHvVh/24WcvJ9scsm0U7FdWynMyr9pLkoqd2taU8wKuCquDdeR5XbE5xfZ8Uva7VhjzCs6kofV2vVe9U7VVWmS/8sX5/pjuAZihUbnrndR+HgDaHt90AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABHZ9Z7ug52spgUjeKWq3raZbidJ/e2o28h8M2t4sMbesDFkTABoBT2czG649S+LC+bNc8esrLWzRmfPq+aXmdkHi95zx1zsZJnW3xaE5F7/95YMx2wrXmd7vZPNDdnv4kq7zXyYKs0s7LX3eO930r355ppJTM5Brcw/YnivbbY7ot+3nWkX95Ah/pi9etvZqnV2FmtBT3dSdr/1gnL7LIw7b3VJkd9RfUgibod19pixlD1oXfVqd8xVyU1unonqcm/MA1p9PACth2+6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACKy65VhdhOIVOdkYT0hXu2Et1+vz8NvApHetaOGsho7pBYM2KN0C8mdEprIeNVB3uU2R13NrLrGr7ap/HihmaVq7QtjxUq7+GuZO6J/y/FuXHYZkZ+F7dfTHuvEvFvkJy3Yr7ftbCfby8nsYq9tvPct5ZR0xbWfs6X/QcGrKfNeW6++yzt3JWmWk61xsvlO9rdF/pg/cyrDvOfZkmtfhezrTbJxvZmVah8zK8jZ2x2zMGGf3cmVy81sceUSM3tH9naStNbt083MW2V+TRmA9otvugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAisusNKeUZjuBVgrVH1IIBHVJPJ/PqfzKtn5L8Sp1MeXOVWlCJlW2X/NTVpdwxU1Vrzaw6ZW9bnbLrbZLuiFKTkzU4Waa1Xy3h/e219zyiZL9jbWNzhpnkv77Zzitc4hRbxdXdHTOtrU5m82rBCt0R28ayMid0LnDetSasarHBOftj6uNk3gc0uzrus60tFamNZvay3jOztqgKXLDSniuA9o1vugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiEn27it3YsY3X8uBt620X1vNT4GT5TrbSjk45/zR3yGfunOHmURh98PFmNu/dv+/GmQDR86p6vAudd5lpSSWYVyvkZdktGNOr4Uo01tpjrlro7tcrFNuoT83Ma4z0S8oy59X4tEXFD1rOq1372MlynTMioX3cMeu1zMy8ijOvFsz76NFWVjknYm6GJ2nYx74c54o8wvkAV6klZvbWsvUhY/Yysw+0wsza2zWjocy/VgNov/imGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIhI9D3dXp92mP2drM7JwrolvfLd90O2NYw6arSbt0VP9/ev+7aZff00erqx5xjmZF63tZdJ/qXGu0Q1Zrid5M/J6zP2/4bV7toO2y86lz4huXf8NbTmRFrBAlWa2bCcEe62sVR3M2vQVjOb7+zTbqBuO95HNO/6ttrJvB5zSdrHGbXQ+Vg6SytD9rwHSN5nRs9sutfeLuTTfszJ085NKZ32duqPmbO3nXkf09Ob/P16Us6cUrV2Vue8BmHndZ73ZJz9ppwPCt7LLmW+9PLea+/1kaR0tT2rjevsN6282s4qaqrdMddXrjWzamfMWCzbzLLdT2hSyjkYVj//TXdbvukGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiEj0lWFh7F9tlz6KaEy7RcT3BTv64v7DM9xpdA4ff2RbTwHYbbxiCa/6ZpWTVYSM6dV2eC0h3mXPL6vIvL4r8zoxqZuTZXoTCas8aW/1U1HZp60n8G/iLcjb23u2xckqUwvdbXtpP2e/yzKaT9j1ZF8nK3CyURnM5TPeddN7lnZRz66MaV+NKtwyMngeuWWemXmVYJIUz83LaNv6OrtEMF3nF1fF850xne28e6vSfmdwXdqek1d/1lhnh2knkyTF7Oe5bOlGM1uwdLmZpWr955l2Xoe8hP3qeu/nlpUr3DHVGFayam7oZGHlZ+udzLvievv1K1R9VIYBAAAAANAmWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEJHoK8O8bhwpvB8nCgc6mVNTtle/3mYWq/XKSaLS1U3fmjlrN80DaHubnWxmRGP2dDK7JMSvXQq7JHpFIZlWkYXVd9U5mXcTackNxnuebXG1zdTIkHzcbpnFrlvT1hPYTdaE1MIM0iYzG1nklHRVLjajmGrdMb2KLq8YZ4STFboj+ue2dw0b5GRhraxDnGy5ykO2hqW+xj5m62P+VT5ZZW8bczq6smN29VJuwi8gzM3O9M4Sdsey5cXsOcXiznydIVMpv9YqO2bfmUv722fSIaP7mNkHZR+7Y1ZW2tV7BQV7O1v2sve5b447ZsWaDWZWX2+/gIVF9lWqqMgv1Uym7E8KS+YvsTestqvawmvKvG19fNMNAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEJFdb3Txfvnf+3X1/iH79ToyouLUgnk2l9s/E7/g4+UZTiZzl172bTc/++zTdtNMgM7JqwSTpGInK3Ay78Ib1rIYVmaRyZhhvMu/VyLiVQ6FzccrhfFeg2onezlkzIaQPBOlIXlRBGOi5bJVZWZj9h9vZkUH2yVxFTNfcsesq7frsuY7273nZN51SPLPX+/Y9aruEiHf58S0n5k9rpVmttbdK+K5Th1WzK97qkvbV9WYc7H2suxE5neduLutncWcei5J8prT0nX2a1BZaZf2LZi/0B2zutqpY3MqzAoK7DtoqtarW5Pmz/SKUr2rifMCZY92x+zS264ba6qz95vob9eCjTnUL93MybWP643HH2pmyRqnlDTtH0PJpFdo6uObbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiu16il2k5bFv0cEfl/SYzqizf/T3dv5n2690+JjoX72/d7KO9c/Feg7COW69judDJvMbU3JAxvc5sT8LJwrrBvb5t7zXynqfXwx2m0cm8W87ikP0uyWAukt/n7nW5S/5rhPYpT3ZH7uCRB9vbJfZ291s94w9mVqFaM7MbhKWkO6I0Ql3NbJgGmdlg58z3rheS1OhccT5yero/Dtnvni6dtq+MMa9QW1LMvQvY+61zuq1Ttf7RV19nn0de/7L/PP27WbrW7liuWGk3wVdXbTSzeuc1CMsrqlY729n91SX97a57SRo03u6oXjLbO5PK7ahxljtmU6U9pgr6mFG2c2yur7TfE0mK19qfiBL5dlbY17kWp/1zpVffsCucjW+6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACKy65VhcN1+7X1tPQXgcxvlZPN22yzalncRDKtz8mrBSpzMK5wIG9PLvefiVYaF1ZC1pOLMYpfFbOPN16sbq8xgLi1V6mRhtXNUhrWNHi3YNunVB+XaZ3evo45291vqVCElZ79kZuk8u/4mJ9e7SknDeg80s7wa+1VqfNd5DWRnkn/Mj3DOpnecQsAV7oh7hlTKrsPyMklub6TXNlZXb+83lp15MaRXC5Zyar/qQp5nY21m/cdeRVdB717utl7tVWHReDOLxew7s1fVJknF/ew5JWvs83PDR7Odvfp1bPsebNcl/udXTjSz0v3t17bSudZKUkV5hZklq+zXKJ5rH185uf77mZuT+V2bb7oBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIkJlGLAH+6KT2UUMkl2A0fF4dVletZckFTlZsZNlWvsVlme635bcCGqdzC7l8Gu/wnhlKeudLKo6Mbt0SfLLR8KPsc5iaPYwMxsxeqS7bapuk5mter/MzOpVbmZhx593TqxP2Uf2KqeSqKB/SIHcSLtyJ8+p64n1dmrKiuw6Hsk/l94rW21mjdl2lU9eo1/zM9g56vOcK+exThXZO6pxx1zgZA3ulh1HxRrvru2LJ+ySxrzediVdzG+Rcnm1YOlau1Syrt4+apM1IdVoafvML8i3j8tEwv6kEPM61SQVO5VYJf32MbNkTdLeadK+JkpSLGG/MSNG2tfbyr72fGLZIc9zgL1tXdp+z94rs6/hq5xKMMk/buvr7Pe6cr59LSlyXgNJKt3fu+MPd7flm24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiFAZBuzBDnSy5U4WVr3UlMFc2opdOBFe51ToZCHlQKaWVBllut+wfXq1Ql5BS0vG9NhlMtIiJ9vcgjG9EpFSJ/Oq46ToKsPu/P0MM4vl2qPm5Nv1QJKUk+PUPeXa2xbk2+VpRUX+0eC0B6liqV1r9dbfX7CzJ59wx6ycP9/MqrPtcr4FK+2Km7j3RCQteN8ec9kK+4qcciqUmpxaJkmSV0vkVn95xZH+HWC8+pvZCPUxs1znLBzmXqWkAm01M7tYTvrY3Wv7Ul1tX41zcrw7nRRzYv8Iss+F+pRzbG17hJPZdw+voivXqf0K2693U6pI2mWUdTH/2CtK2J8UFq20z6P6upD6M0fMeZo5ufZ7Vppr12EVFtnnpiTlOdWF9U5lmHd8DR5+gDtmssapk/zYvmZuqrbfz9UfL3THfOefr5jZ/VOPdrflm24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIrtclfrLy/Kd1O5fS4eM4HXJ1bnVd3bPXK72c8esTdv9bFPv9xobEeassflmVjrSPhjyiuJmlqyx32tJ+vk0r5k3M+ec2NvNhx1qH7jVS2vMrL6qu5nFvJNBUlqfunkm9u5nZwOdU2FZyH5XZDKZNuJdosIaP70u7hz7kJZTqytnM0lSItsJncLLmLNdWJWv1xbqNQ97Z25uyBNNO6dDvTNfuyW5Zby7SomTFYUcRCH1uRm7/MJJ0ey4LfSwowN62e/MMYdcZGZ/HnuEO+Tzv/+9mX00f4mZvTenzMzWl3u919KW+ted1O++7khma2VGmd3SLfnt8j7v+uZ9M9Xe3pHS/e2O5Zw8/3NUfaN9U4rH7Yu196mlLmQB0Oj0UGc7m+bl28/Fu89J/r0u7dzN0s6G9f5HN1Wsqzaz8nK7p7ugoJe/Y0c65fRiOzfXnIR9Q/I6sSUp5byfsYT9hsYS3ocB/wZZX2fPKZGwb77xmH0tTjf61+kG57UNwzfdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABHZ5cqwundrzKyooKuZxd2fgpfq6+yf4S90agGUtnsBKla+549Z5cZoiXU1ZlQ81N6sMNeu0krF/KqLKBx16AFufuGPDnNSp5Oiyj5uN7xv19BIUiqkriETBQX237uNcwpRUiHNel6Dhlfp1BYVLJlWXklSL+fyVlRkZyGtJi6vbiztPBmvgsvbTpKynUPaa8TyLv9eJUwYr/bFLmdpmSFO5lXHhd1lw+pmEI0zjvOv8YnEd82s+sY7zGz5S3bt1xb5n03gs8uV/GxP4VU6pWr9zw/ZTodjTo59J0wlk86YdiZJefl2HVSOc/Ood/qEK6r8O0DMuR4X9ys2s7yizD+Heq+D957l5tpjejVuklTv3PDTaafmuda5uYa8n96LG3eeiydV6xX6SatW2p8oc2L28eV9Hoplh8w1ZF3r4ZtuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIjscmnLFTddbIcp+7fXF63wa5CWfWTnCecn8dPyMvtn4iVpU2ylHS5yN0WIEf8xwswuve5kZ8uNZrKlzv/5/vNvuD1sWp/bh3OW+w+oPd3JPjSjT95fbWYV8za4Q1ZX2a/RgLPcTU0l++9jZsX97XMsVbPM3e96p+XBK53Y7O41Gt5F0L+SSHm97axwXzvz6ioaQ+q7VOtk3pPxKjLy/SELnI4zr/HKaSYJv/k4D/Aq69aH7dcxyMmGOVmxUw8XD3uiVIa1S6OGDzCz0t6FZrbMeUP9KzzQMhWVdn1SzOvKkrR3em8za3RuWN7nknjImDGnPNOrilq10v4clazxa63cWrD8XmaW41RehdVaeS2y3mtbuc6+mxXvaz8PScorsOfrtLyFVst5vJq3WHaPjPaZ9j4sSVLaPsY2OvVxaaeuui6kx7Mp7P128E03AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQER2uTJsr74DzWxzlf2z9iVDD3L3W52yf7b9rddmm1ki1643GLy/P+Yho4fa4ay/u9t2JHs5WVTVTDk1dt1AU5l9nCwrL7OzJXYlheT/zVGTu6XtV39zOokkZR92hZmNcg6varvBQAUhZ2Ph/v38B2SgqLddkZF2qm+GDO/u7nfY3K1m5r2yH7t7jYZXSGe/OtsUOFVROXl21ug0ToQ1hnllFo1Ox5lXXZWX74+ZG7fPsnSjfZalnHqzsCYQ74kmnYMo8zIPyS48lAY772eecxzEWvKGos3k5dpZ3Dl461tUWgdkrleefccq6OtcwCQlnEosrxbMrSIL+Uzj7TeZtDOvoisv33+eefn22qG62u7Scms+vW5MSUnnc3EqZW+bStnzCauA855n2ukwS8fsGrewBWNd2tlvcoudOS9uWAWcp9oZs6nW2W9dyE05N6xM1sY33QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEdrmn+/gRP7RDu5ottDPVbuIOU+Nkb7hbfiPjMaMx1sm8xs/SkP1+MW5nbzm1gt5BUREy5jszV5rZde/fZ2bLnN7dD0LGLHEy5yXQECfL88qbJaWcvu337Mpxtw+5wuk0lqRl8+wX6fg/+Nua83GeZ9rpnizp572y0ogcu6d7lXNR8BrZN7gjZs57Jn7jp1SQb/+9ZV6u3XeZin2a2YQk1Xt9204Wz7G71fOcjlZJSiTsSXmNlvE6+xiqr/G7MOvq7bxa9sni7XWAO6L0RScr2d/O3J7zkPOanu72KaebE6btEy0Z2ZUK8BX1LTSz6hrv7io5l1vFEz3MrK6uzsw2VvvnQp3TlezXUNthLO7fy2Ixu2O5vta+X62vtT+Nx537oySl7JdI2c629c52FeXe6kCqTjpd5s4Hv0Suc68PuVd5x0K6zp7P+kr7uXid7Nt2bEdNcl5A98k4i1pJCjmXPHzTDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABCRXa4MOzDDupScXH+/5xdktt9YP/vvC5K5Te6YCeeX4h9wmtGi8j8zDzezRMyebDzm/2x9Omlve1G2/dbXx+w6gbxEL3fM1MrVdlhnzzfHqVdKO7VDkpROOPUtabsyIOa0CdQnQ8Z0Tp2UUzuxbL5zbIZUC/VzzsFMxZ2+hWynNqGXUyshSYOduQ5+386WOfsMqx/cEpJnwi4X2SYet6tUEs5xWe88G+/YkqQc57XPy7VLznJy7bmG3QpiITVmlnjKPr6SIe9YqtI+Qdc7J4v3TEa4I0oj+tlZUX/7ZpYj53oR9+tHwmpY0Da8byRynIq9mNv5Z1cpAi0VSzif3UI+i68qX2tmG2vsSqc6pwrKu1dJUm6+83kybV8365yLZirlf3Zb/PFye7/OZ7dYzP6MWlhkV7Vt29bOcuJ7m1nCuXekUv4nonrnc/NG5/WL19mfesLuVY2Nzodq5/PkJ14FV9j9MYobaL1dZbdN5rWQfNMNAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEJFdrgwrPtfOetktNe5P5UtSIt8JnW2Xp+3qpTdr/DHXV9rZaTfZmdM+pZJ8f8yCAvvvN14uf8PM4onuZpbn1ENIUtqpoFpdVmVmvfraYybi/pjJtL3fwkRXe8PyT82oIKSuKM95X7xWhVpnu4p1/pjJSnu+3jHvHUP1IZ1Y1fZLmzH/9LQrMrz6B0lKOF1bTkugip0srBhik5PZZSgt5NTrpb0X18kS+f45lpdn16z06O1UsHjz8Zo+wrZ1ql1iafugjsX8Y6jOOVkqnO2842uUO6JUOtTu1clzaqLccjmn9lGSYrt+G0Y7UTp0gJkN00gzW6u3IpgNsM2C+QvtMKRaqbLKvoM21TuVTnV2vdLmtLM4kFRQYN+vvPvnJ2ucatpUSHlorX1P2qvffmbm1QQW9fUrw3Ly7A+x9fV2tVdllV3Vlm5BVZa3aV21fd+td+rhJCntfS70ppt2PuQ7r48kqc77HOE9Ue848T5NSuEFtja+6QYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACIyC53lTwyx84K+zoDhIxQVGRndV69UljFjaPEmVOq2s68p1Kx1B8zmWtXnOU5TTSx9FYzS1fbmSSlauys1nmeBXn2fksGuEMq5fxCf0WtXbOV7b2fIb/OX+S8fnHnTVvvVKrJbg6S5L/fxV5dlnO8N9b4Y8a9+UYg3WhXMWyssmtCJKnWmatXATfCaYfwqqAkaZmTOS2BLq/JQpIancowJeyDL+4cIz2cSjBJklcL5vF6QkKOd1ed/abFYnbtXCLhv7j1zn7Lne3s0hdpsN9go8IC+yjz3jP/tfWfZ1SVYWucbN9IRtxzjDvpGDNb/7FdZ5R+yD8WPpBd+fSJNjhb2p8vsOdYO3+JHYZcb7vk2BfHnkX2VfWTGme/KadqTFJFuX1MF/a173N9etsVXcmYX/fU4FSh1juLjkSij7tfj3evSzj3h2St/Tkr5VSfSVJOjv3ZJCfXqWpL2/fdjdX++5mssee7xasbSzpZjT+m217rtt66HWb+mO6OfXzTDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAERklwtCFztFt14WNsJ+/e3MqUxVkbPfnJBS3mSNnVU4BbC1TnVbWB952skTTkd1sdOB7m0nSZVOUWvM6bZbMM/O6hv9bvCBJXZWlG9nXp92XkhBc6VT8bexys6S3usX8n5mO8fCKme/b8y1M6c6XZLkVFhnLJ22D4T16+zOxepKu3NdkhJOp/GY0V3tTHZn5fo1W9wxn1tld9Uudrbz3uqw8zrb6djsVmD3nnbLdfq9w3q6vQ7Jaqej1KuebElVdJ19DLXkb3UrK2vMzGtiPcTJSvv3dsfMy9/bzBJx+wzsEg8pdPfEWrCt44MP7H7TfUdk2PUOSdKgUfZxctJFZ5pZ2rswSiqZU2Zm731sZ/PrP3T2WuOOiU4kudzOEge5mxbva3dxFxTYx3ssYXcWb/g4pDN7qd1pv6bW/iDVJWHfP5vK17tjKmXnDXF7v6ucXVavs/vGJSmWa79G8Vz7mpBw7iuxkA8ndfX2DT+ZXGtm6XRYR7VtS63zKdX7bOJ1jjeGffJ18tBtM9inJP8TiI9vugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAikhUEQdDWkwAAAAAAoDPim24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDo7kBWrFihrKws3Xrrra22z1dffVVZWVl69dVXW22fAHYd5zXQOXFuA50P5zUyxaI7Yg888ICysrL0zjvvtPVUIvHxxx/riiuu0Pjx45VIJJSVlaUVK1bs8LjPLijW/372s5/t/skDGers5/WAAQPMc3Xw4MHNj9u6dasuuOACHXTQQcrLy1Nubq5GjhypX/3qV/r000/b8BkAmens5/aTTz6pM888U6WlperRo4eGDh2qK6+8UjU1NTs89vHHH9c555yjwYMHKysrSxMmTNjt8wVaQ2c/ryXpH//4h44++mj17t1b+fn5Gjt2rB566CF3mzfeeKP53l5VVbWbZrrnirX1BNCxvfnmm/r1r3+tAw88UMOGDdO//vWvnT5u2LBhOz35H3roIf3973/X8ccfH/FMAeyqO++8U7W1tdv92cqVK3Xttddud65u3bpV8+fP10knnaQBAwaoS5cumj17tq644grNmTNHjzzyyO6eOgDHt771LRUXF+ucc85RSUmJPvjgA02bNk3PP/+83n33XXXv3r35sXfffbfmzZunQw45RNXV1W04awCep59+WpMmTdJhhx2mG264QVlZWfrTn/6kc889V1VVVbriiit22KapqUnf/e53lZOTo1Qq1Qaz3vOw6EaLnHrqqaqpqdFee+2lW2+91Vx0FxUV6Zxzztnhz3/yk59o8ODBOuSQQyKeKYBdNWnSpB3+7KabbpIkfe1rX2v+s169eumtt97a7nGXXHKJ8vLyNG3aNN1+++3q27dvpHMFsOueeOKJHb6xHj16tM477zxNnz5dF154YfOfP/TQQ+rXr5+6dOmigw46aDfPFMCumjZtmvbZZx/NnDlT8XhcknTxxRfrgAMO0AMPPLDTRffvfvc7rV69WhdeeKF+9atf7e4p75H45+XtQENDg66//nqNHj1aeXl5ysnJ0RFHHKFXXnnF3OaOO+5Q//791b17dx111FH68MMPd3jMwoULNXnyZPXq1UuJREJjxozR008/HTqfLVu2aOHChbv0T0169eqlvfbaK/RxOzN37lwtWbJkuw/xQGfRkc/rnXnkkUc0cOBAjR8/PvSxAwYMkKSd/pNVoKPryOf2zv6J+GmnnSZJWrBgwXZ/vt9++6lLFz4mYs/Qkc/rTZs2qWfPns0LbkmKxWLq3bv3dv965TMbN27Utddeq5/+9KfKz88P3T9aB1fTdmDTpk269957NWHCBP3yl7/UDTfcoA0bNmjixIk7/eb4wQcf1K9//Wtdeuml+tGPfqQPP/xQxxxzjCorK5sfM3/+fI0bN04LFizQD3/4Q912223KycnRpEmTNGPGDHc+c+fO1bBhwzRt2rTWfqrbmT59uiSx6Ean1JnO6/fee08LFizQV7/61Z3mDQ0Nqqqq0urVqzVjxgzdeuut6t+/vwYNGvS5xwLau850bkvSunXrJEm9e/fOaHugM+jI5/WECRM0f/58XXfddVqyZImWLl2qG2+8Ue+8846uueaaHR5/3XXXqW/fvrr44ovDXxi0ngCRuv/++wNJwdtvv20+Jp1OB/X19dv92SeffBIUFRUF3/zmN5v/bPny5YGkoHv37sGaNWua/3zOnDmBpOCKK65o/rNjjz02GDFiRFBXV9f8Z01NTcH48eODwYMHN//ZK6+8EkgKXnnllR3+bOrUqZ/rud5yyy2BpGD58uWhj02n00FRUVEwduzYzzUG0B7sSed1EATBlVdeGUgKPvroo53mjz76aCCp+X9jxowJ3n///c89DtDW9rRzOwiC4IILLgiys7ODRYsWmY8ZPnx4cNRRR2W0f6Ctdfbzura2NpgyZUqQlZXVfB/u0aNH8NRTT+3w2LKysiA7Ozt48cUXgyAIgqlTpwaSgg0bNoSOg5bhm+52IDs7W926dZO07YcNNm7cqHQ6rTFjxujdd9/d4fGTJk1Sv379mv//sWPH6tBDD9Xzzz8vads/G5k5c6amTJmizZs3q6qqSlVVVaqurtbEiRO1ePFilZeXm/OZMGGCgiDQDTfc0LpP9P94+eWXVVlZybfc6LQ6y3nd1NSkxx57TKNGjdKwYcN2+pijjz5aL730kv785z/rkksuUdeuXflhFnRaneXclrb9ZyP33Xefrrzyyu2aCYA9TUc+r+PxuIYMGaLJkyfr0Ucf1cMPP6wxY8bonHPO2eF3V773ve/pxBNP5AeM2wA/pNZO/PGPf9Rtt92mhQsXble1M3DgwB0eu7Mb45AhQ/SnP/1JkrRkyRIFQaDrrrtO11133U7HW79+/XYXi91t+vTpys7O1plnntlmcwCi1hnO69dee03l5eU7/SGWzxQVFamoqEiSNHnyZP385z/Xcccdp8WLF/NDauiUOsO5/frrr+uCCy7QxIkTqe0E1HHP68suu0xvvfWW3n333ebfYZgyZYqGDx+uyy+/XHPmzJG0rQZw9uzZO/1vzxE9Ft3twMMPP6xvfOMbmjRpkq6++moVFhYqOztbN998s5YuXfq599fU1CRJuuqqqzRx4sSdPqYt/1vLrVu3asaMGfrSl77U/EEd6Gw6y3k9ffp0denSRWefffYubzN58mT9+Mc/1l//+lf+mzF0Op3h3C4rK9Opp56qgw46SE888YRiMT4OYs/WUc/rhoYG3Xfffbrmmmu2++HDrl276sQTT9S0adPU0NCgbt266eqrr9YZZ5yhbt26acWKFZL+3w+erl69Wg0NDSouLm7xnLBzXGXbgSeeeEKlpaV68sknlZWV1fznU6dO3enjFy9evMOfLVq0qPkXg0tLSyVtO+G+9KUvtf6EW+jpp5/W5s2b+afl6NQ6w3ldX1+vv/zlL5owYcLnuhFv3bpVkpRMJqOaGtBmOvq5vXTpUp1wwgkqLCzU888/r9zc3MjHBNq7jnpeV1dXK51Oq7GxcYfs008/VVNTU3O2evVqPfLII3rkkUd2eOzBBx+skSNHmtW/aDn+m+52IDs7W5IUBEHzn82ZM0dvvvnmTh//1FNPbfffgcydO1dz5szRiSeeKEkqLCzUhAkT9Nvf/lZr167dYfsNGza482lptVCYRx55RD169GiuKQE6o85wXj///POqqakx/4Ksqqpqu+f3mXvvvVeSNGbMmF0eC+goOvK5vW7dOh1//PHq0qWLXnzxRfXp0yd0G2BP0FHP68LCQuXn52vGjBlqaGho/vPa2lo988wzOuCAA5prw2bMmLHD/z77zzwffPBB3XHHHe5YaBm+6d5N/vCHP+iFF17Y4c8vv/xynXzyyXryySd12mmn6ctf/rKWL1+ue+65RwceeKBqa2t32GbQoEE6/PDD9e1vf1v19fW68847VVBQsF0twG9+8xsdfvjhGjFihC666CKVlpaqsrJSb775ptasWaOysjJzrnPnztXRRx+tqVOnhv6AQzKZ1F133SVJ+uc//ylJmjZtmvLz85Wfn6/LLrtsu8dv3LhRf/vb33T66afzt+vo8Drref2Z6dOnKx6P6/TTT99p/vDDD+uee+7RpEmTVFpaqs2bN+vFF1/USy+9pFNOOUXHHHPMLo0DtDed9dw+4YQTtGzZMl1zzTV644039MYbbzRnRUVFOu6445r//1mzZmnWrFmSti0QUqmUbrrpJknSkUceqSOPPNIdC2hvOuN5nZ2drauuukrXXnutxo0bp3PPPVeNjY267777tGbNGj388MPNj500adIO23/2zfaJJ55IbWDU2uQ30/cgn9UUWP9bvXp10NTUFPz85z8P+vfvH8Tj8WDUqFHBs88+G5x33nlB//79m/f1WU3BLbfcEtx2223BfvvtF8Tj8eCII44IysrKdhh76dKlwbnnnhv07ds36Nq1a9CvX7/g5JNPDp544onmx7S0puCzOe3sf/937p+55557AknB008//XleRqBd6ezndRAEQTKZDBKJRPCVr3zFfMzbb78dnHHGGUFJSUkQj8eDnJyc4OCDDw5uv/324NNPP92lcYD2pLOf295z+/dKsM+qhHb2v0zryYC20NnP6yAIgunTpwdjx44N8vPzg+7duweHHnrodmNYqAzbfbKCYCf/NhAAAAAAALQY/003AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQERiu/rArKysKOcBoAUybf7jvAbar5Y0euZ8dS8zy3Zu/YlYyMeCWJ0ZpdON9max7Iy2k6SY7G292dbGtphZ48omd8wtM+3MfmWlIZcVmdmi5EZ3zLyE/Wzq02l328zFncx5ddPeK2+/X9s2tbf1nmWjfegpHfL6xJzn4m3qnQ5hb0mD82yCJ9f5Gxu4Z0dplJl00a/MrEn/CtnvaDO5+PzxZvbb+3/m7PPakDHRFsLu2XzTDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEdvnXywEA6Nx6O9kAO8obaG819AB3xLyE9+vRmYsnEpltGPoj2fbHBu+XnmPeL1aH/Hq5t+PsdL2ZJWLOa7u//7rX1dWY2ebZ9na1f640s5KzS90x1yfXmlnMO07q7Ncv2/nV+G1ynMx7Q+35NIa8n2nnIIs5Pwne6P3KeMhxm47Zx4m7XTrs9bPxrVbn0SSnzkCvhGxtn0fL1ox0tvtLyH7R0XBNAAAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIhIq1SGjT3lv8xsztO3ZbxfuzxDeuPN1Wb22iynz0PSGy+/bmarVtr7/WSRt98qd0wAe5ohdlQ61oy6nXSQu9ejRg8ysxHHn27PptjeZ916d0glCp1ttzjblds1PZUr/EETg/LsMZ16oPXz7Wt4XY3fK1Rfu9HMFr1v7zdZ5dUR1blj1imkLitDOTlOFZTXvRRSvRTzKqiyo6k/y3bqqWJehVm2Pdd0o/+6l4wtMrOk7Fqwj52PCT3vWuaOOeyKUWa2PpayN8y2j794rIc7ZtrreQutj9u5VMqZq6REpoef1wsW8mk2W86x6bwGjU6FWSjvtUU79J6TLXSyrSH7XWEmL73krXS8+aAj4ptuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICJZQRAEu/TArCwz+9PsVWZ2xmH7ff5ZAZ9D2Xq7R/PVWXYn+3Knk33Rx8vdMd/557tmtmFlhb1h6h13v5naxdN4B9557Rn97d+5+bjTxptZQbKPmf30jEucvc4ImVV/O8oebkZd9rdLqEcda28nSXk5vcys18jRZtbY295u8BfsTJLynM7inEK7F7bE2WfCHdHvzvUaeb3W5vUL/C7fdD/7uRTsbe+5usneZ2qdO6TS5XZP9+LZS8xsVU2mr4JUV2mP+c//Odnd1jPwiv3NzGsPzg4pPG6U3QmdTvud5JmKtUXfsXPQZ+fa72nFXLuLe/NMf8iRB3Y1s5wp9jV1fa3deZ8tp69dktJ2X3mj8356PefpsG5rJ250eutD9+sOGU2HvMc+U6TUg0sz2mem92y0V72drGq3zQKtI+yzON90AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESkVXo4SvrvGbVgayvsWpjXZtvVVJJ01uQTzOy3D79gZnfd/DMze+r1v7pjvvj3V8zssrPPNrPxx00ys8efesQd8/d33W9mP/3ht+wxjzzLzP752qPumC8+9KCZPfei/dr+8Vn7uQzo1gYVNR1IccE+br5s5mIzKz3Vvl4cc7ZdkTTz0bKQWW2yI7uFRjlO/Y/qQo4DJ09V2jU+Jf3sGp+8ertCSpIKi+3Xr5+zXbaThVWGJZ3MK+Kp3+Jsl+e/tqmEVw9kj1ro/FXyxlx3SCUT9vtSsr/9uhfn2a9uMu2/utXvfuhPKkOxhP36Zjsvbcw9UqSY89pnu29pZu/nNvYJ7B9F9nNJO1VZkhRL2NvGYvZxMurIcWa2LGeeO2bZM5+a2fjnXzOzwpOOMrOKGvs6JEk5Me/4zOw+GMv2j6F0zHntvcPEnU7YcWtv7DWRxWLeMeQOKffGA0iiFmzPwjfdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFplV6kQ4tbYy/tQ8NGuwOiuF9I34zj1W9fZ2a/vfsOZ8taM/nOGV93x3xppl2XJTWZyeyX/mxmkw5b7Y5ZsdSvJzHHnPWYmc3/4GZ32x9cdUFGY941dYCZffdb57nbPvfgH8xsWZldbXXIf4w1s7Ou/Lk7ZnsS6+9Xhi34+1wzK/7Yrs0ZdtJAe58fn+iOudarXsqzK35yc+0su87vhMnJty+hOU4lUSJl14KlltS7YzYOtKur7KuF5F297DLE8DzbvpS47T8q9mui0k6NVMUm+zoTr8szs1S5O6QqVtr7zcmx35e8fKdqrN/e7pip3EH+pDKUHbNfv5hz5495oeS+qV7dWDptv35eDdlnj7CnY08o7VQ2JZx6OElKx+z9VlTa2QdV9vNs0kHumCq176+z59rVQifmzjaz2PH+dTNZZY8ZS9jvSzodUa2m08MVc7Kw9i5vttnuMe9loZ1hANCMb7oBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIhJR50PbchpsJEn9Yz3NLNlY06pz+cxv776x1ff50sznW32fYea9/9ZuH/OgL9g1Ui3x6F3TzCw1/01320T1YjMrKUqYWbzGrw/qMHr7dTvK72VG9c5lp/TQoWZ20Kl2zZYkxfv3MbPq5CYzq8u2a3Hqa+3tJKnOqYqK19h1MuUfJ80sr8B+7SQpvsqutUrva7+2q7vYc90vpLKpzqvGcaqDvFYhu8zpf7d1ssr5ds1Rstyuq6uv9St+qsvtCecV2e/LiFihmVXW+jWKiX72ti3hVX+15MYfc6rIsp3KMJ9/PYk7E47n2mGq1p7P4n+udcfcMtsZNLXS3XZ3+9vMT81sZM3T7raFU443s4019vUvlnDOe6faa9sD7G29qjvvwPXqxLbNyamsy3YLxdz9euo650ds7LG6h+T/aSajDz7FzLxTt+z95e6Iw79gV27+8Ednmllxf3ufP7pqiTvm3Nl3u7mHb7oBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiHTKEsHvnTrRzdc7Xdx5znZ2+7K0wR0R7dHa1FYze/qZN9xtTx1iZ8MOLjWzwnxnpw0r3DHVbYCf70a1NXZPsiR99Wq7H3G/wfZ2KWefBaNH+5PqZ3egF6bsvtkKp/M5tc7vWF5fY3f9Nqbt7uHstN35nKqx+2QlqbHOzlMj9zGz2P7FZlaX8PvjE3vbrdq1dfa7Fqvz+tr9Xt26avt5rl9qd7avWrrQzJKV/vup2gIzql9n94Gu8u6kdf6Yeb2j6ul2+o6dzuJ4SGe296mhsc7ZLG3fQXv18+680vpq+xgre3KFveHsKne/e4Kyd/38mNzZZlb8lePMrMLt8PbHjDunfqPTi51O29chr4d72wO8bnD7mPfOo7AxY861OnPOhw8timA8RCvXTHpkf8vMpnz1XHevx5400szGjLe367evne3VBl/TzvdrujVsoJ1lOt3Sfva9XpLm6qUM98w33QAAAAAARIZFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAAR6bCVYf+R1c3MZutTd1uveCjpZKv8KaEd2ievu5mtTdqVYXZpyTZDhtv7LRlgVzO9VzbfzO484lR3zK9cd6+ZnXbyWHfb1vbS/S+4+aj/ONTMSmXXU/3LbqGRQiqmvDgRt9/RgiK71irp1GFJUnWVXQdVV+tUaVXZ2cak07skae/+dkdGQcx+Ljnl9lxjtXbNliQV9XZuFU4zlertsHadXfslSY1J+zUqX2PXvK1eamdr3y1zx9Q658n0XWxGSdkdLHkxvw4rJ2ZfEzR4uLutJ+bc3uucqqOwDwXZztUxr7d9Ei5bYb+28747K2RU/56+Z8h3Mq/mrdzd68xZtWY2YMUMMxt29VlmVp+yq70kqS5hHycxpxZMdc52sbC7ti3mvH6xmHMe+ZdqNcZC7lkZyfx5diwjnOxIMxl9sP9Z6Ovnn25mp3/VPg72tW+tiNBwpxKsJZY4C7rXZr4SsvV+GY/LN90AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWnXlWFLnvqLmcWcCpGw8qQip3Gh0mmraAjZL9ofrxasJXr1tqslvJqpN+YsMrOH7WhbfopdwxUEgb9xa5s13Y3/v3Ps16D6povMrLHePgGrl/r1XfVLVphZOmGXAcbznZqtfn5PyPp1dj1VxVJ7Pk1J7yJkbydJa73KmAed+fa3ay667OvXWhW4qS1VY7/uddV+ZVhTndMf5zXxJL3iR6eeS5LkXC+c5qV505bY4YH+HWm4U98lZV4Z5r9ImVcZJXLt6q+ZT6+wN5y5MuMxIUk1GWaZW+H1pN7ymBmN+dEF7n43puxjPu3UbKVjzrkS87oLpYRznqWdmrK0e6pEUQkWZoGTlTrZySH7HWQm3QoGm9n5F33JzK66wl9iDCoMmRI6lCUVdnbe2beaWcn+di/Yo3+wK95a4rm/2+duPM//rLlv3lUZj8s33QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARadeVYc89eJeZVbdgvxVOLZhfYoM9xZqQ/K2ldi3RxvTeZlYf6+rs1a7Ba38q/XiuXSl234X2WTZ6yngzq4v5/SKxhFMZk2tXYsVq7CqZhLw6J0nOtaRpUZmzYUT1Sc58tMyOmpxMkjZkNJk9yWt29JGTSZpffZYd3vXVDOcjpZ0KpVjCvvVnx3q4+33nn07t2kynVw3t0nF32sffh/9818xW/NnuuKy44T53zC//6HtmVp62axjz8uzreH2jX9+VrLOv5TGnhtGrDIvF/I/Q9e4FOQr2PfLE037lbnnVdXZW7HRGHlASNid4NjhdxKl1dvbeTP+zyYL5do3lO2WLzWzRSudun3DqSiXNf/8VJ33YTGbPsrd67v5L3TH/eP+dZrY+ab9GP7n592b2SeVyd8wBX7ArzqSj3W35phsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACISLvu6X56ht1vusDZrqn1pwJs57czvU5tu/S4Z+tPpZ2qtaNVdn919coBZlYf83tY404Xdzxmd6cnVq5w9ppyx6xzOr4j6+JG51L5mBM+mvFuY7EcM8uWfS7l5fRy97tlBl3cHUppPzeuX2P38q6ttq+bUnczaajc6o454we/NrMTbz3HzN6abffn5vU/yB2zsK99f0ikk2aWStvX+HTa7+HODrlntT77/fryUf6Wx4xq5am0U00b7WzZx3b23jx7w6ef9/qppQUrK8xs3kevO1uudbJB7phyuuf9zzWJDPcpSc6LqyInqzSTzfqNO+JXzv+Dkw5wstVO5nx+lbTifS/9vrst33QDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARKQdVIatMJNVzlbUgqEjsotJ9iSLzGTFrPn2Znmb3L326GtXHeX138feMGHXrDTW1Lljbi73KjKAtpPjVIbJqTJKyK5WQrS6OVlDpjtd5tUOSbNubYMKOKdp8eUfPmxmDU7T0SeyK2Yl75Omb99vn2pmRf1CKpRiu/sjtn2PTK3YfbNoS2VeA6OkO6+628xWlS80M++TwPqQatFV3gGv2U7mnZv+mH5dlserwQsb0zPeyWa0YL9ePaFXLt02+KYbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICJtXhk2/4HfmZn34/Te3xYkQsb0fvo/0yqyU0JaVoYNt7NlNXb2r0o7q3MaYSSpwumnaqK7qk3kOUfuJxThSZVzncyr3ZC2VNqVYUrb29btP8DMPplZ5o6p1NN+DrSRnGy7Ci+Wa2/3zPm/jWA22BUZ14K5OtZ9xasFawvra+xayKKhg9xtY7Hd/WSWm8mq+X51nDY5tZr2paRNbHjbzu7+wSPutl4tmPcxvkT2B+5R2s8dM+Vs+5yz3QZ5z8WufdzGezZxJ1vvZGEVqd7x7r1G+U5WEzKmZ4iTFTrZ6pD9rsxgLtvwTTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARKKvDNvi//T6HTffZWbZznZeW1ZYZZi3X+8F8X5gvjqkguu52XZmlzxIW7ydVvtjov2hFixMlZPN8zdN2mfoljL7BN1S51SRpWb4YwLtVMy5meXlh3RcApAkfXH0UDNLyK+xTLufNqPgzCfXry+b43xGbXTqqYr72xeaAcPcIV1bFtvZ72+2J5uu9j8YD1OxmeW4hcL2a5sOqdJKOrk9G2mDW+3lH3tSHzMZqoPMrNGp/ap068SkzVrhpJnWidW4Y0r9nexMJ/PmY7922/ivg4dvugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICItEpP9/HDDzSzf360wN3W7aHO0OaQfC8n8zq+P85gLgBaS03meWq1nb07P4O5oGPKDckHOlkvJ2t0MrvjVpKUM9bPM1Tn9JDmiZ5u4DP/df+PzGx5wj5XyiudMmmplT5hfx41dtTP3/KQ4+1s2Tz7ifz+xy+ZWTrpj5mTdvqtnYrqVKXdUV1S4F/bEmn7uaTq7Z7uWrfX2X+j417PubPtAg03sxy321oapgIzG1K6t5nl5dv7TacHuWNWV9v93xXrKsxsWeMAM1ujD9wx5XarlzmZ936GdaBvDcltfNMNAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEJFWKTR4KaQWrL3xKsXC6sYAdERexUP5bpsFWkt/JxvqZF7tl6R4sZ0NdypavDtp2qsmkc668/tunimvxKyuNqwSBYjO2BvOMLPSkfZ5lpOXY2br5zv1U5Ke+e5vzOz28282s3Nu+oaZlQw9wB0zWbPezXenNz7+0M27dLHroAYdYm9XnLbfk2XzVrhjFu/fw8zq03ah8JD+9nU8p7/TNSYpx2mYWv+xve2iVfZ1fKP8Y8+rIi6R/fodK/s9qXf3KtXJfr8/WLbczHK0j5kVFzn3R0nFBfa2pf3scyU2d5OZrdEQd0y/+st7X+y5RolvugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAi0iqVYQCAf5fvZDW7aQ4d2Rg7yj7azvLtOqy9cvLcEVOxbDNrqqwws277F5hZzlCnakzSqEPtypiWOPbQ48wsL9ce8wnZ1UrAZ0aefY6ZTX/k5+62tU6hXX2TXQGUTtt9TzlH2+egJN182fVm9vufTDOzu6+90cwu+8X33DHP/YH/OuxO1Uu9EkFp3rN2liizs5yP7evtf44/2B2zcIB9HVowf569Ybl9jBT382shiwcU2lmuvW263n79UpV+HZt9V5HizjKsyKkFS8rpPgsZM6WkmS1wKrg+qFztjhmrtOeb48xohRY6ez3ZHdPnVWMOcLKwpfFfPv9U/hffdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhMowAIiEV1fR3cm2tvZE2tAQJ/PrR6SRZtKln1OzFbNrS3J77+2OmKiz37MNyz62hyyy5zph+CB3zME1Tmi324T671HfzGi7/yq165MkqWHZGxntF53LhPEnmtlw+TV5Lu+roG6Z79Zz+9SfmtmpX/1PM/uvSV9291uQZ5/AIy/5cfjEWtGwfYf7D6iyo/J/bnJC+5pZ+h/+Baxwf7uiK1m33sw+mDPfzBJ1duWVJJU6lWJFTr1jY759X4nP6eOOmazeYGYxp6Yy7eyzMe19vpBT/CUtcu5z7yTtOrFV2uiOuUH2cbJBduWmNNvJit0xJa8iznsVvM8fYTdep7I0BN90AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABGhpxsAIuH1bec62Wkh+13oZAtCtrWE/f1rk5P1czKvrzesy9fu2Ezsa2+1pSZuZtU1fodrXr7T+Xmw3bc98PjxZjbxu3YmSbGI/ur7kwy3m/3BK24+JqdrhntGZ3L2t042s7Uh29qNxz6vt9hvLZbsNmSp0clGDR5tZvfN/8gd8+4r/tuf1G5UsXSFm78x8wAzyym3O7Pz8nPMrFfB3u6Y3Yba2SFF9uu+qsbug15VbfdMS1IvZ9tSZ755wweY2WH993HHrFxqnxH1tXVmVtjP7otOxPzlW3m13akdX2p3ZseW2tsNq/TOFOlt57PJx27Ht3f2fuiO6fd4e/3p9uePnrI/Q0hSnkb6U3LwTTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARKgMA1rZaLtBQyOcpoE3l/r7/bgys/mgPap1Mr+ySTrOyeyaFWmFk/kVGW5txxe+amcxpxzoXbu2ZBun8iRtV5dsqbWzhlWL3RHjx5xiZnt9wa4YGffNE8xscMhfbXs1SC3x4Ad2lpdnZwV9wz4WlDrZspBt0VmM6GbXK/mlTWHsM8I7MmMt+Dgbd8as1wYzK5FTMSjpf+64J+M5WXqou5ltca7ji8u9qknpjTn29W1E2q61KijqYWY9+oe8J8PtqMt8OyuI2a/7xhqvmkpa9f5qM0uurDazWMJ+LsVD/cqwguF2XrHSrhMrHmq/Jz3y3SGVV26/Rkn77VRO2v4AGyvyPyfE3rd3vEn2675WdjWa5BwI/7u17Wgz6eNUjQ3OC/k8lNwSMicb33QDAAAAABARFt0AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARITKMKCVfZCys/jHdtavwN9vLKzVCZ1ETUj+ZyfLd7KDzKRbv0HuiA3lzkH9vn1Q7/v1M81sTWVIWVa5XTFSv9KeTxfZVUZNcirMJCXy7PqbEaPtOrYJg93dunIz39T14p+etUOn/sark5GkY374czOb+YuzwqaFDuSdILOeyrBjOq16M8vRcjOLOTWCqQb7eiFJleX2trFs56NwzL6eFBTbtUOSlN5iP5duPc51t7X8h+ze0Ze03szCPuzn9rafZ8z58FEfs99L9Q4Z1FNmR/Xz7fcyL+l/UEo49WcbP7Yrwypr7TGrqwa6Yx4yZbyZ5dXZ1V4x703zWkclpWvt9yUvbd97G2VXbqbTflVWsXNeF8quIlsrr3LNqwQLy5eYSdqpMCvIHeqOmAo5xjx80w0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQESrDgFbW4GSz7UYKycuAXVLjZG+YSUP54pD9etVBQ8wkL21XayROPdIdccnddi1Ydo5ds1K8r12pU+HUs0hSyTF29cuXj9/P3s7ZZ0gxWmTeevJBM/vko5ecLWtafS5ovz5s/NTMhjkfEVNuVU+FO2a6yan3SjuVRevsHqna8oXumBXlTp3R0LFmlkrbzyVe79cZJdfZlWEHHJZZZdiBOtrMXtIjZtYrXuju94vj7etbkbNdduUKO6xxh5Tm2VGTt61Tl9UrJ88dsl8/u56qrrd9jMTX2cdsXq5/XxnQ36mYGmrPp+lde7OF/5zvjrl+qT3f6kq7Wi5ZY99303VOPZykmJJmVii7jlOyj73wZao9pift1OvF3flIaVEZBgAAAABAu8OiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAi9HQD7UTY34AldssssGc6NCSvczK7q3bV/CVmdsiUI9wR08eNd1L71pU3wO7YLO7v96lOPNXu+B7s19yawnq6o+rxLui9t5l90s9+nip/LYLZoK1c/9AsNy/uYp9LXtt2Wna/cFw57piJLsVmVl1nj9qYsHuCa/sd4I45bLR97ud1s8/CiuV2//dGp7tZkuK5mff5WnqNdc7duX8xo8aQT/slw+2sNNfOKv5pv66f2JXPkqSe+XbW5SQ7Ozxmd5XXercqSQnndaiu3GRmQ/r3MbPhp4bcHHo7WbkdJZfYXdIVZXYHvCRVr7SPTe+zZJFzzKa9F09SfbX9hhe51wTvvhy2TLWvQ11kXxNOH2sfQ8W5ftf7gnL7mhCGb7oBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIkJlGNBONIXkIU0YQOaG2LUbktQzf6CZfTJ3npnV1yXNLF7g1wqVjrbHrK2091tysF0/csh/OHU7ksaU2Jk3W7vIKFxU5/VRY+1apnEH26/tw49u9Hdc+UGmU0Ib+OnXj3TzgaMDMztkmL1dtrPPtOy6Okmq9/Jcp4ps79Fm1pJKTa+2b8DAk82syT6N/ldIZ1YGXnSut5JdMbU25VfH/WSqfd0cN9TO/nPoADPr+QX/OHBfHqdma68pTubUm0ly3+yec+z5bp7vHPFhvY+1TlbjzGeoXUV2SMq/l31Q59TrVdv3z7wCe8yw+1y6xr6bFTfaNVxHys5mya7B22axmTQ5S9y6uuPMLK+fXwFXLv9c8vBNNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEqAwDOoiwSjEgY5V21Ywk5fUeZGaxEq+6xK4mSVeudceMJ+ySrmKnFuzwKfuZ2WB7M0n+DdFrt/EaY1q/NGjXrE/aI6djdv3NgKFj3f2uqIvbYfKd0HmhfXnod7ea2fJj7bqs/YYeYGYlRf6Yjc4J08vp/sptwSdWr5pvlRMmnPnkdPPHzHOKBnv6m5omHGlXHc2eNd/Z0jlvJQ12qr9GDLArlJIfLzezzTP8N6yuarWZ5fS2x+xxsFNxOdQd0s+H29Fe5U5pZFlIZ1iN8zp4t0+namyvXPs+J0ljnMqwD+Z8aGbVyU1mtjHtP89koz1maT/7Pasut+9V4eVcXlmgPd/kSrsas663/9rGWrB05ptuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIhQGQYAe7pkoxun8+y6mV5OrcnGSjurmLPQHXPIUXZ/y6iJds/KQKcWLKTYRdVO5r1CdgGXZJeobBPVTbg6bb9n9bV2LUxBkVfBIpV85Wgzm/V3p1anxh5zn/57u2Ou/eg1N4cj5yQ3Lsy36wAr19jnaLZz5Nav89/PdMI+K1I19plWmGvvs19+D3fMnAK7girHOwmdi8aqGndIxZz9jg+pL7Tk5NnPQ/KqjvLc/Sbq7fckvcKud6x4d4mZLUtuccfMSduVksucKrIDsw81s716O3Vikt8dl+9k+zvZypCreNhNwLKvk4UM2e1Q+7we4lR/fTTffj8bnRpKSRpSVGyH2fa2/ZwX6Bi3U01K6mAzy5FzkiXturp3XvMrVKvl5x6+6QYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiQk83AOzxnnXTZP0RZhYrsrswvc7dtNehKWnwMaPNrHiw3QNsN7/6XduSX+Hq8W6kKfndptlyuq1bIJG2u08Tdc6c8r0eYGnBGrujdMAXDjAz71n6zeDS2o/KnNTu/5aaQva8B0g978aP3fCCmZ1yw/1mtr7afk/iCf/cLtnfPsaKc+wjJTeRbWbVCf+4/WC+fdzmOD3Befl2v3VhP7+PvCDswM7Ae+9650Lm/cGryux+5pI8+z0prrOvqnmxuDtmXsx+baur7OdSXb7RzPaqCenprnayfCcb7mQF/pDumE4PvHtDCuv+di64e/Wz+9yLarzrqX98ZSftbVPJDWa2n/Nkviz//cyWfQ42Oi9CpTPmgka7l16SKmSfK2H4phsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIlSGAcAeoauTfepuuXnOPDMr+MrXzCzHKYMaMnqoO2bxULt2qNbd0uP1s0hqcLJu9u2y3tlvMqQyTPJqdTK/RefF7EqUwv52vVKq3itdk+r62vMdNvJQM4s5L331utXumMX7/9jM3ps718zWlNnHreqXuWPuMUrGmlGBc2w+cO2lZnbWTb9yh1xWZp8T9UV2nVF9oz2fkpDapoFOTdk/Z9vHX07KPnAravxze9xIp7eph7upaXm5d66E9UjZchL2XPN629finHq71yoeWhlmV8DlJext66rtyjCVu0NKfUNyS66T7RuyrddVWelk3nMJ67d0rrdN+fZ7He9rnyexVMi9LGm/L4k6e8LeqWuXyv3vnJwsUWJvPdg7pmv8ysN3li0PmZWNb7oBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIkJlGADsEbxaMK8PRVLqMTNaNfMgMysqsmtfivvZ1UCSlPzYrntKjTzAzGLd7H6W+BZ3SMWciqR0nb1xyulvqU74g8a62bVq0t7utp4RBw83s7zeg8ysUBvc/SbrnLqnXPs93bufXRGXl+t/FInF7HzcaPv4W7/yCDOrrN3kjplM2s9z2Zr1Zrb63dlm9sUvjHbHXF9pVz71KtrHzPoNsl/3wn52BZAklQ63j5OY03V0+S9uNLOS/sXumPF8+/ird87B6qq1dvbREnfM4v4DzGzMcPu1jTnHZnWNO6RWed2GfiuRKdvd0KkoCxHLc7Z16rvW19jnUYXzfklSorc9ZsI559O1TnXVSvvclCQVOOeDV8PlvbTeJVxy67u2LLWz+ho7i/cOGdOZbyrfyYr6mFm6yq+TbMxxqtxq7GtbrN6+Ryblj+lJJOzrUGE/+5wf4mSSNHCZX3Hp4ZtuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIhQGQYAe7zBIfl7ZtJU/oKZJbPHmll9yN1no1MxEi+z64FiTg1NzKuLkRRXth06NTUp2XNtrHP6YiTVy6lZKcy8MmxgX/t1WLDOrjzJ2deve8pxXqN07WIzq6+0+3gK8o90x4zl2ZVFI0YfbG83fICZPff8X90xV3283MwKBhxqZkOG2llpSLXQspX2mOmYU2fn7LPIq4KS9N4/y+xt+9vnS/H+I80sJ88/z5RrPxc5lWHDiux6s+ryFe6Q69fYVXiFfe2KIOfQUzzfHVIpp9kqUwm3Mswb0D62JOmZRfZxUrHIfj9LZFd0lVTa75ck5TmVTgnneM/xrqnr/CpArXReP+emtDluj5nO929mOU6ccp5KymnLysl3h3SvCRVOlorZx0E63z+v69L2sZBO2fe5Rme2FQqpdpRdN7a+3L5/9orZ2eq0/356tYZh+KYbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAg93QCwx1sYkuc7md3NvKV+qJlVOx2kklSXcnpaV9o907H+eXaW9vs1Y3XOpGJ2B25d2u7prk/4t9l4lb2tQuqOPYkqu5e3vtzuTK1udOYjqfTAA5zUfq6plN3Tnar50B0zuc4uuC4ZMMDM6pz+9DFDB7ljfnG43UNdMHS8meU5Pearavzi5mNz7ddvVbn9fiZr7a7anIR/zBc02vNNOiXC8VrnOEn4z7NXwtmv0xNcmO9kMa+/WkrV2plXn+51LId+gE57bcmZffwuKLKvb6r0zt393P12c/q/Vzkd32mnp7t46AnumHkF9gUulnR6sdP28b6l2r62SVKPlfbr3qQ+9ob9/L57j9fFnXDezmSVnVU7mSRttC+3WlZrT6g66VxLvHNeUmOq0Z+UIeVcp6tl92lLUqPs96XaOceWla81s7eSdiZJZZrnpN91t+WbbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIZAVBELT1JAAAAAAA6Iz4phsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6O5AVK1YoKytLt956a6vt89VXX1VWVpZeffXVVtsngF3HeQ10TpzbQOfDeY1MseiO2AMPPKCsrCy98847bT2VSDz55JM688wzVVpaqh49emjo0KG68sorVVNTs93jqqurdcstt+jII49Unz59lJ+fr3Hjxunxxx9vm4kDLdDZz2tJeuyxx3TwwQcrkUioT58+uuCCC1RVVbXdY7Zu3aoLLrhABx10kPLy8pSbm6uRI0fqV7/6lT799NM2mjmQuT3h3P7HP/6ho48+Wr1791Z+fr7Gjh2rhx56aIfHVVZW6vzzz1dhYaG6d++ugw8+WH/+85/bYMZAy3Beb7N69Wr95Cc/0dixY9WzZ0/17t1bEyZM0D/+8Y82mvWehUU3WuRb3/qWFixYoHPOOUe//vWvdcIJJ2jatGk67LDDtHXr1ubHvfnmm/rxj3+sXr166dprr9XPfvYz9ejRQ2eddZamTp3ahs8AwL+7++67dfbZZ6tXr166/fbbddFFF+mxxx7Tscceq7q6uubHbd26VfPnz9dJJ52km2++WbfeeqtGjhypK664Quedd14bPgMAO/P000/r+OOPV0NDg2644Qb97Gc/U/fu3XXuuefqjjvuaH7cpk2bdPjhh+svf/mLLr74Yt16663aa6+9NGXKFD3yyCNt+AwA/LtdPa//+te/6pe//KUGDRqkm266Sdddd502b96s4447Tvfff38bPoM9Q6ytJ4CO7YknntCECRO2+7PRo0frvPPO0/Tp03XhhRdKkoYPH67Fixerf//+zY/7zne+oy996Uv65S9/qWuuuUY5OTm7c+oAdqKhoUH//d//rSOPPFIvvfSSsrKyJEnjx4/XKaecot///vf67ne/K0nq1auX3nrrre22v+SSS5SXl6dp06bp9ttvV9++fXf7cwCwc9OmTdM+++yjmTNnKh6PS5IuvvhiHXDAAXrggQd0xRVXSJJ++9vfasmSJXr55Zd1zDHHSJK+/e1va9y4cbryyis1efJkdevWrc2eB4D/Z1fP66OPPlqrVq1S7969m7e95JJL9MUvflHXX3+9zj///DaZ/56Cb7rbgYaGBl1//fUaPXq08vLylJOToyOOOEKvvPKKuc0dd9yh/v37q3v37jrqqKP04Ycf7vCYhQsXavLkyerVq5cSiYTGjBmjp59+OnQ+W7Zs0cKFC3f4p6Q78+8Lbkk67bTTJEkLFixo/rOBAwdut+CWpKysLE2aNEn19fVatmxZ6FhAR9JRz+sPP/xQNTU1OvPMM5sX3JJ08sknKzc3V4899ljoWAMGDJCkHf4zE6Az6KjntrTtG+yePXs2fzCXpFgspt69e6t79+7Nf/b666+rT58+zQtuSerSpYumTJmidevW6bXXXgsdC+hI9oTzevjw4dstuCUpHo/rpJNO0po1a7R58+bQsZA5Ft3twKZNm3TvvfdqwoQJ+uUvf6kbbrhBGzZs0MSJE/Wvf/1rh8c/+OCD+vWvf61LL71UP/rRj/Thhx/qmGOOUWVlZfNj5s+fr3HjxmnBggX64Q9/qNtuu005OTmaNGmSZsyY4c5n7ty5GjZsmKZNm5bR81m3bp0k7XBit/SxQEfSUc/r+vp6SdruRv2Z7t2767333lNTU9N2f97Q0KCqqiqtXr1aM2bM0K233qr+/ftr0KBB7lhAR9RRz21p21+Uz58/X9ddd52WLFmipUuX6sYbb9Q777yja665pvlx9fX1O70G9OjRQ5I0b9680LGAjmRPOK8t69atU48ePZrPb0QkQKTuv//+QFLw9ttvm49Jp9NBfX39dn/2ySefBEVFRcE3v/nN5j9bvnx5ICno3r17sGbNmuY/nzNnTiApuOKKK5r/7Nhjjw1GjBgR1NXVNf9ZU1NTMH78+GDw4MHNf/bKK68EkoJXXnllhz+bOnVqJk85uOCCC4Ls7Oxg0aJF7uOqq6uDwsLC4IgjjshoHKCtdObzesOGDUFWVlZwwQUXbPfnCxcuDCQFkoKqqqrtskcffbQ5kxSMGTMmeP/9991xgPaoM5/bQRAEtbW1wZQpU4KsrKzm87VHjx7BU089td3jvvvd7wZdunQJVqxYsd2fn3XWWYGk4LLLLgsdC2gvOK9tixcvDhKJRPD1r3899LFoGb7pbgeys7Ob/9uopqYmbdy4Uel0WmPGjNG77767w+MnTZqkfv36Nf//Y8eO1aGHHqrnn39ekrRx40bNnDlTU6ZM0ebNm1VVVaWqqipVV1dr4sSJWrx4scrLy835TJgwQUEQ6IYbbvjcz+WRRx7RfffdpyuvvFKDBw82H9fU1KSvfe1rqqmp0V133fW5xwHau456Xvfu3VtTpkzRH//4R912221atmyZXn/9dZ155pnq2rWrJG33I4nStv9O7KWXXtKf//xnXXLJJeratatSqdQuvU5AR9NRz21p2z8lHTJkiCZPnqxHH31UDz/8sMaMGaNzzjlnu99nuPDCC5Wdna0pU6Zo9uzZWrp0qW6++ebmb+f+/RoAdHR7wnn977Zs2aIzzjhD3bt31y9+8YvQcdBCbbvm7/x25W/XgiAIHnjggWDEiBFB165dt/vGaODAgc2P+exv166//vodtv/6178exOPxIAj+39+2ef979913gyDY+d+uZWrWrFlBIpEIJk6cGHz66afuY7/zne8EkoIHH3ywxeMCu1tnP69ramqCU089dbt9n3POOcFXvvKVQFLwySefuNv/7Gc/C3Jzc4O1a9dmND7QVjr7uX3xxRcHI0eODBobG5v/rKGhIRg8eHAwduzY7R775z//OSgoKGieQ9++fYO77747kBRcfvnlGY0PtAXO6x2l0+nglFNOCbp16xa8/PLLGY2Lz4dfL28HHn74YX3jG9/QpEmTdPXVV6uwsFDZ2dm6+eabtXTp0s+9v8/+e8urrrpKEydO3OljWvu/tSwrK9Opp56qgw46SE888YRiMfvQ+slPfqL/+Z//0S9+8Qt9/etfb9V5AO1FRz6v8/Ly9Ne//lWrVq3SihUr1L9/f/Xv31/jx49Xnz59lJ+f724/efJk/fjHP9Zf//pXXXzxxa0yJ6C96KjndkNDg+677z5dc8016tLl//1Dx65du+rEE0/UtGnT1NDQ0Pxt3+TJk3XqqaeqrKxMjY2NOvjgg/Xqq69KkoYMGdLi+QDtyZ5yXn/moosu0rPPPqvp06dv94OJiA6L7nbgiSeeUGlpqZ588sntfi3Y6q9evHjxDn+2aNGi5l8MLi0tlbTthPvSl77U+hP+N0uXLtUJJ5ygwsJCPf/888rNzTUf+5vf/EY33HCDvv/97+sHP/hB5HMD2kpHP68lqaSkRCUlJZK2/RL5vHnzdPrpp4du99k/PU0mk5HOD2gLHfXcrq6uVjqdVmNj4w7Zp59+qqamph2ybt266ZBDDmn+///xj39I0m67BgG7y550Xl999dW6//77deedd+rss8+ObG7YHv9NdzuQnZ0tSQqCoPnP5syZozfffHOnj3/qqae2++9A5s6dqzlz5ujEE0+UJBUWFmrChAn67W9/q7Vr1+6w/YYNG9z5fJ6agnXr1un4449Xly5d9OKLL6pPnz7mYx9//HF973vf09e+9jXdfvvtofsGOrKOfF7vzI9+9COl0+nmvk9Jqqqq2u75febee++VJI0ZMyajsYD2rKOe24WFhcrPz9eMGTPU0NDQ/Oe1tbV65plndMABB+z0F8s/s3jxYt1zzz06+eST+aYbnc6ecl7fcsstuvXWW/Xf//3fuvzyy919o3XxTfdu8oc//EEvvPDCDn9++eWX6+STT9aTTz6p0047TV/+8pe1fPly3XPPPTrwwANVW1u7wzaDBg3S4Ycfrm9/+9uqr6/XnXfeqYKCgu1qAX7zm9/o8MMP14gRI3TRRReptLRUlZWVevPNN7VmzRqVlZWZc507d66OPvpoTZ06NfQHHE444QQtW7ZM11xzjd544w298cYbzVlRUZGOO+645n2ee+65Kigo0LHHHqvp06dvt5/x48c3/60g0FF01vP6F7/4hT788EMdeuihisVieuqpp/T3v/9dN91003bfej388MO65557NGnSJJWWlmrz5s168cUX9dJLL+mUU07hn6yhw+qM53Z2drauuuoqXXvttRo3bpzOPfdcNTY26r777tOaNWv08MMPb/f4Aw88UGeccYZKSkq0fPly3X333erVq5fuueeeXXgFgfZnTz+vZ8yYoWuuuUaDBw/WsGHDdjjnjzvuOBUVFXkvIVqiTf+L8j3AZz/eYP1v9erVQVNTU/Dzn/886N+/fxCPx4NRo0YFzz77bHDeeecF/fv3b97XZz/ecMsttwS33XZbsN9++wXxeDw44ogjgrKysh3GXrp0aXDuuecGffv2Dbp27Rr069cvOPnkk4Mnnnii+TEtrSnwnttRRx21y6/D/fffn8GrC7SNzn5eP/vss8HYsWODvfbaK+jRo0cwbty44E9/+tMOj3v77beDM844IygpKQni8XiQk5MTHHzwwcHtt98e+mOKQHvU2c/tIAiC6dOnB2PHjg3y8/OD7t27B4ceeuh2Y3zmrLPOCvbbb7+gW7duQXFxcXDJJZcElZWVu/xaAu0F5/U2U6dOdV+H1vhRZdiygmAn/zYQAAAAAAC0GP9NNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABCR2K4+MCsrK5oZFDhZwsniGW4Xluc62Swn856HJA11Mmc+XdLOdiHvXlOtE871tzV5r3uY+hZsC1cQBBltt+Xt9WaW3L/QzOq941JS3MnDTs89QZ137kb0Anm7jVX7J2e2slt3MpIac/wLWLotDhTvmumoa8mYyZQZ7TMsJ+PdPvDKU2a2sX6+maXja9391qkFN6UMxZzdZjpiOuRmlk7bz3P5ytX2ds7BMHjAQHfMvHgvO4w552i6Been93Y2ehs6r3w67F3x5uu92fZ75k41RLY7X2eusTx3vwU5+5nZOUeMDZnVzkX1WfzEg+1s3DH5ZrZ45SYzi6nJHfP7t9rZyBJ3U9Mzz/r5KSdntt/Opb+TfdVMNjTZ9ypJ6tNlvJk1yf6s+dyCF8wstc65Jko69ehvmVkP9TGzsvWPm9l7S/0F0kbnGv9fRz/nbss33QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEJJqfGv2/wn5U0/vlTC/z9hv2U7LVTlYZsm0m+5SkMidzftm8yfsF37DnmeEv8br4BfJOpcch9i+U99iN80Ab27sltQSdyN5tMGZx5r9Q7knWLzGz8lq7iqO+zt5OklJp+9eKXc6njZb8On6j87vV2TFvv/6v4las22hmf7vL+aBg/zC8Jt9Z6o7Zr699PfZ+Edz5oXXFQl7beIa/3B1z+hC8+Wx7gL3ftPNT9bFE5idoutZ5nvXOmNn2nTChYnfMxr6t3/oQlQ8+trPDj7fvD4X9nF8oD2mgGJHhL5R7SjP7Ufg9zEonu9lM3nrN32vBvi+Z2SpnPVKx0m7MGDN8kjtmD9m/qP6JkmZW51xnTj/sdHfMvTTazT180w0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQkegrw8JkWhnmzdxuENkm01qwlrB/1V5618kOdLKwyrBlITkAoHNy7p+NTvdjLBHSC5l2ali8m2/arh1KhHwSyc7wk0os2x4zXb/B3bYov4+ZnfhN+3n+7cYqe6f1/mubrrXfNK+Gy21mCqnvSjvvWUxOlWDa+QDS6L9hsWyvSsupE3Nqv8I+9uW5H8Kc+Tbar0E8pO0v26klytTyv880swXldjftyedf4e435hxE3tMYMdLOhh3jDhnJt37DndY9tEy8t58/9/ICM3vPqVVePM/Opn53tjvm2sEDzOytxSvM7Ll/vmJmpUMHuWOef9hgM9vH3ZJvugEAAAAAiAyLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIRN/THVaemGlPt9dR7fTBdTgftfUEAAAdj10inK39zKze6fAO22+izt427vQvx9IhhccZijs90/V1YR9/7DnlxJ1S42q7pzu1zum9lpSXZ78vKeczTzzm79cTcz4Geu9L2jlM0jUhgyac97vAzrze8LyQt3PqRdeGTKr13Tnj762+z2RqlpmlZXfPt+gMc/rRS3vb390dUtLUklHRznxphJ8vsC99KlhpZ8vm29n61CJ3zOWb7L7tZNq5tjnXzFVLN7pjPpp8xsz+64Qj3G35phsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgItFXhrWEVxmW3G2zAIBOrcxvyNDIXrtnHmhFcfv2nltnv6GNNfu4u82u2dvMYmmnSssRi9l1YmG8jwmNMecjTl1IfVeuXbK0aOULIbPauXQy5CNX9UAzynYqwxJFTiFUfeZ1YvEce77Zziufk/DeFSmdyDOzxnqv6s6WqPfHbAsxp7IoU/912Q1mVuu8QJtD9lvgHF95+eVmVtrf3m6P+VavIST3DoMO9CKFTfXyo+3sMeczhn018K/vkvTRytfMLBYbZ2YTDz3OzEr7DfIHrQvJHR3o7QYAAAAAoGNh0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARNq+Mqz9tTwAQLvlVb+8vThlZgvmLzSzVStXu2MuOPiLZjZs+AAz86rGFm5xh1RuDzvb198UkiS7eilH9huTnW509xpP2zfteqcSKzvP6TMK+RwQc6q/cjKtG/PqxCQV5NpVWx/OW5nRkBur1rt54cF2Fc1Gb7otqMvKyXFq3tL2a5uTax8nyVSFO2Z9nX2dKixyKuvq7DFj/mHbJmKxzOvaLDPt9q7IvkUrGWln+46IaNA2sPYxO1swz87Wl/n77WUf7koU2dmwKXbW5yx/zPZmivNZIH2UnVWv8/ebcqruGmvtnrKC3vabMrLHEf6gPUb7uYNvugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAi0vaVYV7NQzusgACAtnTTr54ys8Vr7OqvH1zxXTPLy8tzx6xIVpvZGzOXm9nho+1qjVX/9HtWbvrxxWZ24y//PzO7/KyT3f3uKRqTdl3RgpVrzSydtjNJiufsbY+ZsD9S9IrZ2xX09o+/6iq7+iXt1FolEvZrEHMySUo7n47WfuxuavqgssbPV843s/XlXu+Q/frkxPq4Y6adirjDjx9vZovn2+fvk39/wh1zytfPNbO8/e1uoVR1vb2dO6L0+4fsOW2sTZpZPN+eTyxm1/JJUtw5H6JQ4GQbQraNOa1zxftnMpv2ad4Vdvbe83a2yT5EQldSdU6tVf1sO1s0087ybvTHPONeJzzM3zYK3je8wwba2bKQ1r2S4nFm9sHb9nYvOteoMSP2c8fcR1SGAQAAAADQ7rDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIsOgGAAAAACAiLLoBAAAAAIhIVhAEwS49MCsr6rkAyNAunsbYjWZvtAs4S3qNNLO/POsUd0r6/jmXmdk1v7jNzEYMPcDM3po91x3z2OOPNrNYP7t/+aZrf2lmhw0d7o75wiN3mtnH779lZsd8/VIze/nBae6Yncl/fOsSM5v9+9/uxpkAn4NXNl2d4XaSlHC+YypvsrN+uWY0aP+D3CGHOde4p3/nFSnbovosftzBdvb463bWs0frz6UlFl7t56/dYWdOZb0KnNcn27+VKVljZ/XOMZ1e6eyz3B+zqNTOLn7K2XCEv98o/K3Bzt52usolacoJ9hueJ7tv+/5nf2ZmybpKd8xfTv67kx7nbss33QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARibX1BACgo1riZDdd+wcz+8/TTjazxrg/Zp/hA8xsVflyM/v/vn25s9dl7phvzLFruFKpOjNbMvM+M5sru4pnm9qQfOdmPvQbMzto3Xp327l//5OZtbNmnFDry/znCrRLXi1Yi7ZzasE85fZ1aEm5XV0oSUtmldlhhpVhUZlykZ21t1qwzY/Z2Wt3+dumnFowOTVbbzubVXzsj1ldY2f1XjtV2o6K8/wxk84tfYbdOqrTXvP3G4Xsbnb2zjx/28UrbjGzEaPt/rOcgpSZFcaK3DE/0Voz6+luyTfdAAAAAABEhkU3AAAAAAARYdENAAAAAEBEWHQDAAAAABARFt0AAAAAAESERTcAAAAAABGhMgwADGElMz/50c/M7G+PvG5mFVUbzew/v3qiO+aIkQeY2aqVXnfJCifr7o5Z9szfnNSvG7NlVgnWEvNf+rObD/+Ps81s+T8fbe3pRCq5clNbTwHYw4X0P7YjEya29Qx23V9+YGcV9SEbZ9tRubMiesep9lq1zh/SW2h5FWZbnO3CqqkmOFneLDsr/aWdjXRe95YY4mQjhvrbDnTyw0dMMrNkhV3n99rMp90x71pq18Fef9a57rZ80w0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQESrDAHxuWyoWuvnT7843s8HHnGBmo3vkZDynTK3RWjO7+57H3W0f/sX1GY1ZNtvuLSmb966/cdquoRk0aFBG85G2huSZ1oJ1LCtmP2Zmn8iuDAurb2kL6Xi6rafQDuQ72ZEh21Y4WcrJCjPcTpKc/iDtk+F+XwsZE3uCS0/x80EDd888dtWGZ+0sucrOwj5BVDqrng+cWrCyZMiOd7NPQvJFTpbnZB84l4uoKsMGONn3J/vbem/LIOe6uKXvSDN7ucavDPvT3+0X6fqz3E35phsAAAAAgKiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICD3dAHZq1mO/NLOjzv5hJGM+PesZMzvliJMjGbN2k905Xl25xN32xPMvM7M3Xiszs83LounOXbLKa+dES/z+4dfN7JpzjtiNM9k1pSm7eHfeHtPdXONkc0K2dUp7XQsy3C7MRidbHdGYaIk+sruAd7djT2rrGXw+1UvtzGuzD7O+3s4WOFlH412Fip2s4n07a3rTH7PLYX6eiT4h+YIGO1v18SwzK8q1z83iAf730aNGNoXMysY33QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARoTIM2IP9/Ef/bWZP/uLm3TiTbU498hQzu//RGe623zhrkpltcbZLOJfBUUOHu2Oee649ZvlFaTM754zzzayh/GV3TLSNt+fNs8N2WBmmvLidVXd3Ntza6lNpnza09QQ+p5VtPQF8TsndPN4Fp9nZl8/dffNoDckVdmbfWaW6kP1WONnmkG07Eq/UynsN6srtbNGT/pgHRFAZ5tx1JUl/fdrOKpa+Y2alRXal6+D+zr1T0rhDs0NmZeObbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACICItuAAAAAAAiwqIbAAAAAICIUBn2OXh/Q+H9PD/QXqWdK0BOkbNhZatPJdT5Zzt9KJKWV/3RzP56/91mNmpkoZlddPX33TEf/N2t9n6Psjtavn7+V83svptmu2PuOZVO7cuQ/Qe19RQ+l3Qy5aQcQ9y1EbW06lt9n+dc1M/M/vOk9WbWrcenrT6XKFV8bGdeLVjYK16dyWQ6mY1OlnCyVXP9/R6QyWQkzXKy557wt33L6RSrcDr7FtfY50My7Z8rJbn+nDx80w0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQkT2yMmyoU4X0sVOFlOPsc3PGswHazvU3/tzMls+3q6tmzXgtium0yE+/e15G25W9a2cXXfFtd9ucRNrMfnL5ZWa2dlGFs9ewOifKC9vCi09PN7OfXXbybpzJLkq3fl0RgLY17gsnmFm6ZomZrV3u37P3GZjxlCKxfqmdOU1QoZVhXt3YnsL+1CLFs+2ssaa1Z7LNkU6W/oq/7foaJ3MqzlaV21ldyEFy+qFd/Qc4+KYbAAAAAICIsOgGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiMge2dNdX2tnw50O71VOhzfQ2dz/5KtmNua757vbXjbtgdadTBv5jy+c6OaPPnqDmd2+6I3MBs0b4ufJRZntFy0y76XHzOwTPWpmPaOYzC6IxXKclK53IHrx1t9jfZ6ZVaT2M7On/3S4u9/i/vb9avBoe7t+g+3M69OWpOR6O6uotjNv4RLW022/ensOr4Y6mWtncWd9FJVjQr4aPuRCOzvPWbPN+J2dpQ71x+y1f6H/AAffdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEWHRDQAAAABARFh0AwAAAAAQERbdAAAAAABEZI+sDFuRsrOxzk/ie1UDmzOeDdDxXHrX/W7+k/ufMrMNqZrWnUwbuu6SG8ysW4b7LCnyqp6kJWE9LIhElxy7S+WCb9m9Jd+/9jfufo8saf1aIUlK13oHCrVgQNSaQgusPr+NdXbPVlp2Fqt1PvhK8u46jfvb2V7Odl4mSfs6zUvZBXa2zKkTcxqvJPmf4717dkPIfjsSr/mryFkVDhna6lNpsb3U28wu+fp4M5vx9NNmtuXP/pg/611uZpcf4m/LN90AAAAAAESERTcAAAAAABFh0Q0AAAAAQERYdAMAAAAAEBEW3QAAAAAARIRFNwAAAAAAEdkjK8P2dRpaYs4r0vrlD+F6OlkiZNu1rTkR4HOYX/uJme2blWVmHa2WI9P6rrFD7NKOvL57+2MuymxM729YKZAKl+PcHGb8/j4zG/GF49z9HnnZmRnPyVNW/2wk+wWwq1a3+h6TNfZ1KB1bYmbDRi5z93vOWRlPKRLjTrWzilvtzGkakyRlO5//i50P1Ss6UVXnCCerq7Wz2sZWn0oryDaT40tuM7NHZ4w0s7PH3uiOuOFuJ/wfd1O+6QYAAAAAICosugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiLDoBgAAAAAgIiy6AQAAAACIyB5ZGZbT2wnTGUUt4v3NR7HTf5CsCdlxu/x5f+wJ+jhZfRCY2QMPPGhm559/Xgtm1L7MXVRpZsf03S+SMc893+6EeeD+xyIZ03NkSb6bz1pVs1vmsas2h15wd26/AXmtO5Fd9mkbjQtgG7/+MRPJ1EYzSztFsXlhHbPtzGFH2dmCh+wsZd9aJUkFTvfvMOdDvtcYZhektp2xTlbsZI3O69PP22lE5iz38+Q6+w0//jD7fBhXfLS90wP9yjCV+7GHb7oBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiAiLbgAAAAAAIsKiGwAAAACAiOyRPd2bauwskWtneXE7iznddpKU42R5Rc5+nXfI6w0EOqJvfONcM6tY6hc2/vimG1p5Nm1j5qx3ItlvW3Rxe9pbD3d06tp6AgDaRGOr7zFV59wH0/aH1FVlW939bj7YzvYqCZtV69vrZDsbNd7O3pjh79erKx/mvF3ediHV4N7bopSzdshz9uksGyRJ3lvmTEc5pXa211dCBo3AkIF+/vs5dvbPlSfZYcHozCbUQnzTDQAAAABARFh0AwAAAAAQERbdAAAAAABEhEU3AAAAAAARYdENAAAAAEBEWHQDAAAAABCRPbIyrDplZ161V0FvOysMGbMxHfIAQ9rZrrH12yiAduu/b5zq5lFUhvUJ+XvJDWpq9THRuRTEC9p6CgDaxKJW32N5zQdm1lhtb1e70t9vaX87O+Ub/ra724hD7ezVkMqwWifzrtROm7CGeR1cktLOxkmniyzhTDYe8vnfW9x5z3PcV51wb3/MKIRVI7/tHNdP/MF5AWOv2Vl+yKBOrVoYvukGAAAAACAiLLoBAAAAAIgIi24AAAAAACLCohsAAAAAgIiw6AYAAAAAICIsugEAAAAAiMgeWRnmtXd5WZ0Thr2Q6bqQB/z/7d1dbNV3GQfwf6GhWJh1LHRMgZkyNNogIQwXcb6AgQu3GImLBjTL8G1ZzNQlzkVxYU6N8Wo3i7o4o/EFLyS7INFlLlHnJrI5NWqQKTIjGxA2ihAoKxVaL0wWs+V5/vWc8zvntP18br/7veScduc8/JN+o3VJVven9GE2uWHrljDb/aO4R2TTQLzntvevT8/c/q3Hau/F7Pbk3qfSfMumt7XpJsB09/DvkvBIHC3NOq+qqroxqcTtNpdujrOBL+VrjyaVwZmsZmuwphJ4NGmuarQW+Hx+ZJU0kVUrN8bZ6ttrNi7g7u/G2QN78rV/3JuExxu5TZW/2VVVVQ3Oc1XlSTcAAAAUY+gGAACAQgzdAAAAUIihGwAAAAoxdAMAAEAhhm4AAAAoZFZWhmWyv/w/eirOegu9kqNJvcG5MkfCtPS+bR8Ms6wy7MO3DoXZvp+pBKM5o/Nf2ekrAB3xqtZvebixZUdrOman1TCwJo6uvTNf+txX4yx7jbKmsaGaF2806fc6mazrS7K6VquVb4yza29LFnbg4+ree+Ls+T+17x4vGim3tSfdAAAAUIihGwAAAAoxdAMAAEAhhm4AAAAoxNANAAAAhRi6AQAAoBBDNwAAABQyrar5WmUiyU4mZXwLknV9WcF3VVXVxThKKvxK1sXBjPKe668Ps098bFWYfW/Pn8PswU50RNKVPnvX18PsCztvCbNLSlwGmAau6PQFXrQmK32uqurIniSMP1q7zto78vzAiTgb+06cLTwVZ4tW5GcOJvNBXzJzDCZ3HRjOz7zui3HW32Xv51jd/DSDeNINAAAAhRi6AQAAoBBDNwAAABRi6AYAAIBCDN0AAABQiKEbAAAACpmVlWGZc0k2N8lGk0qwun2B5vVXcSfK6Km4l+PA/hK3oaSlSfZsE/s+eXwyzNYONrExMAu195vfpjfF2V3vzteuf0dr79KtVt0YZ73z42xB0t97uuZDJ23EWpjcJ5nQ1m3Oz+y2WrDMmb90+gbt40k3AAAAFGLoBgAAgEIM3QAAAFCIoRsAAAAKMXQDAABAIYZuAAAAKERl2P/hTKcvAIQe/M0vwuyhPU+H2bGaur/pJe4f2bjpA2E2d8mydNfT/3wmzJ741bfrr9Vi6y57RZg9O/JCmH3/5wfTfdWCAa3T3/IdV18eZ59JasHWbx7KNx6JPyP/dVu87GjcxlkNb82PrDbU5AWsXtVYlqprhkt+DPYfjrPh5Q3dpmN+/Ns4e+ZI++7RzTzpBgAAgEIM3QAAAFCIoRsAAAAKMXQDAABAIYZuAAAAKMTQDQAAAIWoDANmhLGzcXfJ2Pk2XqRJi5evSfP7dv8kzK5bd0WYzWv4Rrnx6v4w+8jHvxZmP9j1w3Tf/gsnw+yxkfi9/tTWO8LsQxuuSs8EaJ3Wf/Bsf3Ocbb4mrlKsltT0If4+rgz7w954WW9SYVZdkx85YzTRDLdymtWCZQaujLObP922a3Q1T7oBAACgEEM3AAAAFGLoBgAAgEIM3QAAAFCIoRsAAAAKMXQDAABAIYZuAAAAKKRncnJyckr/YU9P6bsADZrir/Gsdd+9nw+zkb37w2zfr59K9x0c3hBm2+7cEWYb37Is3Rea1dOzOElPtO0ewMs1+pk9/s15YTZv7UC88EJfuu/EA0fC7MCheN3wxjjruTU9siO67avSuSRrov67OpNklzSxbyb7tvSGtybh40l2scHLdEjd77Un3QAAAFCIoRsAAAAKMXQDAABAIYZuAAAAKMTQDQAAAIUYugEAAKAQlWEwA6gMA/5XX887w2y8eqRt9wBeruHP7EeXxlnv+Tg7lNcETiS1YHNekyx8Vxz1DKVHdkS3fVU6lmSXJVlcHPdfOz8XZ7/cFWePHGz80LsfTu7z5WRh8rNXncrPrEZr8jZTGQYAAAAdYugGAACAQgzdAAAAUIihGwAAAAoxdAMAAEAhhm4AAAAopLfTF6D7ZP8SM79m7YUGs4mafQGYuoFqUZg938Z7AC008lycjf07zo7n285ZkYRvXxNny/uShfvyQ6eR+x9NwpF87UffG2enx+PsoZ/G2U3JnlVVVbuSWrC/H46zBcnbueMb+Zk770nCI0m2MMmy3rS6taeSLGnXK8mTbgAAACjE0A0AAACFGLoBAACgEEM3AAAAFGLoBgAAgEIM3QAAAFDIlCvDFidZVhhQJ/ur7dlf4Vcx1Zz+JBtMsmY65rLKsNEkq2lj8LMA8BK9nepEAYoZfzyuBcu+Y/Uvqdl4xVCcLb8qWbggyaZXZdj+JLv5pji7+vX5vlll2D/+GmefvCHZdHd+ZlYLljmXZDtuaWzPWtkAUFcZdmWSDSdZ9styqObMrP6shifdAAAAUIihGwAAAAoxdAMAAEAhhm4AAAAoxNANAAAAhRi6AQAAoBBDNwAAABQy5drlrLs5a+kbqNl3/lQv8BJjSXa6Zm1WCXcyyc42uGdV5S90VkO3KMnqXrtX1+SRZrq4M9l71mhfe1Xlr32jPyd176ducKCbLXtd3K177G9JJ2/1dOsvA7PQpXO3tHzPg0mH8IKFcfbaFTXP2JYsS8K+JMu+ZU0vt38lziaS/y0+UfclNXE2GSzOXIyz7a3/0epOda9tNrBcHkdzkuF04kTNmdnQW8OTbgAAACjE0A0AAACFGLoBAACgEEM3AAAAFGLoBgAAgEIM3QAAAFBIz+Tk5GSnLwEAAAAzkSfdAAAAUIihGwAAAAoxdAMAAEAhhm4AAAAoxNANAAAAhRi6AQAAoBBDNwAAABRi6AYAAIBCDN0AAABQyH8AfDcYyy5OSFYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Load ResNet18 without pretrained weights\n",
        "model = resnet18(weights=None, num_classes=100).to(device)\n",
        "\n",
        "# Modify the final fully connected layer for CIFAR-100 (100 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the summary (for CIFAR-100 input size)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_JMSwm0vQiV",
        "outputId": "e1459331-a7b9-43f1-9034-bd0431d7ac1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
            "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
            "             ReLU-26            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
            "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
            "             ReLU-30            [-1, 128, 4, 4]               0\n",
            "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
            "             ReLU-37            [-1, 256, 2, 2]               0\n",
            "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
            "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
            "             ReLU-42            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
            "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
            "             ReLU-46            [-1, 256, 2, 2]               0\n",
            "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
            "             ReLU-49            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
            "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-53            [-1, 512, 1, 1]               0\n",
            "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
            "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-58            [-1, 512, 1, 1]               0\n",
            "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
            "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-62            [-1, 512, 1, 1]               0\n",
            "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-65            [-1, 512, 1, 1]               0\n",
            "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 11,227,812\n",
            "Trainable params: 11,227,812\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.29\n",
            "Params size (MB): 42.83\n",
            "Estimated Total Size (MB): 44.13\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha=1.0, device='cuda'):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.0\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n"
      ],
      "metadata": {
        "id": "W8kyBS2C8iKL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# Training & Testing\n",
        "# ------------------------------\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True, mixup_alpha=0.4):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_mixup:\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
        "            outputs = model(inputs)\n",
        "            loss = lam * F.cross_entropy(outputs, targets_a) + (1 - lam) * F.cross_entropy(outputs, targets_b)\n",
        "        else:\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Accuracy tracking (still approximate when using MixUp)\n",
        "        _, pred = outputs.max(1)\n",
        "        if use_mixup:\n",
        "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
        "        else:\n",
        "            correct += pred.eq(target).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100*correct/processed:.2f}\")\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({acc:.2f}%)\\n\")\n",
        "    return acc\n",
        "\n",
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS = 40\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xY1f5OOvvaAG",
        "outputId": "3ac4ea06-8d71-42bb-811f-ea833d9955e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=3.4221 Acc=14.92: 100%|██████████| 391/391 [00:30<00:00, 12.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2998, Accuracy: 2028/10000 (20.28%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=3.4910 Acc=18.87: 100%|██████████| 391/391 [00:30<00:00, 12.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.1236, Accuracy: 2349/10000 (23.49%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.3535 Acc=22.10: 100%|██████████| 391/391 [00:30<00:00, 12.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0261, Accuracy: 2502/10000 (25.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.2286 Acc=24.60: 100%|██████████| 391/391 [00:32<00:00, 11.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9580, Accuracy: 2744/10000 (27.44%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=2.6924 Acc=27.50: 100%|██████████| 391/391 [00:30<00:00, 12.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7870, Accuracy: 2991/10000 (29.91%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=2.7445 Acc=30.46: 100%|██████████| 391/391 [00:30<00:00, 12.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6035, Accuracy: 3402/10000 (34.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=2.4460 Acc=32.97: 100%|██████████| 391/391 [00:30<00:00, 12.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4771, Accuracy: 3616/10000 (36.16%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=2.3339 Acc=35.59: 100%|██████████| 391/391 [00:31<00:00, 12.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3704, Accuracy: 3859/10000 (38.59%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=2.5520 Acc=38.20: 100%|██████████| 391/391 [00:30<00:00, 12.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3657, Accuracy: 3872/10000 (38.72%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=2.2250 Acc=40.52: 100%|██████████| 391/391 [00:30<00:00, 13.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2985, Accuracy: 4087/10000 (40.87%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.3408 Acc=42.45: 100%|██████████| 391/391 [00:29<00:00, 13.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1994, Accuracy: 4266/10000 (42.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=2.0510 Acc=44.70: 100%|██████████| 391/391 [00:30<00:00, 13.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1202, Accuracy: 4453/10000 (44.53%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=2.1098 Acc=46.50: 100%|██████████| 391/391 [00:30<00:00, 12.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1382, Accuracy: 4423/10000 (44.23%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=2.0236 Acc=48.29: 100%|██████████| 391/391 [00:30<00:00, 12.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1053, Accuracy: 4550/10000 (45.50%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=1.8044 Acc=49.52:  64%|██████▍   | 251/391 [00:20<00:11, 12.30it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3846335632.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3846335632.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Training & Testing\n",
        "# ------------------------------\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True, mixup_alpha=0.4):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_mixup:\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
        "            outputs = model(inputs)\n",
        "            loss = lam * F.cross_entropy(outputs, targets_a) + (1 - lam) * F.cross_entropy(outputs, targets_b)\n",
        "        else:\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Accuracy tracking (still approximate when using MixUp)\n",
        "        _, pred = outputs.max(1)\n",
        "        if use_mixup:\n",
        "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
        "        else:\n",
        "            correct += pred.eq(target).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100*correct/processed:.2f}\")\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({acc:.2f}%)\\n\")\n",
        "    return acc"
      ],
      "metadata": {
        "id": "33CJfErigRHM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS = 40\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zXkvj1v88YJ",
        "outputId": "2e06218c-dcfb-4120-8522-56fc2ae08839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.1080 Acc=4.93: 100%|██████████| 391/391 [00:35<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.8700, Accuracy: 1146/10000 (11.46%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=3.7708 Acc=8.33: 100%|██████████| 391/391 [00:32<00:00, 11.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.6858, Accuracy: 1371/10000 (13.71%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.8650 Acc=10.80: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4331, Accuracy: 1931/10000 (19.31%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.4393 Acc=13.67: 100%|██████████| 391/391 [00:33<00:00, 11.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2391, Accuracy: 2208/10000 (22.08%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=3.2511 Acc=15.54: 100%|██████████| 391/391 [00:33<00:00, 11.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0029, Accuracy: 2696/10000 (26.96%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=4.1449 Acc=17.31: 100%|██████████| 391/391 [00:33<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.1769, Accuracy: 2445/10000 (24.45%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=3.7653 Acc=19.57: 100%|██████████| 391/391 [00:33<00:00, 11.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9067, Accuracy: 2835/10000 (28.35%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=2.9900 Acc=21.15: 100%|██████████| 391/391 [00:33<00:00, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7548, Accuracy: 3121/10000 (31.21%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.0817 Acc=23.03: 100%|██████████| 391/391 [00:33<00:00, 11.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6914, Accuracy: 3391/10000 (33.91%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=3.1194 Acc=24.09: 100%|██████████| 391/391 [00:33<00:00, 11.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6683, Accuracy: 3266/10000 (32.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.6397 Acc=25.52: 100%|██████████| 391/391 [00:33<00:00, 11.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5892, Accuracy: 3535/10000 (35.35%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=3.4141 Acc=27.16: 100%|██████████| 391/391 [00:34<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5892, Accuracy: 3435/10000 (34.35%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=3.0038 Acc=28.26: 100%|██████████| 391/391 [00:34<00:00, 11.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5131, Accuracy: 3670/10000 (36.70%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=2.5250 Acc=30.06: 100%|██████████| 391/391 [00:34<00:00, 11.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4048, Accuracy: 3911/10000 (39.11%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=3.3204 Acc=29.31: 100%|██████████| 391/391 [00:33<00:00, 11.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3810, Accuracy: 3953/10000 (39.53%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=2.6870 Acc=31.37: 100%|██████████| 391/391 [00:33<00:00, 11.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2785, Accuracy: 4209/10000 (42.09%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss=2.0350 Acc=32.29: 100%|██████████| 391/391 [00:34<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2638, Accuracy: 4195/10000 (41.95%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss=3.5353 Acc=31.85: 100%|██████████| 391/391 [00:37<00:00, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2711, Accuracy: 4211/10000 (42.11%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss=2.3673 Acc=34.05: 100%|██████████| 391/391 [00:34<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2630, Accuracy: 4185/10000 (41.85%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss=2.4790 Acc=35.21: 100%|██████████| 391/391 [00:33<00:00, 11.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2004, Accuracy: 4419/10000 (44.19%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss=2.1419 Acc=34.65: 100%|██████████| 391/391 [00:34<00:00, 11.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1757, Accuracy: 4424/10000 (44.24%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss=2.7917 Acc=36.62: 100%|██████████| 391/391 [00:33<00:00, 11.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1677, Accuracy: 4446/10000 (44.46%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss=2.2937 Acc=36.55: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1410, Accuracy: 4523/10000 (45.23%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss=3.8063 Acc=37.75: 100%|██████████| 391/391 [00:34<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1330, Accuracy: 4539/10000 (45.39%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss=2.1359 Acc=38.65: 100%|██████████| 391/391 [00:34<00:00, 11.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0854, Accuracy: 4642/10000 (46.42%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss=2.4483 Acc=39.53: 100%|██████████| 391/391 [00:34<00:00, 11.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0427, Accuracy: 4759/10000 (47.59%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss=3.5080 Acc=39.19: 100%|██████████| 391/391 [00:33<00:00, 11.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0551, Accuracy: 4742/10000 (47.42%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss=3.5178 Acc=41.78: 100%|██████████| 391/391 [00:33<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9689, Accuracy: 4921/10000 (49.21%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Loss=2.9985 Acc=42.47: 100%|██████████| 391/391 [00:34<00:00, 11.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0152, Accuracy: 4847/10000 (48.47%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Loss=2.8367 Acc=43.20: 100%|██████████| 391/391 [00:35<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9498, Accuracy: 4943/10000 (49.43%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Loss=2.3514 Acc=42.87: 100%|██████████| 391/391 [00:33<00:00, 11.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0660, Accuracy: 4857/10000 (48.57%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Loss=1.5283 Acc=45.01: 100%|██████████| 391/391 [00:33<00:00, 11.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9240, Accuracy: 5052/10000 (50.52%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Loss=1.6636 Acc=47.21: 100%|██████████| 391/391 [00:33<00:00, 11.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9262, Accuracy: 5089/10000 (50.89%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Loss=2.5289 Acc=45.95: 100%|██████████| 391/391 [00:35<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8795, Accuracy: 5163/10000 (51.63%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Loss=3.3824 Acc=48.72: 100%|██████████| 391/391 [00:35<00:00, 11.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8244, Accuracy: 5267/10000 (52.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Loss=3.0239 Acc=50.60: 100%|██████████| 391/391 [00:34<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8451, Accuracy: 5234/10000 (52.34%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Loss=2.1382 Acc=52.40: 100%|██████████| 391/391 [00:34<00:00, 11.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8164, Accuracy: 5307/10000 (53.07%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Loss=1.9740 Acc=53.36: 100%|██████████| 391/391 [00:33<00:00, 11.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7984, Accuracy: 5315/10000 (53.15%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Loss=3.7840 Acc=52.19: 100%|██████████| 391/391 [00:35<00:00, 10.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8411, Accuracy: 5287/10000 (52.87%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Loss=1.4055 Acc=53.01: 100%|██████████| 391/391 [00:35<00:00, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8010, Accuracy: 5344/10000 (53.44%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Load ResNet18 without pretrained weights\n",
        "model = resnet34(weights=None, num_classes=100).to(device)\n",
        "\n",
        "# Modify the final fully connected layer for CIFAR-100 (100 classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, 100)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the summary (for CIFAR-100 input size)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS = 40\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH4Wf9ikkJ_G",
        "outputId": "7b5f549e-67c0-4466-d7ba-4e31484627e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-25             [-1, 64, 8, 8]               0\n",
            "           Conv2d-26            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-27            [-1, 128, 4, 4]             256\n",
            "             ReLU-28            [-1, 128, 4, 4]               0\n",
            "           Conv2d-29            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-30            [-1, 128, 4, 4]             256\n",
            "           Conv2d-31            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-36            [-1, 128, 4, 4]             256\n",
            "             ReLU-37            [-1, 128, 4, 4]               0\n",
            "           Conv2d-38            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-39            [-1, 128, 4, 4]             256\n",
            "             ReLU-40            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-41            [-1, 128, 4, 4]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "             ReLU-44            [-1, 128, 4, 4]               0\n",
            "           Conv2d-45            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-46            [-1, 128, 4, 4]             256\n",
            "             ReLU-47            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-48            [-1, 128, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-55            [-1, 128, 4, 4]               0\n",
            "           Conv2d-56            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-57            [-1, 256, 2, 2]             512\n",
            "             ReLU-58            [-1, 256, 2, 2]               0\n",
            "           Conv2d-59            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 2, 2]             512\n",
            "           Conv2d-61            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-62            [-1, 256, 2, 2]             512\n",
            "             ReLU-63            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-64            [-1, 256, 2, 2]               0\n",
            "           Conv2d-65            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-66            [-1, 256, 2, 2]             512\n",
            "             ReLU-67            [-1, 256, 2, 2]               0\n",
            "           Conv2d-68            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-69            [-1, 256, 2, 2]             512\n",
            "             ReLU-70            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-71            [-1, 256, 2, 2]               0\n",
            "           Conv2d-72            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-73            [-1, 256, 2, 2]             512\n",
            "             ReLU-74            [-1, 256, 2, 2]               0\n",
            "           Conv2d-75            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-76            [-1, 256, 2, 2]             512\n",
            "             ReLU-77            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-78            [-1, 256, 2, 2]               0\n",
            "           Conv2d-79            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-80            [-1, 256, 2, 2]             512\n",
            "             ReLU-81            [-1, 256, 2, 2]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-85            [-1, 256, 2, 2]               0\n",
            "           Conv2d-86            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-87            [-1, 256, 2, 2]             512\n",
            "             ReLU-88            [-1, 256, 2, 2]               0\n",
            "           Conv2d-89            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-90            [-1, 256, 2, 2]             512\n",
            "             ReLU-91            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-92            [-1, 256, 2, 2]               0\n",
            "           Conv2d-93            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-94            [-1, 256, 2, 2]             512\n",
            "             ReLU-95            [-1, 256, 2, 2]               0\n",
            "           Conv2d-96            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-97            [-1, 256, 2, 2]             512\n",
            "             ReLU-98            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-99            [-1, 256, 2, 2]               0\n",
            "          Conv2d-100            [-1, 512, 1, 1]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-102            [-1, 512, 1, 1]               0\n",
            "          Conv2d-103            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 1, 1]           1,024\n",
            "          Conv2d-105            [-1, 512, 1, 1]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-107            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-108            [-1, 512, 1, 1]               0\n",
            "          Conv2d-109            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-111            [-1, 512, 1, 1]               0\n",
            "          Conv2d-112            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-114            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-115            [-1, 512, 1, 1]               0\n",
            "          Conv2d-116            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-118            [-1, 512, 1, 1]               0\n",
            "          Conv2d-119            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-121            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-122            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "          Linear-124                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 21,335,972\n",
            "Trainable params: 21,335,972\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.97\n",
            "Params size (MB): 81.39\n",
            "Estimated Total Size (MB): 83.37\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.4827 Acc=3.83: 100%|██████████| 391/391 [00:39<00:00,  9.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 4.1112, Accuracy: 856/10000 (8.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=4.6035 Acc=6.69: 100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.9845, Accuracy: 1241/10000 (12.41%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.6625 Acc=8.88: 100%|██████████| 391/391 [00:39<00:00,  9.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.7173, Accuracy: 1471/10000 (14.71%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=4.0104 Acc=11.02: 100%|██████████| 391/391 [00:39<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.5019, Accuracy: 1825/10000 (18.25%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=3.3945 Acc=13.36: 100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4572, Accuracy: 2072/10000 (20.72%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=3.4253 Acc=14.88: 100%|██████████| 391/391 [00:40<00:00,  9.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0861, Accuracy: 2471/10000 (24.71%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=3.4863 Acc=16.61: 100%|██████████| 391/391 [00:39<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2807, Accuracy: 2569/10000 (25.69%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=3.4991 Acc=18.12: 100%|██████████| 391/391 [00:40<00:00,  9.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9569, Accuracy: 2661/10000 (26.61%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.2083 Acc=19.63: 100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9831, Accuracy: 2732/10000 (27.32%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=3.6236 Acc=19.59: 100%|██████████| 391/391 [00:39<00:00,  9.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9725, Accuracy: 2794/10000 (27.94%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=3.0361 Acc=21.70: 100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7515, Accuracy: 3199/10000 (31.99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=2.8610 Acc=23.21: 100%|██████████| 391/391 [00:39<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8382, Accuracy: 3185/10000 (31.85%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=2.6759 Acc=24.88: 100%|██████████| 391/391 [00:39<00:00,  9.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7116, Accuracy: 3302/10000 (33.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=3.2159 Acc=24.52: 100%|██████████| 391/391 [00:39<00:00,  9.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6344, Accuracy: 3486/10000 (34.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=2.8031 Acc=26.30: 100%|██████████| 391/391 [00:39<00:00,  9.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5478, Accuracy: 3614/10000 (36.14%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=2.4857 Acc=26.85: 100%|██████████| 391/391 [00:40<00:00,  9.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4559, Accuracy: 3714/10000 (37.14%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss=2.7070 Acc=28.73: 100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3927, Accuracy: 3927/10000 (39.27%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss=3.4180 Acc=29.12: 100%|██████████| 391/391 [00:39<00:00,  9.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4061, Accuracy: 4018/10000 (40.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss=3.7059 Acc=30.07: 100%|██████████| 391/391 [00:39<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3484, Accuracy: 4032/10000 (40.32%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss=3.2391 Acc=30.30: 100%|██████████| 391/391 [00:39<00:00,  9.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4004, Accuracy: 3938/10000 (39.38%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss=2.7848 Acc=31.44: 100%|██████████| 391/391 [00:39<00:00,  9.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2915, Accuracy: 4156/10000 (41.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss=3.8998 Acc=28.61: 100%|██████████| 391/391 [00:39<00:00,  9.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4277, Accuracy: 3855/10000 (38.55%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss=2.7942 Acc=30.66: 100%|██████████| 391/391 [00:40<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5103, Accuracy: 3768/10000 (37.68%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss=2.6582 Acc=31.77: 100%|██████████| 391/391 [00:39<00:00,  9.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3494, Accuracy: 4086/10000 (40.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss=3.1091 Acc=33.37: 100%|██████████| 391/391 [00:40<00:00,  9.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2785, Accuracy: 4204/10000 (42.04%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss=3.1837 Acc=35.63: 100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1604, Accuracy: 4483/10000 (44.83%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss=2.1554 Acc=35.23: 100%|██████████| 391/391 [00:40<00:00,  9.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1305, Accuracy: 4489/10000 (44.89%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss=3.1482 Acc=36.86: 100%|██████████| 391/391 [00:40<00:00,  9.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1319, Accuracy: 4561/10000 (45.61%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Loss=2.4366 Acc=36.86: 100%|██████████| 391/391 [00:40<00:00,  9.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1457, Accuracy: 4558/10000 (45.58%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Loss=1.7643 Acc=39.06: 100%|██████████| 391/391 [00:41<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0669, Accuracy: 4653/10000 (46.53%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Loss=3.3755 Acc=39.15: 100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0430, Accuracy: 4758/10000 (47.58%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Loss=2.5172 Acc=40.66: 100%|██████████| 391/391 [00:40<00:00,  9.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0140, Accuracy: 4856/10000 (48.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Loss=1.9585 Acc=41.54: 100%|██████████| 391/391 [00:39<00:00,  9.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0351, Accuracy: 4855/10000 (48.55%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Loss=3.9218 Acc=42.01: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0098, Accuracy: 4784/10000 (47.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Loss=3.9070 Acc=44.67: 100%|██████████| 391/391 [00:39<00:00,  9.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9673, Accuracy: 4930/10000 (49.30%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Loss=1.7384 Acc=44.02: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9282, Accuracy: 5067/10000 (50.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Loss=2.1711 Acc=45.93: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8829, Accuracy: 5118/10000 (51.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Loss=1.4311 Acc=46.15: 100%|██████████| 391/391 [00:39<00:00,  9.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9013, Accuracy: 5101/10000 (51.01%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Loss=2.9695 Acc=46.72: 100%|██████████| 391/391 [00:40<00:00,  9.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9120, Accuracy: 5087/10000 (50.87%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Loss=1.8613 Acc=48.24: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8892, Accuracy: 5117/10000 (51.17%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Load ResNet34 without pretrained weights\n",
        "model = resnet34(weights=None, num_classes=100).to(device)\n",
        "\n",
        "# Modify the final fully connected layer for CIFAR-100 (100 classes)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.4),  # Reduced from 0.4\n",
        "    nn.Linear(model.fc.in_features, 100),\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the summary (for CIFAR-100 input size)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "EPOCHS = 100\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxEJColutNuL",
        "outputId": "4a43a38c-62aa-408a-9e3c-25c2ff5986b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-25             [-1, 64, 8, 8]               0\n",
            "           Conv2d-26            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-27            [-1, 128, 4, 4]             256\n",
            "             ReLU-28            [-1, 128, 4, 4]               0\n",
            "           Conv2d-29            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-30            [-1, 128, 4, 4]             256\n",
            "           Conv2d-31            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-36            [-1, 128, 4, 4]             256\n",
            "             ReLU-37            [-1, 128, 4, 4]               0\n",
            "           Conv2d-38            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-39            [-1, 128, 4, 4]             256\n",
            "             ReLU-40            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-41            [-1, 128, 4, 4]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "             ReLU-44            [-1, 128, 4, 4]               0\n",
            "           Conv2d-45            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-46            [-1, 128, 4, 4]             256\n",
            "             ReLU-47            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-48            [-1, 128, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-55            [-1, 128, 4, 4]               0\n",
            "           Conv2d-56            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-57            [-1, 256, 2, 2]             512\n",
            "             ReLU-58            [-1, 256, 2, 2]               0\n",
            "           Conv2d-59            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 2, 2]             512\n",
            "           Conv2d-61            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-62            [-1, 256, 2, 2]             512\n",
            "             ReLU-63            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-64            [-1, 256, 2, 2]               0\n",
            "           Conv2d-65            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-66            [-1, 256, 2, 2]             512\n",
            "             ReLU-67            [-1, 256, 2, 2]               0\n",
            "           Conv2d-68            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-69            [-1, 256, 2, 2]             512\n",
            "             ReLU-70            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-71            [-1, 256, 2, 2]               0\n",
            "           Conv2d-72            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-73            [-1, 256, 2, 2]             512\n",
            "             ReLU-74            [-1, 256, 2, 2]               0\n",
            "           Conv2d-75            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-76            [-1, 256, 2, 2]             512\n",
            "             ReLU-77            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-78            [-1, 256, 2, 2]               0\n",
            "           Conv2d-79            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-80            [-1, 256, 2, 2]             512\n",
            "             ReLU-81            [-1, 256, 2, 2]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-85            [-1, 256, 2, 2]               0\n",
            "           Conv2d-86            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-87            [-1, 256, 2, 2]             512\n",
            "             ReLU-88            [-1, 256, 2, 2]               0\n",
            "           Conv2d-89            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-90            [-1, 256, 2, 2]             512\n",
            "             ReLU-91            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-92            [-1, 256, 2, 2]               0\n",
            "           Conv2d-93            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-94            [-1, 256, 2, 2]             512\n",
            "             ReLU-95            [-1, 256, 2, 2]               0\n",
            "           Conv2d-96            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-97            [-1, 256, 2, 2]             512\n",
            "             ReLU-98            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-99            [-1, 256, 2, 2]               0\n",
            "          Conv2d-100            [-1, 512, 1, 1]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-102            [-1, 512, 1, 1]               0\n",
            "          Conv2d-103            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 1, 1]           1,024\n",
            "          Conv2d-105            [-1, 512, 1, 1]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-107            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-108            [-1, 512, 1, 1]               0\n",
            "          Conv2d-109            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-111            [-1, 512, 1, 1]               0\n",
            "          Conv2d-112            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-114            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-115            [-1, 512, 1, 1]               0\n",
            "          Conv2d-116            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-118            [-1, 512, 1, 1]               0\n",
            "          Conv2d-119            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-121            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-122            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "         Dropout-124                  [-1, 512]               0\n",
            "          Linear-125                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 21,335,972\n",
            "Trainable params: 21,335,972\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.97\n",
            "Params size (MB): 81.39\n",
            "Estimated Total Size (MB): 83.38\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.3799 Acc=2.51: 100%|██████████| 391/391 [00:44<00:00,  8.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 4.1688, Accuracy: 681/10000 (6.81%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=4.5202 Acc=5.36: 100%|██████████| 391/391 [00:41<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.9373, Accuracy: 973/10000 (9.73%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=4.3829 Acc=7.54: 100%|██████████| 391/391 [00:41<00:00,  9.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.7438, Accuracy: 1277/10000 (12.77%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.8694 Acc=9.10: 100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.6791, Accuracy: 1460/10000 (14.60%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=4.0065 Acc=10.60: 100%|██████████| 391/391 [00:41<00:00,  9.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.5067, Accuracy: 1686/10000 (16.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=4.0046 Acc=11.90: 100%|██████████| 391/391 [00:40<00:00,  9.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.3699, Accuracy: 1933/10000 (19.33%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=3.9693 Acc=12.62: 100%|██████████| 391/391 [00:40<00:00,  9.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.3905, Accuracy: 1945/10000 (19.45%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=4.2293 Acc=14.76: 100%|██████████| 391/391 [00:40<00:00,  9.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2141, Accuracy: 2239/10000 (22.39%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.3406 Acc=15.65: 100%|██████████| 391/391 [00:40<00:00,  9.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2587, Accuracy: 2356/10000 (23.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=3.8494 Acc=16.28: 100%|██████████| 391/391 [00:41<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0697, Accuracy: 2647/10000 (26.47%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.9224 Acc=18.33: 100%|██████████| 391/391 [00:40<00:00,  9.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9142, Accuracy: 2813/10000 (28.13%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=2.8306 Acc=19.20: 100%|██████████| 391/391 [00:41<00:00,  9.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9173, Accuracy: 2884/10000 (28.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=3.2951 Acc=18.31: 100%|██████████| 391/391 [00:40<00:00,  9.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8328, Accuracy: 3086/10000 (30.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=3.3308 Acc=20.09: 100%|██████████| 391/391 [00:40<00:00,  9.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8272, Accuracy: 3025/10000 (30.25%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=3.5174 Acc=20.95: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8881, Accuracy: 3004/10000 (30.04%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=2.9290 Acc=21.54: 100%|██████████| 391/391 [00:40<00:00,  9.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7135, Accuracy: 3266/10000 (32.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss=4.5659 Acc=22.74: 100%|██████████| 391/391 [00:40<00:00,  9.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.5451, Accuracy: 2403/10000 (24.03%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss=2.7465 Acc=22.99: 100%|██████████| 391/391 [00:41<00:00,  9.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6341, Accuracy: 3460/10000 (34.60%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss=4.0916 Acc=24.29: 100%|██████████| 391/391 [00:40<00:00,  9.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6477, Accuracy: 3386/10000 (33.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss=3.0524 Acc=23.54: 100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7336, Accuracy: 3299/10000 (32.99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss=3.4789 Acc=24.79: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7697, Accuracy: 3211/10000 (32.11%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss=3.4858 Acc=24.69: 100%|██████████| 391/391 [00:41<00:00,  9.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4957, Accuracy: 3698/10000 (36.98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss=2.8586 Acc=25.83: 100%|██████████| 391/391 [00:41<00:00,  9.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4876, Accuracy: 3684/10000 (36.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss=3.0457 Acc=25.51: 100%|██████████| 391/391 [00:41<00:00,  9.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5677, Accuracy: 3533/10000 (35.33%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss=3.1707 Acc=25.93: 100%|██████████| 391/391 [00:41<00:00,  9.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5394, Accuracy: 3639/10000 (36.39%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss=3.6851 Acc=27.21: 100%|██████████| 391/391 [00:41<00:00,  9.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3594, Accuracy: 4024/10000 (40.24%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss=2.6827 Acc=28.31: 100%|██████████| 391/391 [00:41<00:00,  9.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3742, Accuracy: 3998/10000 (39.98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss=3.8015 Acc=28.51: 100%|██████████| 391/391 [00:41<00:00,  9.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4288, Accuracy: 3974/10000 (39.74%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29 Loss=3.2550 Acc=28.76: 100%|██████████| 391/391 [00:41<00:00,  9.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3688, Accuracy: 3981/10000 (39.81%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30 Loss=2.9683 Acc=29.64: 100%|██████████| 391/391 [00:41<00:00,  9.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5318, Accuracy: 3643/10000 (36.43%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31 Loss=2.4178 Acc=29.93: 100%|██████████| 391/391 [00:42<00:00,  9.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2979, Accuracy: 4180/10000 (41.80%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32 Loss=3.1465 Acc=30.26: 100%|██████████| 391/391 [00:41<00:00,  9.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3080, Accuracy: 4197/10000 (41.97%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33 Loss=2.9337 Acc=30.40: 100%|██████████| 391/391 [00:41<00:00,  9.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2948, Accuracy: 4230/10000 (42.30%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34 Loss=3.9369 Acc=31.51: 100%|██████████| 391/391 [00:40<00:00,  9.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2935, Accuracy: 4318/10000 (43.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35 Loss=3.3666 Acc=31.93: 100%|██████████| 391/391 [00:40<00:00,  9.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2430, Accuracy: 4277/10000 (42.77%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36 Loss=2.4794 Acc=32.38: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2610, Accuracy: 4192/10000 (41.92%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37 Loss=2.3344 Acc=32.57: 100%|██████████| 391/391 [00:41<00:00,  9.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1729, Accuracy: 4538/10000 (45.38%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38 Loss=2.1040 Acc=32.38: 100%|██████████| 391/391 [00:41<00:00,  9.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2218, Accuracy: 4446/10000 (44.46%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39 Loss=3.5264 Acc=32.99: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2514, Accuracy: 4344/10000 (43.44%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40 Loss=2.2446 Acc=35.09: 100%|██████████| 391/391 [00:41<00:00,  9.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2422, Accuracy: 4320/10000 (43.20%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41 Loss=3.8598 Acc=33.25: 100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2388, Accuracy: 4396/10000 (43.96%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42 Loss=2.5827 Acc=34.13: 100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2483, Accuracy: 4434/10000 (44.34%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43 Loss=2.0486 Acc=34.33: 100%|██████████| 391/391 [00:42<00:00,  9.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1333, Accuracy: 4567/10000 (45.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44 Loss=2.2229 Acc=34.55: 100%|██████████| 391/391 [00:41<00:00,  9.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2052, Accuracy: 4471/10000 (44.71%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45 Loss=2.1908 Acc=33.92: 100%|██████████| 391/391 [00:43<00:00,  8.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2709, Accuracy: 4312/10000 (43.12%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46 Loss=3.2081 Acc=34.45: 100%|██████████| 391/391 [00:41<00:00,  9.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2474, Accuracy: 4415/10000 (44.15%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47 Loss=2.7093 Acc=34.39: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2537, Accuracy: 4340/10000 (43.40%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48 Loss=3.1718 Acc=35.69: 100%|██████████| 391/391 [00:41<00:00,  9.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1510, Accuracy: 4611/10000 (46.11%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49 Loss=2.1665 Acc=35.91: 100%|██████████| 391/391 [00:42<00:00,  9.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1414, Accuracy: 4594/10000 (45.94%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50 Loss=2.1711 Acc=35.05: 100%|██████████| 391/391 [00:40<00:00,  9.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1616, Accuracy: 4518/10000 (45.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51 Loss=3.0735 Acc=36.35: 100%|██████████| 391/391 [00:40<00:00,  9.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0697, Accuracy: 4684/10000 (46.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 52 Loss=3.7749 Acc=37.25: 100%|██████████| 391/391 [00:40<00:00,  9.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0793, Accuracy: 4698/10000 (46.98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 53 Loss=2.4492 Acc=35.29: 100%|██████████| 391/391 [00:40<00:00,  9.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1058, Accuracy: 4733/10000 (47.33%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 54 Loss=2.3862 Acc=37.41: 100%|██████████| 391/391 [00:40<00:00,  9.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0244, Accuracy: 4832/10000 (48.32%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 55 Loss=2.2049 Acc=37.46: 100%|██████████| 391/391 [00:41<00:00,  9.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1307, Accuracy: 4588/10000 (45.88%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 56 Loss=3.1165 Acc=37.36: 100%|██████████| 391/391 [00:39<00:00,  9.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1215, Accuracy: 4599/10000 (45.99%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 57 Loss=3.6416 Acc=37.94: 100%|██████████| 391/391 [00:40<00:00,  9.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0591, Accuracy: 4808/10000 (48.08%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 58 Loss=1.9130 Acc=38.91: 100%|██████████| 391/391 [00:40<00:00,  9.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0804, Accuracy: 4743/10000 (47.43%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 59 Loss=2.1427 Acc=37.25: 100%|██████████| 391/391 [00:40<00:00,  9.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0130, Accuracy: 4931/10000 (49.31%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 60 Loss=2.0129 Acc=37.84: 100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1066, Accuracy: 4645/10000 (46.45%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 61 Loss=1.9860 Acc=38.76: 100%|██████████| 391/391 [00:40<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0426, Accuracy: 4795/10000 (47.95%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 62 Loss=1.7882 Acc=39.43: 100%|██████████| 391/391 [00:41<00:00,  9.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0772, Accuracy: 4733/10000 (47.33%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 63 Loss=3.5100 Acc=39.15: 100%|██████████| 391/391 [00:40<00:00,  9.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0859, Accuracy: 4833/10000 (48.33%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 64 Loss=3.6269 Acc=39.67: 100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.1337, Accuracy: 4684/10000 (46.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 65 Loss=3.6836 Acc=40.65: 100%|██████████| 391/391 [00:40<00:00,  9.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0173, Accuracy: 4872/10000 (48.72%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 66 Loss=2.0294 Acc=40.22: 100%|██████████| 391/391 [00:40<00:00,  9.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9657, Accuracy: 5055/10000 (50.55%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 67 Loss=1.8870 Acc=40.75: 100%|██████████| 391/391 [00:41<00:00,  9.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9962, Accuracy: 4923/10000 (49.23%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 68 Loss=1.8706 Acc=40.86: 100%|██████████| 391/391 [00:42<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9599, Accuracy: 4941/10000 (49.41%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 69 Loss=3.6456 Acc=41.51: 100%|██████████| 391/391 [00:41<00:00,  9.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9620, Accuracy: 5022/10000 (50.22%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 70 Loss=2.1718 Acc=41.91: 100%|██████████| 391/391 [00:44<00:00,  8.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9751, Accuracy: 5048/10000 (50.48%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 71 Loss=2.0582 Acc=42.55: 100%|██████████| 391/391 [00:44<00:00,  8.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9131, Accuracy: 5084/10000 (50.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 72 Loss=3.5524 Acc=42.70: 100%|██████████| 391/391 [00:42<00:00,  9.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9225, Accuracy: 5226/10000 (52.26%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 73 Loss=3.0034 Acc=43.34: 100%|██████████| 391/391 [00:42<00:00,  9.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9233, Accuracy: 5068/10000 (50.68%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 74 Loss=3.4818 Acc=42.95: 100%|██████████| 391/391 [00:42<00:00,  9.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.0065, Accuracy: 4995/10000 (49.95%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 75 Loss=1.3507 Acc=44.97: 100%|██████████| 391/391 [00:41<00:00,  9.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8126, Accuracy: 5321/10000 (53.21%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 76 Loss=2.3392 Acc=43.39: 100%|██████████| 391/391 [00:41<00:00,  9.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9133, Accuracy: 5184/10000 (51.84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 77 Loss=1.8452 Acc=45.37: 100%|██████████| 391/391 [00:41<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8016, Accuracy: 5338/10000 (53.38%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 78 Loss=2.3698 Acc=46.18: 100%|██████████| 391/391 [00:40<00:00,  9.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8664, Accuracy: 5291/10000 (52.91%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 79 Loss=2.0112 Acc=46.78: 100%|██████████| 391/391 [00:40<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7717, Accuracy: 5466/10000 (54.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 80 Loss=3.1143 Acc=47.15: 100%|██████████| 391/391 [00:41<00:00,  9.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7864, Accuracy: 5401/10000 (54.01%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 81 Loss=1.7036 Acc=47.97: 100%|██████████| 391/391 [00:40<00:00,  9.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7535, Accuracy: 5453/10000 (54.53%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 82 Loss=2.2882 Acc=48.71: 100%|██████████| 391/391 [00:41<00:00,  9.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.8205, Accuracy: 5354/10000 (53.54%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 83 Loss=2.7484 Acc=47.92: 100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7411, Accuracy: 5522/10000 (55.22%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 84 Loss=3.1927 Acc=51.38: 100%|██████████| 391/391 [00:41<00:00,  9.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7520, Accuracy: 5504/10000 (55.04%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 85 Loss=1.1969 Acc=50.61: 100%|██████████| 391/391 [00:41<00:00,  9.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7187, Accuracy: 5575/10000 (55.75%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 86 Loss=1.3540 Acc=52.66: 100%|██████████| 391/391 [00:42<00:00,  9.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7203, Accuracy: 5598/10000 (55.98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 87 Loss=1.0356 Acc=51.35: 100%|██████████| 391/391 [00:41<00:00,  9.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6858, Accuracy: 5702/10000 (57.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 88 Loss=2.2885 Acc=55.39: 100%|██████████| 391/391 [00:41<00:00,  9.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7118, Accuracy: 5647/10000 (56.47%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 89 Loss=0.9146 Acc=54.32: 100%|██████████| 391/391 [00:41<00:00,  9.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.7232, Accuracy: 5686/10000 (56.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 90 Loss=0.9410 Acc=58.00: 100%|██████████| 391/391 [00:41<00:00,  9.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6589, Accuracy: 5787/10000 (57.87%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 91 Loss=1.7641 Acc=58.11: 100%|██████████| 391/391 [00:41<00:00,  9.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6800, Accuracy: 5720/10000 (57.20%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 92 Loss=1.1965 Acc=56.87: 100%|██████████| 391/391 [00:42<00:00,  9.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6840, Accuracy: 5755/10000 (57.55%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 93 Loss=2.5605 Acc=59.29: 100%|██████████| 391/391 [00:42<00:00,  9.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6773, Accuracy: 5786/10000 (57.86%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 94 Loss=1.0247 Acc=60.72: 100%|██████████| 391/391 [00:41<00:00,  9.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6394, Accuracy: 5845/10000 (58.45%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 95 Loss=0.6151 Acc=60.79: 100%|██████████| 391/391 [00:41<00:00,  9.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6338, Accuracy: 5847/10000 (58.47%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 96 Loss=1.4532 Acc=59.25: 100%|██████████| 391/391 [00:45<00:00,  8.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6430, Accuracy: 5810/10000 (58.10%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 97 Loss=0.9922 Acc=63.59: 100%|██████████| 391/391 [00:43<00:00,  9.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6142, Accuracy: 5895/10000 (58.95%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 98 Loss=0.6722 Acc=63.77: 100%|██████████| 391/391 [00:43<00:00,  9.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6513, Accuracy: 5851/10000 (58.51%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 99 Loss=0.5311 Acc=63.55: 100%|██████████| 391/391 [00:41<00:00,  9.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6290, Accuracy: 5859/10000 (58.59%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 100 Loss=3.0378 Acc=64.39: 100%|██████████| 391/391 [00:42<00:00,  9.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.6370, Accuracy: 5872/10000 (58.72%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if available\n",
        "# Load ResNet18 without pretrained weights\n",
        "model = resnet18(weights=None, num_classes=100).to(device)\n",
        "\n",
        "# Modify the final fully connected layer for CIFAR-100 (100 classes)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.4),  # Reduced from 0.4\n",
        "    nn.Linear(model.fc.in_features, 100),\n",
        ")\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "# Print the summary (for CIFAR-100 input size)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# ------------------------------\n",
        "# Run Training\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "EPOCHS = 100\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1zWsxTHDApIM",
        "outputId": "49578797-e1a7-4cc4-c031-aa8bf872b23b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
            "             ReLU-21            [-1, 128, 4, 4]               0\n",
            "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
            "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
            "             ReLU-26            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
            "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
            "             ReLU-30            [-1, 128, 4, 4]               0\n",
            "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-36            [-1, 256, 2, 2]             512\n",
            "             ReLU-37            [-1, 256, 2, 2]               0\n",
            "           Conv2d-38            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-39            [-1, 256, 2, 2]             512\n",
            "           Conv2d-40            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-41            [-1, 256, 2, 2]             512\n",
            "             ReLU-42            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-43            [-1, 256, 2, 2]               0\n",
            "           Conv2d-44            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-45            [-1, 256, 2, 2]             512\n",
            "             ReLU-46            [-1, 256, 2, 2]               0\n",
            "           Conv2d-47            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-48            [-1, 256, 2, 2]             512\n",
            "             ReLU-49            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-50            [-1, 256, 2, 2]               0\n",
            "           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-53            [-1, 512, 1, 1]               0\n",
            "           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n",
            "           Conv2d-56            [-1, 512, 1, 1]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-58            [-1, 512, 1, 1]               0\n",
            "       BasicBlock-59            [-1, 512, 1, 1]               0\n",
            "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-62            [-1, 512, 1, 1]               0\n",
            "           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n",
            "             ReLU-65            [-1, 512, 1, 1]               0\n",
            "       BasicBlock-66            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "          Dropout-68                  [-1, 512]               0\n",
            "           Linear-69                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 11,227,812\n",
            "Trainable params: 11,227,812\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.29\n",
            "Params size (MB): 42.83\n",
            "Estimated Total Size (MB): 44.13\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.4528 Acc=3.34: 100%|██████████| 391/391 [00:36<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.9962, Accuracy: 902/10000 (9.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=3.6957 Acc=6.51: 100%|██████████| 391/391 [00:37<00:00, 10.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.7335, Accuracy: 1407/10000 (14.07%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.7826 Acc=8.89: 100%|██████████| 391/391 [00:36<00:00, 10.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.6236, Accuracy: 1549/10000 (15.49%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.4289 Acc=10.73: 100%|██████████| 391/391 [00:36<00:00, 10.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4260, Accuracy: 1836/10000 (18.36%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=3.6193 Acc=12.07: 100%|██████████| 391/391 [00:35<00:00, 10.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4248, Accuracy: 1885/10000 (18.85%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=4.2496 Acc=13.60: 100%|██████████| 391/391 [00:35<00:00, 10.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.1894, Accuracy: 2319/10000 (23.19%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=3.6180 Acc=14.93: 100%|██████████| 391/391 [00:35<00:00, 10.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0661, Accuracy: 2548/10000 (25.48%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=2.9956 Acc=16.51: 100%|██████████| 391/391 [00:35<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.0196, Accuracy: 2607/10000 (26.07%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.2833 Acc=18.03: 100%|██████████| 391/391 [00:36<00:00, 10.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9488, Accuracy: 2805/10000 (28.05%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=4.0936 Acc=18.46: 100%|██████████| 391/391 [00:35<00:00, 10.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.9062, Accuracy: 2854/10000 (28.54%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.9935 Acc=20.43: 100%|██████████| 391/391 [00:35<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8240, Accuracy: 3041/10000 (30.41%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=3.1482 Acc=20.59: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6945, Accuracy: 3303/10000 (33.03%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=3.0377 Acc=22.12: 100%|██████████| 391/391 [00:34<00:00, 11.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6829, Accuracy: 3395/10000 (33.95%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=2.7811 Acc=23.36: 100%|██████████| 391/391 [00:34<00:00, 11.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5967, Accuracy: 3405/10000 (34.05%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=3.4989 Acc=23.58: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5283, Accuracy: 3646/10000 (36.46%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=3.6500 Acc=24.19: 100%|██████████| 391/391 [00:36<00:00, 10.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6936, Accuracy: 3343/10000 (33.43%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Loss=2.7559 Acc=25.07: 100%|██████████| 391/391 [00:35<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5556, Accuracy: 3566/10000 (35.66%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Loss=3.8060 Acc=25.54: 100%|██████████| 391/391 [00:35<00:00, 10.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5640, Accuracy: 3502/10000 (35.02%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Loss=2.6664 Acc=26.05: 100%|██████████| 391/391 [00:36<00:00, 10.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5526, Accuracy: 3635/10000 (36.35%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Loss=2.9846 Acc=26.29: 100%|██████████| 391/391 [00:35<00:00, 10.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5528, Accuracy: 3647/10000 (36.47%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21 Loss=2.8206 Acc=28.21: 100%|██████████| 391/391 [00:35<00:00, 10.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4499, Accuracy: 3857/10000 (38.57%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22 Loss=3.6071 Acc=28.06: 100%|██████████| 391/391 [00:35<00:00, 11.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3872, Accuracy: 3918/10000 (39.18%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23 Loss=2.7964 Acc=28.65: 100%|██████████| 391/391 [00:36<00:00, 10.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3833, Accuracy: 3967/10000 (39.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24 Loss=3.7451 Acc=29.10: 100%|██████████| 391/391 [00:35<00:00, 10.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4319, Accuracy: 3838/10000 (38.38%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25 Loss=2.7115 Acc=29.91: 100%|██████████| 391/391 [00:35<00:00, 11.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4055, Accuracy: 3947/10000 (39.47%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26 Loss=2.7743 Acc=29.42: 100%|██████████| 391/391 [00:35<00:00, 11.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4464, Accuracy: 3842/10000 (38.42%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27 Loss=3.9586 Acc=29.72: 100%|██████████| 391/391 [00:35<00:00, 11.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3644, Accuracy: 4113/10000 (41.13%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28 Loss=2.1135 Acc=31.09:   9%|▉         | 37/391 [00:03<00:37,  9.42it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1257670714.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-174365245.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, scheduler, epoch, use_mixup, mixup_alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixup_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.equalInOut = in_planes == out_planes\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.dropRate = dropRate\n",
        "        self.shortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, 1, stride=stride, bias=False) or None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.conv1(out if self.equalInOut else x)\n",
        "        out = self.relu2(self.bn2(out))\n",
        "        if self.dropRate > 0:\n",
        "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return out + (x if self.equalInOut else self.shortcut(x))\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
        "                                i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth=28, num_classes=100, widen_factor=10, dropRate=0.3):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert ((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) // 6\n",
        "        block = BasicBlock\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.adaptive_avg_pool2d(out, 1)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)\n"
      ],
      "metadata": {
        "id": "OIGajc51Ljfl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIaz6eSCRbaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = WideResNet(depth=28, widen_factor=10, dropRate=0.3, num_classes=100).to(device)\n",
        "\n",
        "# Print the model summary\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# ------------------------------\n",
        "# Optimizer and Scheduler\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# Training Loop (assuming train/test functions defined)\n",
        "# ------------------------------\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True)\n",
        "    acc = test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "Ce71WahFH5Oo",
        "outputId": "f49d3975-f931-4450-ef80-cfe8e598f820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "              ReLU-3           [-1, 16, 32, 32]               0\n",
            "            Conv2d-4          [-1, 160, 32, 32]          23,040\n",
            "       BatchNorm2d-5          [-1, 160, 32, 32]             320\n",
            "              ReLU-6          [-1, 160, 32, 32]               0\n",
            "            Conv2d-7          [-1, 160, 32, 32]         230,400\n",
            "            Conv2d-8          [-1, 160, 32, 32]           2,560\n",
            "        BasicBlock-9          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-10          [-1, 160, 32, 32]             320\n",
            "             ReLU-11          [-1, 160, 32, 32]               0\n",
            "           Conv2d-12          [-1, 160, 32, 32]         230,400\n",
            "      BatchNorm2d-13          [-1, 160, 32, 32]             320\n",
            "             ReLU-14          [-1, 160, 32, 32]               0\n",
            "           Conv2d-15          [-1, 160, 32, 32]         230,400\n",
            "       BasicBlock-16          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-17          [-1, 160, 32, 32]             320\n",
            "             ReLU-18          [-1, 160, 32, 32]               0\n",
            "           Conv2d-19          [-1, 160, 32, 32]         230,400\n",
            "      BatchNorm2d-20          [-1, 160, 32, 32]             320\n",
            "             ReLU-21          [-1, 160, 32, 32]               0\n",
            "           Conv2d-22          [-1, 160, 32, 32]         230,400\n",
            "       BasicBlock-23          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-24          [-1, 160, 32, 32]             320\n",
            "             ReLU-25          [-1, 160, 32, 32]               0\n",
            "           Conv2d-26          [-1, 160, 32, 32]         230,400\n",
            "      BatchNorm2d-27          [-1, 160, 32, 32]             320\n",
            "             ReLU-28          [-1, 160, 32, 32]               0\n",
            "           Conv2d-29          [-1, 160, 32, 32]         230,400\n",
            "       BasicBlock-30          [-1, 160, 32, 32]               0\n",
            "     NetworkBlock-31          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-32          [-1, 160, 32, 32]             320\n",
            "             ReLU-33          [-1, 160, 32, 32]               0\n",
            "           Conv2d-34          [-1, 320, 16, 16]         460,800\n",
            "      BatchNorm2d-35          [-1, 320, 16, 16]             640\n",
            "             ReLU-36          [-1, 320, 16, 16]               0\n",
            "           Conv2d-37          [-1, 320, 16, 16]         921,600\n",
            "           Conv2d-38          [-1, 320, 16, 16]          51,200\n",
            "       BasicBlock-39          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-40          [-1, 320, 16, 16]             640\n",
            "             ReLU-41          [-1, 320, 16, 16]               0\n",
            "           Conv2d-42          [-1, 320, 16, 16]         921,600\n",
            "      BatchNorm2d-43          [-1, 320, 16, 16]             640\n",
            "             ReLU-44          [-1, 320, 16, 16]               0\n",
            "           Conv2d-45          [-1, 320, 16, 16]         921,600\n",
            "       BasicBlock-46          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-47          [-1, 320, 16, 16]             640\n",
            "             ReLU-48          [-1, 320, 16, 16]               0\n",
            "           Conv2d-49          [-1, 320, 16, 16]         921,600\n",
            "      BatchNorm2d-50          [-1, 320, 16, 16]             640\n",
            "             ReLU-51          [-1, 320, 16, 16]               0\n",
            "           Conv2d-52          [-1, 320, 16, 16]         921,600\n",
            "       BasicBlock-53          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-54          [-1, 320, 16, 16]             640\n",
            "             ReLU-55          [-1, 320, 16, 16]               0\n",
            "           Conv2d-56          [-1, 320, 16, 16]         921,600\n",
            "      BatchNorm2d-57          [-1, 320, 16, 16]             640\n",
            "             ReLU-58          [-1, 320, 16, 16]               0\n",
            "           Conv2d-59          [-1, 320, 16, 16]         921,600\n",
            "       BasicBlock-60          [-1, 320, 16, 16]               0\n",
            "     NetworkBlock-61          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-62          [-1, 320, 16, 16]             640\n",
            "             ReLU-63          [-1, 320, 16, 16]               0\n",
            "           Conv2d-64            [-1, 640, 8, 8]       1,843,200\n",
            "      BatchNorm2d-65            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-66            [-1, 640, 8, 8]               0\n",
            "           Conv2d-67            [-1, 640, 8, 8]       3,686,400\n",
            "           Conv2d-68            [-1, 640, 8, 8]         204,800\n",
            "       BasicBlock-69            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-70            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-71            [-1, 640, 8, 8]               0\n",
            "           Conv2d-72            [-1, 640, 8, 8]       3,686,400\n",
            "      BatchNorm2d-73            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-74            [-1, 640, 8, 8]               0\n",
            "           Conv2d-75            [-1, 640, 8, 8]       3,686,400\n",
            "       BasicBlock-76            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-77            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-78            [-1, 640, 8, 8]               0\n",
            "           Conv2d-79            [-1, 640, 8, 8]       3,686,400\n",
            "      BatchNorm2d-80            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-81            [-1, 640, 8, 8]               0\n",
            "           Conv2d-82            [-1, 640, 8, 8]       3,686,400\n",
            "       BasicBlock-83            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-84            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-85            [-1, 640, 8, 8]               0\n",
            "           Conv2d-86            [-1, 640, 8, 8]       3,686,400\n",
            "      BatchNorm2d-87            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-88            [-1, 640, 8, 8]               0\n",
            "           Conv2d-89            [-1, 640, 8, 8]       3,686,400\n",
            "       BasicBlock-90            [-1, 640, 8, 8]               0\n",
            "     NetworkBlock-91            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-92            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-93            [-1, 640, 8, 8]               0\n",
            "           Linear-94                  [-1, 100]          64,100\n",
            "================================================================\n",
            "Total params: 36,536,884\n",
            "Trainable params: 36,536,884\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 66.00\n",
            "Params size (MB): 139.38\n",
            "Estimated Total Size (MB): 205.39\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.5490 Acc=2.80: 100%|██████████| 391/391 [03:40<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 4.1978, Accuracy: 620/10000 (6.20%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=4.1924 Acc=5.00: 100%|██████████| 391/391 [03:40<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 4.0399, Accuracy: 924/10000 (9.24%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=4.2313 Acc=6.83: 100%|██████████| 391/391 [03:40<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 4.0032, Accuracy: 956/10000 (9.56%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.8915 Acc=8.77:  85%|████████▍ | 331/391 [03:07<00:34,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-211100226.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-174365245.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, scheduler, epoch, use_mixup, mixup_alpha)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True, mixup_alpha=0.4, label_smoothing=0.0):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_mixup:\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
        "            outputs = model(inputs)\n",
        "            loss = lam * F.cross_entropy(outputs, targets_a) + (1 - lam) * F.cross_entropy(outputs, targets_b)\n",
        "        else:\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, target, label_smoothing=label_smoothing)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Accuracy (approximate for mixup)\n",
        "        _, pred = outputs.max(1)\n",
        "        if use_mixup:\n",
        "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
        "        else:\n",
        "            correct += pred.eq(target).sum().item()\n",
        "        processed += len(data)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100 * correct / processed:.2f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss, 100 * correct / processed\n",
        "\n",
        "\n",
        "# Testing function\n",
        "def test(model, device, test_loader, class_names=None, plot_class_counts=True):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            all_preds.extend(pred.view(-1).cpu().numpy())\n",
        "            all_targets.extend(target.view(-1).cpu().numpy())\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({acc:.2f}%)\\n\")\n",
        "\n",
        "    # Plot predicted vs actual counts\n",
        "    if plot_class_counts:\n",
        "        pred_counts = Counter(all_preds)\n",
        "        target_counts = Counter(all_targets)\n",
        "        classes = list(range(100)) if class_names is None else class_names\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.bar(classes, [pred_counts.get(i, 0) for i in range(len(classes))], alpha=0.6, label='Predicted')\n",
        "        plt.bar(classes, [target_counts.get(i, 0) for i in range(len(classes))], alpha=0.6, label='Actual')\n",
        "        plt.title(\"Predicted vs Actual Class Counts\")\n",
        "        plt.xlabel(\"Class\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return acc, test_loss\n"
      ],
      "metadata": {
        "id": "ibQONpL-YeMq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = WideResNet(depth=28, widen_factor=10, dropRate=0.3, num_classes=100).to(device)\n",
        "\n",
        "# Print the model summary\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# ------------------------------\n",
        "# Optimizer and Scheduler\n",
        "# ------------------------------\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# Training Loop (assuming train/test functions defined)\n",
        "# ------------------------------\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = train(model, device, train_loader, optimizer, scheduler, epoch, use_mixup=True, label_smoothing=0.1)\n",
        "    test_acc, test_loss = test(model, device, test_loader, class_names=cifar100_classes)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n"
      ],
      "metadata": {
        "id": "m4jhcEi8QorZ",
        "outputId": "715b2c24-5c12-4af4-9615-7cc21ebde77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "              ReLU-3           [-1, 16, 32, 32]               0\n",
            "            Conv2d-4          [-1, 160, 32, 32]          23,040\n",
            "       BatchNorm2d-5          [-1, 160, 32, 32]             320\n",
            "              ReLU-6          [-1, 160, 32, 32]               0\n",
            "            Conv2d-7          [-1, 160, 32, 32]         230,400\n",
            "            Conv2d-8          [-1, 160, 32, 32]           2,560\n",
            "        BasicBlock-9          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-10          [-1, 160, 32, 32]             320\n",
            "             ReLU-11          [-1, 160, 32, 32]               0\n",
            "           Conv2d-12          [-1, 160, 32, 32]         230,400\n",
            "      BatchNorm2d-13          [-1, 160, 32, 32]             320\n",
            "             ReLU-14          [-1, 160, 32, 32]               0\n",
            "           Conv2d-15          [-1, 160, 32, 32]         230,400\n",
            "       BasicBlock-16          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-17          [-1, 160, 32, 32]             320\n",
            "             ReLU-18          [-1, 160, 32, 32]               0\n",
            "           Conv2d-19          [-1, 160, 32, 32]         230,400\n",
            "      BatchNorm2d-20          [-1, 160, 32, 32]             320\n",
            "             ReLU-21          [-1, 160, 32, 32]               0\n",
            "           Conv2d-22          [-1, 160, 32, 32]         230,400\n",
            "       BasicBlock-23          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-24          [-1, 160, 32, 32]             320\n",
            "             ReLU-25          [-1, 160, 32, 32]               0\n",
            "           Conv2d-26          [-1, 160, 32, 32]         230,400\n",
            "      BatchNorm2d-27          [-1, 160, 32, 32]             320\n",
            "             ReLU-28          [-1, 160, 32, 32]               0\n",
            "           Conv2d-29          [-1, 160, 32, 32]         230,400\n",
            "       BasicBlock-30          [-1, 160, 32, 32]               0\n",
            "     NetworkBlock-31          [-1, 160, 32, 32]               0\n",
            "      BatchNorm2d-32          [-1, 160, 32, 32]             320\n",
            "             ReLU-33          [-1, 160, 32, 32]               0\n",
            "           Conv2d-34          [-1, 320, 16, 16]         460,800\n",
            "      BatchNorm2d-35          [-1, 320, 16, 16]             640\n",
            "             ReLU-36          [-1, 320, 16, 16]               0\n",
            "           Conv2d-37          [-1, 320, 16, 16]         921,600\n",
            "           Conv2d-38          [-1, 320, 16, 16]          51,200\n",
            "       BasicBlock-39          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-40          [-1, 320, 16, 16]             640\n",
            "             ReLU-41          [-1, 320, 16, 16]               0\n",
            "           Conv2d-42          [-1, 320, 16, 16]         921,600\n",
            "      BatchNorm2d-43          [-1, 320, 16, 16]             640\n",
            "             ReLU-44          [-1, 320, 16, 16]               0\n",
            "           Conv2d-45          [-1, 320, 16, 16]         921,600\n",
            "       BasicBlock-46          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-47          [-1, 320, 16, 16]             640\n",
            "             ReLU-48          [-1, 320, 16, 16]               0\n",
            "           Conv2d-49          [-1, 320, 16, 16]         921,600\n",
            "      BatchNorm2d-50          [-1, 320, 16, 16]             640\n",
            "             ReLU-51          [-1, 320, 16, 16]               0\n",
            "           Conv2d-52          [-1, 320, 16, 16]         921,600\n",
            "       BasicBlock-53          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-54          [-1, 320, 16, 16]             640\n",
            "             ReLU-55          [-1, 320, 16, 16]               0\n",
            "           Conv2d-56          [-1, 320, 16, 16]         921,600\n",
            "      BatchNorm2d-57          [-1, 320, 16, 16]             640\n",
            "             ReLU-58          [-1, 320, 16, 16]               0\n",
            "           Conv2d-59          [-1, 320, 16, 16]         921,600\n",
            "       BasicBlock-60          [-1, 320, 16, 16]               0\n",
            "     NetworkBlock-61          [-1, 320, 16, 16]               0\n",
            "      BatchNorm2d-62          [-1, 320, 16, 16]             640\n",
            "             ReLU-63          [-1, 320, 16, 16]               0\n",
            "           Conv2d-64            [-1, 640, 8, 8]       1,843,200\n",
            "      BatchNorm2d-65            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-66            [-1, 640, 8, 8]               0\n",
            "           Conv2d-67            [-1, 640, 8, 8]       3,686,400\n",
            "           Conv2d-68            [-1, 640, 8, 8]         204,800\n",
            "       BasicBlock-69            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-70            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-71            [-1, 640, 8, 8]               0\n",
            "           Conv2d-72            [-1, 640, 8, 8]       3,686,400\n",
            "      BatchNorm2d-73            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-74            [-1, 640, 8, 8]               0\n",
            "           Conv2d-75            [-1, 640, 8, 8]       3,686,400\n",
            "       BasicBlock-76            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-77            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-78            [-1, 640, 8, 8]               0\n",
            "           Conv2d-79            [-1, 640, 8, 8]       3,686,400\n",
            "      BatchNorm2d-80            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-81            [-1, 640, 8, 8]               0\n",
            "           Conv2d-82            [-1, 640, 8, 8]       3,686,400\n",
            "       BasicBlock-83            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-84            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-85            [-1, 640, 8, 8]               0\n",
            "           Conv2d-86            [-1, 640, 8, 8]       3,686,400\n",
            "      BatchNorm2d-87            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-88            [-1, 640, 8, 8]               0\n",
            "           Conv2d-89            [-1, 640, 8, 8]       3,686,400\n",
            "       BasicBlock-90            [-1, 640, 8, 8]               0\n",
            "     NetworkBlock-91            [-1, 640, 8, 8]               0\n",
            "      BatchNorm2d-92            [-1, 640, 8, 8]           1,280\n",
            "             ReLU-93            [-1, 640, 8, 8]               0\n",
            "           Linear-94                  [-1, 100]          64,100\n",
            "================================================================\n",
            "Total params: 36,536,884\n",
            "Trainable params: 36,536,884\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 66.00\n",
            "Params size (MB): 139.38\n",
            "Estimated Total Size (MB): 205.39\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=4.5293 Acc=2.58: 100%|██████████| 391/391 [03:42<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 4.3558, Accuracy: 412/10000 (4.12%)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=4.2688 Acc=4.56: 100%|██████████| 391/391 [03:49<00:00,  1.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvements to Reach 74% Test Accuracy\n",
        "\n",
        "## Analysis of Previous Runs:\n",
        "- **40 epochs run (cell-5)**: Peak test accuracy 53.44% without proper MixUp training\n",
        "- **150 epochs run (new_trial)**: Peak test accuracy 55.78% but severe overfitting (train 73%+ vs test 55%)\n",
        "\n",
        "## Key Issues Identified:\n",
        "1. **Model capacity too small** - ResNet18 (11.2M params) insufficient for 100 classes\n",
        "2. **MixUp too aggressive** - Alpha=0.4 causing performance degradation\n",
        "3. **Data augmentation overlap** - ColorJitter + RandomBrightnessContrast + HueSaturationValue are redundant\n",
        "4. **Training plateaued** - Model maxed out at ~56% accuracy\n",
        "\n",
        "## Improvements Implemented Below:\n",
        "\n",
        "### 1. Model Architecture: ResNet18 → ResNet34\n",
        "- **Parameters**: 11.2M → 21.3M (nearly 2x capacity)\n",
        "- **Depth**: 18 layers → 34 layers\n",
        "- Better feature learning for 100-class classification\n",
        "\n",
        "### 2. MixUp Tuning: Alpha 0.4 → 0.2\n",
        "- Less aggressive blending\n",
        "- Previous runs showed MixUp hurt performance\n",
        "- Lower alpha = more focused training\n",
        "\n",
        "### 3. Simplified Data Augmentation\n",
        "**Removed:**\n",
        "- ColorJitter (redundant)\n",
        "- RandomBrightnessContrast (redundant)\n",
        "- HueSaturationValue (too aggressive)\n",
        "\n",
        "**Kept:**\n",
        "- HorizontalFlip (standard)\n",
        "- Affine (translation, scale, rotation)\n",
        "- RandomCrop + Resize\n",
        "- CoarseDropout (cutout)\n",
        "- Normalize\n",
        "\n",
        "### 4. Extended Training: 100 Epochs\n",
        "- More time for convergence\n",
        "- OneCycleLR for learning rate scheduling"
      ],
      "metadata": {
        "id": "iiCLV8_hbj-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified Data Augmentation (Removed redundant color transforms)\n",
        "class SimplifiedAlbumentationsTransforms:\n",
        "    def __init__(self, mean, std):\n",
        "        self.aug = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Affine(translate_percent=0.0625, scale=(0.9, 1.1), rotate=(-7, 7), p=0.5),\n",
        "            A.RandomCrop(height=28, width=28, p=0.5),\n",
        "            A.Resize(height=32, width=32),\n",
        "            A.CoarseDropout(\n",
        "                max_holes=1,\n",
        "                max_height=16, max_width=16,\n",
        "                fill_value=tuple(int(m * 255) for m in mean),\n",
        "                p=0.5\n",
        "            ),\n",
        "            A.Normalize(mean=mean, std=std),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        image = np.array(img)\n",
        "        return self.aug(image=image)[\"image\"]\n",
        "\n",
        "# Create new datasets with simplified augmentations\n",
        "train_transforms_v2 = SimplifiedAlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std)\n",
        "train_dataset_v2 = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms_v2)\n",
        "train_loader_v2 = DataLoader(train_dataset_v2, batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
        "])\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transforms)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=5, pin_memory=True)\n",
        "\n",
        "print(\"Simplified augmentation pipeline created!\")\n",
        "print(\"Removed: ColorJitter, RandomBrightnessContrast, HueSaturationValue\")\n",
        "print(\"Kept: HorizontalFlip, Affine, RandomCrop, CoarseDropout\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCJgarKrbj-i",
        "outputId": "cb264481-beee-4973-cf7c-8ca1456caf65"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-624767531.py:9: UserWarning: Argument(s) 'max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simplified augmentation pipeline created!\n",
            "Removed: ColorJitter, RandomBrightnessContrast, HueSaturationValue\n",
            "Kept: HorizontalFlip, Affine, RandomCrop, CoarseDropout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ResNet34 for CIFAR-100\n",
        "model_v2 = resnet34(weights=None, num_classes=100).to(device)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Model Upgrade: ResNet18 → ResNet34\")\n",
        "print(\"=\" * 70)\n",
        "summary(model_v2, input_size=(3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWET5uENbj-i",
        "outputId": "84a27aa9-7126-4a1e-aa4d-b823f7762fd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Model Upgrade: ResNet18 → ResNet34\n",
            "======================================================================\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
            "              ReLU-7             [-1, 64, 8, 8]               0\n",
            "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
            "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
            "             ReLU-10             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
            "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
            "             ReLU-14             [-1, 64, 8, 8]               0\n",
            "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
            "             ReLU-17             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
            "           Conv2d-19             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-20             [-1, 64, 8, 8]             128\n",
            "             ReLU-21             [-1, 64, 8, 8]               0\n",
            "           Conv2d-22             [-1, 64, 8, 8]          36,864\n",
            "      BatchNorm2d-23             [-1, 64, 8, 8]             128\n",
            "             ReLU-24             [-1, 64, 8, 8]               0\n",
            "       BasicBlock-25             [-1, 64, 8, 8]               0\n",
            "           Conv2d-26            [-1, 128, 4, 4]          73,728\n",
            "      BatchNorm2d-27            [-1, 128, 4, 4]             256\n",
            "             ReLU-28            [-1, 128, 4, 4]               0\n",
            "           Conv2d-29            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-30            [-1, 128, 4, 4]             256\n",
            "           Conv2d-31            [-1, 128, 4, 4]           8,192\n",
            "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
            "             ReLU-33            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
            "           Conv2d-35            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-36            [-1, 128, 4, 4]             256\n",
            "             ReLU-37            [-1, 128, 4, 4]               0\n",
            "           Conv2d-38            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-39            [-1, 128, 4, 4]             256\n",
            "             ReLU-40            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-41            [-1, 128, 4, 4]               0\n",
            "           Conv2d-42            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-43            [-1, 128, 4, 4]             256\n",
            "             ReLU-44            [-1, 128, 4, 4]               0\n",
            "           Conv2d-45            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-46            [-1, 128, 4, 4]             256\n",
            "             ReLU-47            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-48            [-1, 128, 4, 4]               0\n",
            "           Conv2d-49            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-50            [-1, 128, 4, 4]             256\n",
            "             ReLU-51            [-1, 128, 4, 4]               0\n",
            "           Conv2d-52            [-1, 128, 4, 4]         147,456\n",
            "      BatchNorm2d-53            [-1, 128, 4, 4]             256\n",
            "             ReLU-54            [-1, 128, 4, 4]               0\n",
            "       BasicBlock-55            [-1, 128, 4, 4]               0\n",
            "           Conv2d-56            [-1, 256, 2, 2]         294,912\n",
            "      BatchNorm2d-57            [-1, 256, 2, 2]             512\n",
            "             ReLU-58            [-1, 256, 2, 2]               0\n",
            "           Conv2d-59            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-60            [-1, 256, 2, 2]             512\n",
            "           Conv2d-61            [-1, 256, 2, 2]          32,768\n",
            "      BatchNorm2d-62            [-1, 256, 2, 2]             512\n",
            "             ReLU-63            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-64            [-1, 256, 2, 2]               0\n",
            "           Conv2d-65            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-66            [-1, 256, 2, 2]             512\n",
            "             ReLU-67            [-1, 256, 2, 2]               0\n",
            "           Conv2d-68            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-69            [-1, 256, 2, 2]             512\n",
            "             ReLU-70            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-71            [-1, 256, 2, 2]               0\n",
            "           Conv2d-72            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-73            [-1, 256, 2, 2]             512\n",
            "             ReLU-74            [-1, 256, 2, 2]               0\n",
            "           Conv2d-75            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-76            [-1, 256, 2, 2]             512\n",
            "             ReLU-77            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-78            [-1, 256, 2, 2]               0\n",
            "           Conv2d-79            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-80            [-1, 256, 2, 2]             512\n",
            "             ReLU-81            [-1, 256, 2, 2]               0\n",
            "           Conv2d-82            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-83            [-1, 256, 2, 2]             512\n",
            "             ReLU-84            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-85            [-1, 256, 2, 2]               0\n",
            "           Conv2d-86            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-87            [-1, 256, 2, 2]             512\n",
            "             ReLU-88            [-1, 256, 2, 2]               0\n",
            "           Conv2d-89            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-90            [-1, 256, 2, 2]             512\n",
            "             ReLU-91            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-92            [-1, 256, 2, 2]               0\n",
            "           Conv2d-93            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-94            [-1, 256, 2, 2]             512\n",
            "             ReLU-95            [-1, 256, 2, 2]               0\n",
            "           Conv2d-96            [-1, 256, 2, 2]         589,824\n",
            "      BatchNorm2d-97            [-1, 256, 2, 2]             512\n",
            "             ReLU-98            [-1, 256, 2, 2]               0\n",
            "       BasicBlock-99            [-1, 256, 2, 2]               0\n",
            "          Conv2d-100            [-1, 512, 1, 1]       1,179,648\n",
            "     BatchNorm2d-101            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-102            [-1, 512, 1, 1]               0\n",
            "          Conv2d-103            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-104            [-1, 512, 1, 1]           1,024\n",
            "          Conv2d-105            [-1, 512, 1, 1]         131,072\n",
            "     BatchNorm2d-106            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-107            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-108            [-1, 512, 1, 1]               0\n",
            "          Conv2d-109            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-110            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-111            [-1, 512, 1, 1]               0\n",
            "          Conv2d-112            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-113            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-114            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-115            [-1, 512, 1, 1]               0\n",
            "          Conv2d-116            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-117            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-118            [-1, 512, 1, 1]               0\n",
            "          Conv2d-119            [-1, 512, 1, 1]       2,359,296\n",
            "     BatchNorm2d-120            [-1, 512, 1, 1]           1,024\n",
            "            ReLU-121            [-1, 512, 1, 1]               0\n",
            "      BasicBlock-122            [-1, 512, 1, 1]               0\n",
            "AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n",
            "          Linear-124                  [-1, 100]          51,300\n",
            "================================================================\n",
            "Total params: 21,335,972\n",
            "Trainable params: 21,335,972\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.97\n",
            "Params size (MB): 81.39\n",
            "Estimated Total Size (MB): 83.37\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with Improved Configuration\n",
        "# Key changes: ResNet34, Reduced MixUp (alpha=0.2), Simplified augmentation, 100 epochs\n",
        "\n",
        "optimizer_v2 = optim.SGD(model_v2.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "EPOCHS_V2 = 40\n",
        "\n",
        "scheduler_v2 = OneCycleLR(\n",
        "    optimizer_v2,\n",
        "    max_lr=0.05,\n",
        "    steps_per_epoch=len(train_loader_v2),\n",
        "    epochs=EPOCHS_V2\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Training Configuration:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Model: ResNet34 (21.3M parameters)\")\n",
        "print(f\"MixUp Alpha: 0.2 (reduced from 0.4)\")\n",
        "print(f\"Epochs: {EPOCHS_V2}\")\n",
        "print(f\"Optimizer: SGD (lr=0.01, momentum=0.9, weight_decay=1e-4)\")\n",
        "print(f\"Scheduler: OneCycleLR (max_lr=0.05)\")\n",
        "print(f\"Data Augmentation: Simplified (removed redundant color transforms)\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "best_acc = 0.0\n",
        "for epoch in range(1, EPOCHS_V2 + 1):\n",
        "    train(model_v2, device, train_loader_v2, optimizer_v2, scheduler_v2, epoch, use_mixup=True, mixup_alpha=0.2)\n",
        "    acc = test(model_v2, device, test_loader)\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        print(f\"*** New best accuracy: {best_acc:.2f}% ***\")\n",
        "\n",
        "    # Early stopping if target reached\n",
        "    if acc >= 74.0:\n",
        "        print(f\"\\n{'=' * 70}\")\n",
        "        print(f\"Target accuracy of 74% reached at epoch {epoch}!\")\n",
        "        print(f\"Final test accuracy: {acc:.2f}%\")\n",
        "        print(f\"{'=' * 70}\")\n",
        "        break\n",
        "\n",
        "print(f\"\\nTraining completed. Best test accuracy: {best_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OzwQF7wYbj-i",
        "outputId": "99c61c6b-c28d-4f13-9702-ef20af203648"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Training Configuration:\n",
            "======================================================================\n",
            "Model: ResNet34 (21.3M parameters)\n",
            "MixUp Alpha: 0.2 (reduced from 0.4)\n",
            "Epochs: 40\n",
            "Optimizer: SGD (lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
            "Scheduler: OneCycleLR (max_lr=0.05)\n",
            "Data Augmentation: Simplified (removed redundant color transforms)\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Loss=3.7487 Acc=6.66: 100%|██████████| 391/391 [00:35<00:00, 10.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.8757, Accuracy: 1100/10000 (11.00%)\n",
            "\n",
            "*** New best accuracy: 11.00% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss=3.7514 Acc=12.25: 100%|██████████| 391/391 [00:34<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.6104, Accuracy: 1564/10000 (15.64%)\n",
            "\n",
            "*** New best accuracy: 15.64% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Loss=3.8730 Acc=15.47: 100%|██████████| 391/391 [00:34<00:00, 11.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4190, Accuracy: 1977/10000 (19.77%)\n",
            "\n",
            "*** New best accuracy: 19.77% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Loss=3.2073 Acc=18.16: 100%|██████████| 391/391 [00:35<00:00, 11.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.2032, Accuracy: 2318/10000 (23.18%)\n",
            "\n",
            "*** New best accuracy: 23.18% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Loss=3.1162 Acc=20.67: 100%|██████████| 391/391 [00:34<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 3.4802, Accuracy: 2391/10000 (23.91%)\n",
            "\n",
            "*** New best accuracy: 23.91% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Loss=2.8163 Acc=22.98: 100%|██████████| 391/391 [00:34<00:00, 11.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8450, Accuracy: 3056/10000 (30.56%)\n",
            "\n",
            "*** New best accuracy: 30.56% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Loss=2.8000 Acc=25.68: 100%|██████████| 391/391 [00:34<00:00, 11.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.7288, Accuracy: 3188/10000 (31.88%)\n",
            "\n",
            "*** New best accuracy: 31.88% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Loss=2.5254 Acc=27.74: 100%|██████████| 391/391 [00:34<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.8832, Accuracy: 3243/10000 (32.43%)\n",
            "\n",
            "*** New best accuracy: 32.43% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Loss=3.2784 Acc=30.23: 100%|██████████| 391/391 [00:34<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.6290, Accuracy: 3554/10000 (35.54%)\n",
            "\n",
            "*** New best accuracy: 35.54% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Loss=3.0043 Acc=32.03: 100%|██████████| 391/391 [00:34<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.5623, Accuracy: 3427/10000 (34.27%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Loss=2.5393 Acc=33.66: 100%|██████████| 391/391 [00:34<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3158, Accuracy: 4028/10000 (40.28%)\n",
            "\n",
            "*** New best accuracy: 40.28% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Loss=2.8353 Acc=35.59: 100%|██████████| 391/391 [00:35<00:00, 11.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3719, Accuracy: 3867/10000 (38.67%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Loss=3.8075 Acc=36.64: 100%|██████████| 391/391 [00:34<00:00, 11.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.4736, Accuracy: 3961/10000 (39.61%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Loss=3.1262 Acc=38.18: 100%|██████████| 391/391 [00:34<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2981, Accuracy: 4110/10000 (41.10%)\n",
            "\n",
            "*** New best accuracy: 41.10% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Loss=3.5747 Acc=39.54: 100%|██████████| 391/391 [00:34<00:00, 11.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2715, Accuracy: 4179/10000 (41.79%)\n",
            "\n",
            "*** New best accuracy: 41.79% ***\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Loss=2.4041 Acc=41.79:  17%|█▋        | 65/391 [00:05<00:28, 11.58it/s]Exception in thread Thread-35 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 61, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 37, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 519, in Client\n",
            "    c = SocketClient(address)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 647, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Epoch 16 Loss=2.4041 Acc=41.79:  17%|█▋        | 66/391 [00:05<00:29, 11.09it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-418396189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS_V2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixup_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-174365245.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, scheduler, epoch, use_mixup, mixup_alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_mixup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmixup_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2815\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2817\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2818\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2819\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}