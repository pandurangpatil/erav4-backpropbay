{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-100 Training - Optimized for 74% Accuracy\n",
    "\n",
    "## Key Improvements:\n",
    "- Fixed data augmentation (proper Cutout implementation)\n",
    "- Cosine annealing scheduler with warmup\n",
    "- Optimized hyperparameters (batch size 256, MixUp alpha 0.2)\n",
    "- Label smoothing (0.1)\n",
    "- Gradient clipping\n",
    "- Mixed precision training\n",
    "- Model checkpointing and early stopping\n",
    "- WideResNet-28-10 (36.5M parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# CIFAR-100 Mean and Std\n",
    "cifar100_mean = (0.5071, 0.4865, 0.4409)\n",
    "cifar100_std = (0.2673, 0.2564, 0.2761)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Data Augmentation\n",
    "class ImprovedAlbumentationsTransforms:\n",
    "    def __init__(self, mean, std):\n",
    "        self.aug = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "            A.RandomResizedCrop(height=32, width=32, scale=(0.8, 1.0), ratio=(0.9, 1.1), p=0.5),\n",
    "            A.CoarseDropout(max_holes=1, max_height=8, max_width=8, p=0.5),  # Cutout\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),\n",
    "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        image = np.array(img)\n",
    "        return self.aug(image=image)[\"image\"]\n",
    "\n",
    "# Instantiate transforms\n",
    "train_transforms = ImprovedAlbumentationsTransforms(mean=cifar100_mean, std=cifar100_std)\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar100_mean, std=cifar100_std)\n",
    "])\n",
    "\n",
    "# CIFAR-100 Dataset with increased batch size\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transforms)\n",
    "\n",
    "# DataLoaders with batch size 256\n",
    "BATCH_SIZE = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "cifar100_classes = datasets.CIFAR100(root='./data', train=False).classes\n",
    "print(f\"Training batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented samples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "batch_data, batch_label = next(iter(train_loader))\n",
    "batch_data = batch_data.cpu().detach()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "for i in range(12):\n",
    "    img = batch_data[i].numpy().transpose((1, 2, 0))\n",
    "    # Denormalize\n",
    "    img = img * np.array(cifar100_std) + np.array(cifar100_mean)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Class: {cifar100_classes[batch_label[i]]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WideResNet Architecture\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.equalInOut = in_planes == out_planes\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.dropRate = dropRate\n",
    "        self.shortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, 1, stride=stride, bias=False) or None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.conv1(out if self.equalInOut else x)\n",
    "        out = self.relu2(self.bn2(out))\n",
    "        if self.dropRate > 0:\n",
    "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return out + (x if self.equalInOut else self.shortcut(x))\n",
    "\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes,\n",
    "                                i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth=28, num_classes=100, widen_factor=10, dropRate=0.3):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) // 6\n",
    "        block = BasicBlock\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.adaptive_avg_pool2d(out, 1)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = WideResNet(depth=28, widen_factor=10, dropRate=0.3, num_classes=100).to(device)\n",
    "summary(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MixUp function\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Warmup Scheduler\n",
    "class WarmupScheduler:\n",
    "    def __init__(self, optimizer, warmup_epochs, initial_lr, target_lr, steps_per_epoch):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_epochs * steps_per_epoch\n",
    "        self.initial_lr = initial_lr\n",
    "        self.target_lr = target_lr\n",
    "        self.current_step = 0\n",
    "        \n",
    "    def step(self):\n",
    "        if self.current_step < self.warmup_steps:\n",
    "            lr = self.initial_lr + (self.target_lr - self.initial_lr) * self.current_step / self.warmup_steps\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        self.current_step += 1\n",
    "        \n",
    "    def is_warmup(self):\n",
    "        return self.current_step < self.warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with mixed precision and gradient clipping\n",
    "def train(model, device, train_loader, optimizer, scheduler, warmup_scheduler, scaler, epoch, \n",
    "          use_mixup=True, mixup_alpha=0.2, label_smoothing=0.1):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            if use_mixup:\n",
    "                inputs, targets_a, targets_b, lam = mixup_data(data, target, alpha=mixup_alpha, device=device)\n",
    "                outputs = model(inputs)\n",
    "                loss = lam * F.cross_entropy(outputs, targets_a, label_smoothing=label_smoothing) + \\\n",
    "                       (1 - lam) * F.cross_entropy(outputs, targets_b, label_smoothing=label_smoothing)\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "                loss = F.cross_entropy(outputs, target, label_smoothing=label_smoothing)\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Update learning rate\n",
    "        if warmup_scheduler.is_warmup():\n",
    "            warmup_scheduler.step()\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Accuracy tracking\n",
    "        _, pred = outputs.max(1)\n",
    "        if use_mixup:\n",
    "            correct += lam * pred.eq(targets_a).sum().item() + (1 - lam) * pred.eq(targets_b).sum().item()\n",
    "        else:\n",
    "            correct += pred.eq(target).sum().item()\n",
    "        processed += len(data)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        pbar.set_description(f\"Epoch {epoch} Loss={loss.item():.4f} Acc={100*correct/processed:.2f}% LR={current_lr:.6f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / processed\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n\")\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "print(\"=\" * 70)\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model: WideResNet-28-10 (36.5M parameters)\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"MixUp Alpha: 0.2\")\n",
    "print(f\"Label Smoothing: 0.1\")\n",
    "print(f\"Weight Decay: 1e-3\")\n",
    "print(f\"Gradient Clipping: 1.0\")\n",
    "print(f\"Scheduler: CosineAnnealingWarmRestarts\")\n",
    "print(f\"Warmup: 5 epochs (0.01 -> 0.1)\")\n",
    "print(f\"Mixed Precision: Enabled\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "EPOCHS = 100\n",
    "WARMUP_EPOCHS = 5\n",
    "INITIAL_LR = 0.01\n",
    "MAX_LR = 0.1\n",
    "MIN_LR = 1e-4\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=INITIAL_LR, momentum=0.9, weight_decay=1e-3)\n",
    "\n",
    "# Cosine annealing with warm restarts (T_0 = 25 epochs per cycle)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=25, T_mult=1, eta_min=MIN_LR)\n",
    "\n",
    "# Warmup scheduler\n",
    "warmup_scheduler = WarmupScheduler(optimizer, WARMUP_EPOCHS, INITIAL_LR, MAX_LR, len(train_loader))\n",
    "\n",
    "# Gradient scaler for mixed precision\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Tracking variables\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "learning_rates = []\n",
    "\n",
    "# Early stopping and checkpointing\n",
    "best_test_acc = 0.0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # Train\n",
    "    train_loss, train_acc = train(\n",
    "        model, device, train_loader, optimizer, scheduler, warmup_scheduler, scaler, epoch,\n",
    "        use_mixup=True, mixup_alpha=0.2, label_smoothing=0.1\n",
    "    )\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    \n",
    "    # Record metrics\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Save best model\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'test_accuracy': test_acc,\n",
    "            'train_accuracy': train_acc,\n",
    "        }, os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "        print(f\"*** New best model saved! Test Accuracy: {best_test_acc:.2f}% ***\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch}. No improvement for {patience} epochs.\")\n",
    "        break\n",
    "    \n",
    "    # Check if target reached\n",
    "    if test_acc >= 74.0:\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(f\"Target accuracy of 74% reached at epoch {epoch}!\")\n",
    "        print(f\"Final test accuracy: {test_acc:.2f}%\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Best Test Accuracy so far: {best_test_acc:.2f}% | Patience: {patience_counter}/{patience}\\n\")\n",
    "\n",
    "print(f\"\\nTraining completed. Best test accuracy: {best_test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss curves\n",
    "axes[0, 0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(test_losses, label='Test Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Test Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[0, 1].plot(train_accuracies, label='Train Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(test_accuracies, label='Test Accuracy', linewidth=2)\n",
    "axes[0, 1].axhline(y=74, color='r', linestyle='--', label='Target (74%)')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Training and Test Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "axes[1, 0].plot(learning_rates, linewidth=2, color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Gap between train and test accuracy (overfitting indicator)\n",
    "accuracy_gap = [train - test for train, test in zip(train_accuracies, test_accuracies)]\n",
    "axes[1, 1].plot(accuracy_gap, linewidth=2, color='orange')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy Gap (%)')\n",
    "axes[1, 1].set_title('Train-Test Accuracy Gap (Overfitting Indicator)')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"Best Test Accuracy: {best_test_acc:.2f}%\")\n",
    "print(f\"Final Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Train-Test Gap: {accuracy_gap[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']} with test accuracy: {checkpoint['test_accuracy']:.2f}%\")\n",
    "\n",
    "# Final evaluation\n",
    "test_loss, test_acc = test(model, device, test_loader)\n",
    "print(f\"\\nFinal Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
